{"meta":{"title":null,"subtitle":"","description":"","author":"zj","url":"https://18360732385.github.io","root":"/"},"pages":[{"title":"关于","date":"2020-06-20T07:01:27.424Z","updated":"2020-06-20T07:01:27.424Z","comments":false,"path":"about/index.html","permalink":"https://18360732385.github.io/about/index.html","excerpt":"","text":"这里空空如也。。。"},{"title":"所有标签","date":"2020-06-19T08:43:53.217Z","updated":"2020-06-19T08:43:53.217Z","comments":true,"path":"tags/index.html","permalink":"https://18360732385.github.io/tags/index.html","excerpt":"","text":""},{"title":"所有分类","date":"2020-06-21T04:31:22.801Z","updated":"2020-06-19T08:45:06.547Z","comments":true,"path":"categories/index.html","permalink":"https://18360732385.github.io/categories/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2019-05-04T16:00:00.000Z","updated":"2020-06-21T06:30:08.115Z","comments":true,"path":"friends/index.html","permalink":"https://18360732385.github.io/friends/index.html","excerpt":"","text":""}],"posts":[{"title":"Redission分布式锁","slug":"Redission分布式锁","date":"2020-05-04T16:00:00.000Z","updated":"2015-05-04T16:00:00.000Z","comments":true,"path":"2020/05/05/Redission分布式锁/","link":"","permalink":"https://18360732385.github.io/2020/05/05/Redission%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/","excerpt":"使用及原理","text":"使用及原理 1234&#x2F;&#x2F;看门狗（锁）RLock lock &#x3D; redisson.getLock(&quot;myLock&quot;);lock.lock();lock.unlock(); 加锁机制（源码Lua脚本分析1） ​ 某个客户端要加锁，如果他面对的是一个redis cluster集群，他首先会根据hash节点选择一台机器。这里注意，仅仅只是选择一台机器！这点很关键！ ​ 紧接着，就会发送一段lua脚本到该redis机器上。为什么要使用lua脚本？因为一大坨复杂的业务逻辑，可以通过封装在lua脚本中发送给redis，保证这段复杂业务逻辑执行的原子性。 ​ 首先解释一下名词： ​ KEYS[1] ——你加锁的那个key，比如代码里我们获取锁是这样的：RLock lock = redisson.getLock(“myLock”)。这里你自己设置了加锁的那个锁key就是“myLock”。 ​ ARGV[1] ——锁key的默认生存时间，默认30秒。 ​ ARGV[2] ——加锁的客户端的ID，类似于这样：8743c9c0-0795-4907-87fd-6c719a6b4586:1 ​ 第一段if判断语句，就是用 exists myLock 命令判断一下，如果你要加锁的那个锁key不存在的话，你就进行加锁。 ​ 如何加锁？如第2行lua脚本，创建一个名为myLock的hash结构数据，在redis中的命令就是： ​ hset myLock 8743c9c0-0795-4907-87fd-6c719a6b4586:1 1 ​ 而第3行lua脚本则会执行 pexpire myLock 30000 命令，设置myLock这个锁key的生存时间是30秒。 ​ OK,加锁完成！ 1234myLock:&#123; &quot;8743c9c0-0795-4907-87fd-6c719a6b4586:1&quot; : 1&#125; ###锁的互斥机制（Lua脚本分析2） 那么在这个时候，如果客户端2来尝试加锁，执行了同样的一段lua脚本，会咋样呢？ 很简单，第一个if判断会执行 exists myLock 命令，发现myLock这个锁key已经存在了。 接着第二个if判断，判断一下，myLock锁key的hash数据结构中，是否包含客户端2的ID，但是明显不是的，因为那里包含的是客户端1的ID。 所以，客户端2会获取到 pttl myLock 返回的一个数字，这个数字代表了myLock这个锁key的剩余生存时间。比如还剩15000毫秒的生存时间。 此时客户端2会进入一个while循环，不停的尝试加锁。(自旋锁) watch dog自动延期机制 ​ 客户端1加锁的锁key默认生存时间才30秒，如果超过了30秒，客户端1还想一直持有这把锁，怎么办呢？ ​ 简单！只要客户端1一旦加锁成功，就会启动一个watch dog看门狗，他是一个后台线程，会每隔10秒检查一下，如果客户端1还持有锁key，那么就会不断的延长锁key的生存时间。 可重入加锁机制（Lua脚本分析3）​ 那如果客户端1都已经持有了这把锁了，结果可重入的加锁会怎么样呢？ 12345678910&#x2F;&#x2F;看门狗（锁）RLock lock &#x3D; redisson.getLock(&quot;myLock&quot;);lock.lock();&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;业务代码&#x3D;&#x3D;&#x3D;lock.lock();&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;业务代码&#x3D;&#x3D;&#x3D;lock.unlock();lock.unlock(); 第一个if判断肯定不成立，“exists myLock”会显示锁key已经存在了。 第二个if判断会成立，因为myLock的hash数据结构中包含的那个ID，就是客户端1的那个ID，也就是 ​ 8743c9c0-0795-4907-87fd-6c719a6b4586:1 此时就会执行可重入加锁的逻辑，并使用下面命令： ​ incrby myLock 8743c9c0-0795-4907-87fd-6c71a6b4586:1 1 通过这个命令，对客户端1的加锁次数，累加1。 1234myLock:&#123; &quot;8743c9c0-0795-4907-87fd-6c719a6b4586:1&quot; : 2&#125; 释放锁机制 执行lock.unlock()，就可以释放分布式锁，每次都对myLock数据结构中的那个加锁次数减1。 如果发现加锁次数是0了，说明客户端1已经不再持有锁了，此时就会用 del myLock 命令，从redis里删除这个key。 然后呢，另外一直自旋的客户端2就可以尝试完成加锁了。 Redission分布式锁的缺点 ​ 如果你对某个redis master实例，写入了myLock这种锁key的value，此时会异步复制给对应的master slave实例。 但是这个过程中一旦发生redis master宕机，主备切换，redis slave变为了redis master。 ​ 接着就会导致，客户端2来尝试加锁的时候，在新的redis master上完成了加锁，而客户端1也以为自己成功加了锁。 此时就会导致多个客户端对一个分布式锁完成了加锁,，从而产生脏数据。 ​ 这个就是redis cluster，或者是redis master-slave架构的主从异步复制导致的redis分布式锁的最大缺陷：在redis master实例宕机的时候，可能导致多个客户端同时完成加锁。","categories":[{"name":"Redis","slug":"Redis","permalink":"https://18360732385.github.io/categories/Redis/"},{"name":"分布式","slug":"分布式","permalink":"https://18360732385.github.io/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://18360732385.github.io/tags/Redis/"},{"name":"分布式","slug":"分布式","permalink":"https://18360732385.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"}]},{"title":"Docker命令","slug":"Docker命令","date":"2020-03-01T16:00:00.000Z","updated":"2020-03-01T16:00:00.000Z","comments":true,"path":"2020/03/02/Docker命令/","link":"","permalink":"https://18360732385.github.io/2020/03/02/Docker%E5%91%BD%E4%BB%A4/","excerpt":"安装","text":"安装 123yum -y update 更新yumyum install docker -y 使用yum安装docker 启动Docker123456systemctl start docker.service 启动systemctl status docker.service 查看状态systemctl stop docker.service 停止systemctl restart docker.service 重启sudo systemctl enable docker 加入开机启动计划 预备工作1.安装常用工具1yum install iproute ftp bind-utils net-tools wget -y 2.配置镜像加速器1234567891.直接运行下面代码:sudo mkdir -p &#x2F;etc&#x2F;dockersudo tee &#x2F;etc&#x2F;docker&#x2F;daemon.json &lt;&lt;-&#39;EOF&#39;&#123; &quot;registry-mirrors&quot;: [&quot;https:&#x2F;&#x2F;hvmf8r55.mirror.aliyuncs.com&quot;]&#125;EOFsudo systemctl daemon-reloadsudo systemctl restart docker 3.查看 daemon.json 是否已经生效1tail &#x2F;etc&#x2F;docker&#x2F;daemon.json 镜像命令1.从官网上拉取镜像1docker pull 镜像名(nginx,centos等) 2.官网仓库查询镜像名1docker search 镜像名 3.查看所有的本地镜像1docker images 4.删除镜像(当镜像有对应容器时,无法删除)12docker rmi 镜像名(或id) 删除单个镜像docker rmi $(docker images -q) 删除全部镜像 5.标记(重命名)镜像 同一个id就会有两个镜像 1docker tag 镜像名1:tag1 镜像名2:tag2 此时docker images会发现2个镜像的id相同 6.提交镜像到官网12docker login 先登录docker push 镜像名 这里的镜像名有要求:用户名&#x2F;镜像名:版本号,如zj&#x2F;nginx:latest 容器命令1.根据镜像生成容器,并运行12345678docker run -dit --privileged -p21:21 -p80:80 -p8080:8080 -p30000-30010:30000-30010 --name how2jtmall how2j&#x2F;tmall:latest &#x2F;usr&#x2F;sbin&#x2F;init --privileged 启动容器的时候，把权限带进去。 -dit -d后台运行 -i提供交互接口 -t提供命令行终端 -p 80:80 第一个80是CentOS 上80端口,第二个80是容器的80端口,两个端口对接开放 --name how2jtmall how2j&#x2F;tmall:latest --name 容器别名 镜像名:版本号 &#x2F;usr&#x2F;sbin&#x2F;init 表示启动后运行的程序，即通过这个命令做初始化 2.创建容器1docker create 创建,不启动 3.进入容器12docker exec -it 容器名 &#x2F;bin&#x2F;bashexit 离开容器 4.将容器提交成镜像1docker commit 容器名 镜像名 5.启动容器12345docker start 容器名 启动docker restart 容器名 重启docker stop 容器名 停止docker pause 容器名 暂停docker unpause 容器名 恢复 6.查看容器12docker ps -a 查询所有容器docker ps 查询运行中的容器 7.查看容器所有信息1docker inspect 容器名 8.删除容器12docker rm 容器名 删除单个docker rm &#96;docker ps -a -q&#96; -f 删除所有容器 (-f强制删除) 9.查看容器日志12docker logs 容器名docker logs -f --tail&#x3D;3 -t 容器名 -f实时日志 -t添加时间 --tail&#x3D;3最近3条数据 10.将宿主机的文件复制到容器中123docker cp 宿主机文件地址 容器名:容器目标地址docker cp .&#x2F;blog&#x2F;docker&#x2F;index.html nginx:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html&#x2F; 11.容器的导出123docker export 容器名 &gt; 宿主机地址&#x2F;文件名(.tar)docker export nginx &gt; .&#x2F;blog&#x2F;docker&#x2F;nginx.tar 12.容器的导入123cat 文件名(.tar) | docker import 容器名:版本号cat nginx.tar | docker import -importnginx:latest 数据卷1.挂载1234docker run -itd --name 容器名 -v 宿主机挂载的绝对地址:容器目录 -p 80:80 nginxdocker run -itd --name nginx2 -v .&#x2F;home:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html -p 80:80 nginx将宿主机的.&#x2F;home目录挂载到容器的&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html目录下,这样在宿主机里修改文件,对应的容器改目录下也会修改 2.查看数据卷12docker volume ls 查看所有数据卷docker volume inspect 数据卷名(根据上一个命令查看名称) 查看数据卷详情 3.删除数据卷12docker volume rm 数据卷名 删除单个数据卷docker volume prune 数据卷名 批量删除(与容器相关的数据卷不会删除) ​ 数据卷容器 数据卷容器也是容器,用来挂载数据卷的容器,专门供其他容器来引用 1.创建数据卷容器123docker run -itd -v 容器目录 --name 容器名 容器类docker run -itd -v &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html --name mydata centos 2.引用容器123456docker run -itd --volumes-from 数据卷容器名 -p 80:80 --name 创建的容器名 容器类docker run -itd --volums-from mydata -p 80:80 --name nginx1 nginxdocker run -itd --volums-from mydata -p 80:80 --name nginx2 nginxnginx1和nginx2都挂载了同一个数据卷到 &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html 目录下,三个容器中,修改任意一处,三处都会修改 3.利用数据卷容器对数据备份和恢复1查看文档19 ​ Dockerfile和自动化构建 创建本地镜像的方式:1.commit 2.dockerfile 1查看文档10-12 容器连接与容器编排1查看文档20-21","categories":[],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://18360732385.github.io/tags/Docker/"}]},{"title":"Docker进阶","slug":"Docker进阶","date":"2020-03-01T16:00:00.000Z","updated":"2020-03-01T16:00:00.000Z","comments":true,"path":"2020/03/02/Docker进阶/","link":"","permalink":"https://18360732385.github.io/2020/03/02/Docker%E8%BF%9B%E9%98%B6/","excerpt":"关于阿里云镜像1.登录","text":"关于阿里云镜像1.登录 1sudo docker login --username&#x3D;光妹的欧派 registry.cn-hangzhou.aliyuncs.com 2.拉取镜像123docker pull registry.cn-hangzhou.aliyuncs.com&#x2F;用户名&#x2F;仓库:[镜像版本号]docker pull registry.cn-hangzhou.aliyuncs.com&#x2F;jueduifeng&#x2F;zj:p1 3.标记镜像123docker tag registry.cn-hangzhou.aliyuncs.com&#x2F;用户名&#x2F;仓库:[镜像版本号]docker tag registry.cn-hangzhou.aliyuncs.com&#x2F;jueduifeng&#x2F;zj:p2 4.推送至阿里云对应仓库 推送时可以使用专用网址,如registry-vpc.cn-hangzhou.aliyuncs.com/jueduifeng/zj 前提是标记tag时也使用专用网址,如registry-vpc.cn-hangzhou.aliyuncs.com/jueduifeng/zj:p2 123docker push registry.cn-hangzhou.aliyuncs.com&#x2F;用户名&#x2F;仓库:[镜像版本号]docker push registry.cn-hangzhou.aliyuncs.com&#x2F;jueduifeng&#x2F;zj:p2 5.总结1pull&gt;&gt;tag&gt;&gt;push 使用Dockerfile文件构建镜像 https://mp.weixin.qq.com/s?__biz=MzAxNjk4ODE4OQ==&amp;mid=2247484233&amp;idx=1&amp;sn=4350e1cb4f031abeba6b1223ff73a58d&amp;chksm=9bed223bac9aab2da679d0b40f5dbf6e4ebf1a5bde030b9b25197db3783b3200a2b9fc0f2494&amp;scene=21#wechat_redirect 1.Dockerfile 注意这里定义了2个文件:requirements.txt和app.py 1234567891011121314151617181920# 在镜像python:2.7-slim上基础上构建FROM python:2.7-slim# 将工作目录设置为 &#x2F;appWORKDIR &#x2F;app# 将当前目录内容复制到容器目录 &#x2F;appADD . &#x2F;app# 安装requirements.txt中指定的任何需要的包RUN pip install --trusted-host pypi.python.org -r requirements.txt# 使80号端口对外暴露EXPOSE 80# 定义环境变量ENV NAME World# 在容器启动时运行app.py文件CMD [&quot;python&quot;, &quot;app.py&quot;] 2.requirements.txt 需要安装的包 12FlaskRedis 3.app.py 现在我们看到pip install -r requirements.txt为Python安装Flask和Redis库，并且该应用程序打印环境变量NAME以及调用socket.gethostname（）的输出。 最后，因为Redis没有运行（因为我们只安装了Python库，而不是Redis本身），所以我们应该期望在这里尝试使用它会失败并产生错误消息。 那正是要点！ 您的系统上不需要Python或任何requirements.txt文件，也不需要在您的系统上安装或运行此映像。看起来你并没有真正用Python和Flask建立一个环境，但是你已经拥有了。 123456789101112131415161718192021222324from flask import Flaskfrom redis import Redis, RedisErrorimport osimport socket# 连接Redisredis &#x3D; Redis(host&#x3D;&quot;redis&quot;, db&#x3D;0, socket_connect_timeout&#x3D;2, socket_timeout&#x3D;2)app &#x3D; Flask(__name__)@app.route(&quot;&#x2F;&quot;)def hello(): try: visits &#x3D; redis.incr(&quot;counter&quot;) except RedisError: visits &#x3D; &quot;&lt;i&gt;cannot connect to Redis, counter disabled&lt;&#x2F;i&gt;&quot; html &#x3D; &quot;&lt;h3&gt;Hello &#123;name&#125;!&lt;&#x2F;h3&gt;&quot; \\ &quot;&lt;b&gt;Hostname:&lt;&#x2F;b&gt; &#123;hostname&#125;&lt;br&#x2F;&gt;&quot; \\ &quot;&lt;b&gt;Visits:&lt;&#x2F;b&gt; &#123;visits&#125;&quot; return html.format(name&#x3D;os.getenv(&quot;NAME&quot;, &quot;world&quot;), hostname&#x3D;socket.gethostname(), visits&#x3D;visits)if __name__ &#x3D;&#x3D; &quot;__main__&quot;: app.run(host&#x3D;&#39;0.0.0.0&#39;, port&#x3D;80) 4.构建应用 此时文件夹下应该有这3个文件 注意构建目录目录最后的 . 12$ lsDockerfile app.py requirements.txt 1docker build -t friendlyhello . ​","categories":[],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://18360732385.github.io/tags/Docker/"}]},{"title":"代理模式和AOP","slug":"代理模式和AOP","date":"2019-11-04T16:00:00.000Z","updated":"2019-11-04T16:00:00.000Z","comments":true,"path":"2019/11/05/代理模式和AOP/","link":"","permalink":"https://18360732385.github.io/2019/11/05/%E4%BB%A3%E7%90%86%E6%A8%A1%E5%BC%8F%E5%92%8CAOP/","excerpt":"​ 官述:为其他对象提供一种代理以控制对这个对象的访问。 ​ 比如A对象要做一件事情，在没有代理前，自己来做；在对 A 代理后，由 A 的代理类 B 来做。","text":"​ 官述:为其他对象提供一种代理以控制对这个对象的访问。 ​ 比如A对象要做一件事情，在没有代理前，自己来做；在对 A 代理后，由 A 的代理类 B 来做。 ​ 代理其实是在原实例前后加了一层处理，这也是 AOP 的初级轮廓。 静态代理 ​ 静态代理说白了，就是在程序运行前就已经存在代理类的字节码文件、代理类和原始类的关系在运行前就已经确定。静态代理的特点: 目标对象必须要实现接口 代理对象，要实现与目标对象一样的接口 1234567891011121314151617&#x2F;&#x2F; 接口public interface IUserDao &#123; void save(); void find();&#125; &#x2F;&#x2F;目标对象(必须要实现接口)class UserDao implements IUserDao&#123; @Override public void save() &#123; System.out.println(&quot;模拟：保存用户！&quot;); &#125; @Override public void find() &#123; System.out.println(&quot;模拟：查询用户&quot;); &#125;&#125; 123456789101112131415161718&#x2F;&#x2F;静态代理,代理对象(要实现与目标对象一样的接口)class UserDaoProxy implements IUserDao&#123; &#x2F;&#x2F; 代理对象，需要维护一个目标对象 private IUserDao target &#x3D; new UserDao(); @Override public void save() &#123; System.out.println(&quot;代理操作： 开启事务...&quot;); target.save(); &#x2F;&#x2F; 执行目标对象的方法 System.out.println(&quot;代理操作：提交事务...&quot;); &#125; @Override public void find() &#123; target.find(); &#125;&#125; ​ 静态代理虽然保证了业务类只需关注逻辑本身，代理对象的一个接口只服务于一种类型的对象。如果要代理的方法很多，势必要为每一种方法都进行代理。再者，如果增加一个方法，除了实现类需要实现这个方法外，所有的代理类也要实现此方法。增加了代码的维护成本。那么要如何解决呢？答案是使用动态代理。 动态代理 ​ 动态代理类的源码是在程序运行期间，通过 JVM 反射等机制动态生成。代理类和委托类的关系是运行时才确定的。 1接口和目标代理一致 ​ find方法直接调用,save方法加一层事务 123456789101112131415161718192021222324252627282930313233343536373839404142434445&#x2F;** * 动态代理(jdk)： * 代理工厂，给多个目标对象生成代理对象！ * *&#x2F;class ProxyFactory &#123; &#x2F;&#x2F; 接收一个目标对象 private Object target; public ProxyFactory(Object target) &#123; this.target &#x3D; target; &#125; &#x2F;&#x2F; 返回对目标对象(target)代理后的对象(proxy) public Object getProxyInstance() &#123; Object proxy &#x3D; Proxy.newProxyInstance( target.getClass().getClassLoader(), &#x2F;&#x2F; 目标对象使用的类加载器 target.getClass().getInterfaces(), &#x2F;&#x2F; 目标对象实现的所有接口 new InvocationHandler() &#123; &#x2F;&#x2F; 执行代理对象方法时候触发 @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; &#x2F;&#x2F; 获取当前执行的方法的方法名 String methodName &#x3D; method.getName(); &#x2F;&#x2F; 方法返回值 Object result &#x3D; null; if (&quot;find&quot;.equals(methodName)) &#123; &#x2F;&#x2F; 直接调用目标对象方法 result &#x3D; method.invoke(target, args); &#125; else &#123; System.out.println(&quot;开启事务...&quot;); &#x2F;&#x2F; 执行目标对象方法 result &#x3D; method.invoke(target, args); System.out.println(&quot;提交事务...&quot;); &#125; return result; &#125; &#125; ); return proxy; &#125;&#125; 动态代理测试如图: 动态代理的分类 ​ 使用 JDK 生成的动态代理的前提是目标类必须有实现的接口。但这里又引入一个问题，如果某个类没有实现接口，就不能使用 JDK 动态代理。所以 CGLIB 代理就是解决这个问题的。 ​ CGLIB 是以动态生成的子类继承目标的方式实现，在运行期动态的在内存中构建一个子类，如下： ​ CGLIB 使用的前提是目标类不能为 final 修饰。因为 final 修饰的类不能被继承。 CGLib动态代理 ​ CGLib采用了非常底层的字节码技术，其原理是通过字节码技术为一个类创建子类，并在子类中采用方法拦截的技术拦截所有父类方法的调用，顺势织入横切逻辑。 ​ CglibBlogFactory代理工厂类如下： 1234567891011121314151617181920212223242526272829public class CglibBlogFactory implements MethodInterceptor &#123; private Object target; public CglibBlogFactory(Object target) &#123; this.target &#x3D; target; &#125; &#x2F;&#x2F;给目标对象创建一个代理对象 public Object getProxyInstance() &#123; &#x2F;&#x2F;1.工具类 Enhancer en &#x3D; new Enhancer(); &#x2F;&#x2F;2.设置父类 en.setSuperclass(target.getClass()); &#x2F;&#x2F;3.设置回调函数 en.setCallback(this); &#x2F;&#x2F;4.创建子类(代理对象) return en.create(); &#125; @Override public Object intercept(Object o, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable &#123; System.out.println(&quot;start writing...&quot;); &#x2F;&#x2F;执行目标对象的方法 Object returnValue &#x3D; method.invoke(target, objects); System.out.println(&quot;end writing...&quot;); return returnValue; &#125;&#125; 测试类: 12345678910public class TestCglib &#123; public static void main(String[] args) &#123; IBlogService target &#x3D; new BlogService(); &#x2F;&#x2F;代理对象 IBlogService proxy &#x3D; (IBlogService) new CglibBlogFactory(target).getProxyInstance(); &#x2F;&#x2F;执行代理对象的方法 proxy.writeBlog(); &#125;&#125; Spring Aop切面编程 创建容器对象的时候，根据切入点表达式拦截的类，生成代理对象。 如果目标对象有实现接口，使用 JDK 代理。如果目标对象没有实现接口，则使用 CGLIB 代理。然后从容器获取代理后的对象，在运行期植入“切面”类的方法。 Spring Aop能做什么 Controller层的参数校验 使用 Spring AOP 实现 MySQL 数据库读写分离案例分析 在执行方法前，判断是否具有权限 对部分函数的调用进行日志记录：监控部分重要函数，若抛出指定的异常，可以以短信或邮件方式通知相关人员。 信息过滤，页面转发等等功能","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://18360732385.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"AOP","slug":"AOP","permalink":"https://18360732385.github.io/tags/AOP/"}]},{"title":"适配器设计模式","slug":"适配器设计模式","date":"2019-11-03T16:00:00.000Z","updated":"2019-11-03T16:00:00.000Z","comments":true,"path":"2019/11/04/适配器设计模式/","link":"","permalink":"https://18360732385.github.io/2019/11/04/%E9%80%82%E9%85%8D%E5%99%A8%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/","excerpt":"​ 适配器模式将某个类的接口转换成客户端期望的另一个接口表示，目的是消除由于接口不匹配所造成的类的兼容性问题。主要分为三类：类的适配器模式、对象的适配器模式、接口的适配器模式。","text":"​ 适配器模式将某个类的接口转换成客户端期望的另一个接口表示，目的是消除由于接口不匹配所造成的类的兼容性问题。主要分为三类：类的适配器模式、对象的适配器模式、接口的适配器模式。 类的适配器模式12345678910111213public class Source &#123; public void method1() &#123; System.out.println(&quot;我是类Source的方法!&quot;); &#125; &#125; public interface Targetable &#123; &#x2F;* 与原类中的方法相同 *&#x2F; public void method1(); &#x2F;* 新类的方法 *&#x2F; public void method2();&#125; 1234567public class Adapter extends Source implements Targetable &#123; @Override public void method2() &#123; System.out.println(&quot;我是适配器Adapter重写接口Targetable的方法!&quot;); &#125;&#125; 12345678public class AdapterTest &#123; public static void main(String[] args) &#123; &#x2F;&#x2F;多态 Targetable target &#x3D; new Adapter(); target.method1(); target.method2(); &#125;&#125; 对象的适配器模式 ​ 基本思路和类的适配器模式相同，只是将 Adapter 类作修改，这次不继承 Source 类，*而是持有 Source 类的实例 *，以达到解决兼容性的问题。 123456789101112131415161718public class Wrapper implements Targetable &#123; private Source source; public Wrapper(Source source) &#123; super(); this.source &#x3D; source; &#125; @Override public void method2() &#123; System.out.println(&quot;this is the targetable method!&quot;); &#125; @Override public void method1() &#123; source.method1(); &#125; &#125; 12345678public class AdapterTest &#123; public static void main(String[] args) &#123; Source source &#x3D; new Source(); Targetable target &#x3D; new Wrapper(source); target.method1(); target.method2(); &#125; &#125; 接口的适配器模式 ​ 接口的适配器是这样的：有时我们写的一个接口中有多个抽象方法，当我们写该接口的实现类时，必须实现该接口的所有方法，这明显有时比较浪费，因为并不是所有的方法都是我们需要的，有时只需要某一些。 ​ 此处为了解决这个问题，我们引入了接口的适配器模式，借助于一个抽象类，该抽象类实现了该接口，实现了所有的方法，而我们不和原始的接口打交道，只和该抽象类取得联系，所以我们写一个类，继承该抽象类，重写我们需要的方法。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://18360732385.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[]},{"title":"策略模式","slug":"策略模式","date":"2019-11-02T16:00:00.000Z","updated":"2019-11-02T16:00:00.000Z","comments":true,"path":"2019/11/03/策略模式/","link":"","permalink":"https://18360732385.github.io/2019/11/03/%E7%AD%96%E7%95%A5%E6%A8%A1%E5%BC%8F/","excerpt":"​ 策略模式可以解决使用 if…else 所带来的复杂和难以维护。这里举一个例子：","text":"​ 策略模式可以解决使用 if…else 所带来的复杂和难以维护。这里举一个例子： ​ 订单系统中订单的状态，用户对订单的操作随着订单状态改变。如订单在配送中时，用户不能执行删除、评论、支付、取消订单的操作。但是订单状态种类太多，用if…else来写太麻烦，可以抽离出来一个类OrderHandleOption，用来存储当前订单的各项执行选项状态。 ​ 同样的场景还适用于会员等级与价格折扣之间、优惠券种类与最优优惠价之间、优惠协议与具体优惠方式之间。 ​ https://www.runoob.com/design-pattern/strategy-pattern.html 结合 ​ 策略模式和枚举类结合的案例： 12345678910111213141516&#x2F;&#x2F;1、定义一个包含通知实现机制的“充血”的枚举类型enum NOTIFY_TYPE &#123; email(&quot;邮件&quot;,NotifyMechanismInterface.byEmail()), sms(&quot;短信&quot;,NotifyMechanismInterface.bySms()), wechat(&quot;微信&quot;,NotifyMechanismInterface.byWechat()); String memo; NotifyMechanismInterface notifyMechanism; &#x2F;&#x2F;2、私有构造函数，用于初始化枚举值 private NOTIFY_TYPE(String memo,NotifyMechanismInterface notifyMechanism)&#123; this.memo&#x3D;memo; this.notifyMechanism&#x3D;notifyMechanism; &#125; &#x2F;&#x2F;getters ...&#125; 12345678910111213141516171819202122232425262728293031&#x2F;&#x2F;3、定义通知机制的接口或抽象父类public interface NotifyMechanismInterface&#123; public boolean doNotify(String msg); &#x2F;&#x2F;3.1 返回一个定义了邮件通知机制的策的实现——一个匿名内部类实例 public static NotifyMechanismInterface byEmail()&#123; return new NotifyMechanismInterface()&#123; public boolean doNotify(String msg)&#123; ....... &#125; &#125;; &#125; &#x2F;&#x2F;3.2 定义短信通知机制的实现策略 public static NotifyMechanismInterface bySms()&#123; return new NotifyMechanismInterface()&#123; public boolean doNotify(String msg)&#123; ....... &#125; &#125;; &#125; &#x2F;&#x2F;3.3 定义微信通知机制的实现策略 public static NotifyMechanismInterface byWechat()&#123; return new NotifyMechanismInterface()&#123; public boolean doNotify(String msg)&#123; ....... &#125; &#125;; &#125;&#125; 12&#x2F;&#x2F;4、使用场景NOTIFY_TYPE.valueof(type).getNotifyMechanism().doNotify(msg);","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://18360732385.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[]},{"title":"装饰者模式","slug":"装饰者模式","date":"2019-11-02T16:00:00.000Z","updated":"2019-11-02T16:00:00.000Z","comments":true,"path":"2019/11/03/装饰者模式/","link":"","permalink":"https://18360732385.github.io/2019/11/03/%E8%A3%85%E9%A5%B0%E8%80%85%E6%A8%A1%E5%BC%8F/","excerpt":"​ 装饰器模式（Decorator Pattern）允许向一个现有的对象添加新的功能，同时又不改变其结构。这种类型的设计模式属于结构型模式，它是作为现有的类的一个包装。","text":"​ 装饰器模式（Decorator Pattern）允许向一个现有的对象添加新的功能，同时又不改变其结构。这种类型的设计模式属于结构型模式，它是作为现有的类的一个包装。 父类food123456789101112131415public class Food &#123; private String food_name; public Food() &#123; &#125; public Food(String food_name) &#123; this.food_name &#x3D; food_name; &#125; public String make() &#123; return food_name; &#125;;&#125; 子类12345678910111213&#x2F;&#x2F;面包类public class Bread extends Food &#123; private Food basic_food; public Bread(Food basic_food) &#123; this.basic_food &#x3D; basic_food; &#125; public String make() &#123; return basic_food.make()+&quot;+面包&quot;; &#125;&#125; 12345678910111213&#x2F;&#x2F;奶油类public class Cream extends Food &#123; private Food basic_food; public Cream(Food basic_food) &#123; this.basic_food &#x3D; basic_food; &#125; public String make() &#123; return basic_food.make()+&quot;+奶油&quot;; &#125;&#125; 1234567891011121314&#x2F;&#x2F;蔬菜类public class Vegetable extends Food &#123; private Food basic_food; public Vegetable(Food basic_food) &#123; this.basic_food &#x3D; basic_food; &#125; public String make() &#123; return basic_food.make()+&quot;+蔬菜&quot;; &#125;&#125; Test123456public class Test &#123; public static void main(String[] args) &#123; Food food &#x3D; new Bread(new Vegetable(new Cream(new Food(&quot;香肠&quot;)))); System.out.println(food.make()); &#125;&#125; 12打印日志：香肠+奶油+蔬菜+面包","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://18360732385.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[]},{"title":"观察者模式","slug":"观察者模式","date":"2019-11-02T16:00:00.000Z","updated":"2019-11-02T16:00:00.000Z","comments":true,"path":"2019/11/03/观察者模式/","link":"","permalink":"https://18360732385.github.io/2019/11/03/%E8%A7%82%E5%AF%9F%E8%80%85%E6%A8%A1%E5%BC%8F/","excerpt":"​ 当对象间存在一对多关系时，则使用观察者模式（Observer Pattern）。比如，当一个对象被修改时，则会自动通知它的依赖对象。拍卖的时候，拍卖师观察最高标价，然后通知给其他竞价者竞价。","text":"​ 当对象间存在一对多关系时，则使用观察者模式（Observer Pattern）。比如，当一个对象被修改时，则会自动通知它的依赖对象。拍卖的时候，拍卖师观察最高标价，然后通知给其他竞价者竞价。 ​ 观察者模式的实践是监听事件。 ​ 观察者模式使用三个类 Subject、Observer 和 Client。 Subject 对象带有绑定观察者到 Client 对象和从 Client 对象解绑观察者的方法。我们创建 Subject 类、Observer 抽象类和扩展了抽象类 Observer 的实体类。 Subject 类 ​ 被观察者subject是一个父类，它需要维护一个被通知对象的集合 ，而ConcreteSubject是一个具体的被观察的对象。 ​ 一旦ConcreteSubject的状态改变，我们就去通知集合里的所有对象，告知：ConcreteSubject干了XXX 。 123456789101112131415161718192021public class Subject &#123; &#x2F;&#x2F;保存注册的观察者对象 private List&lt;Observer&gt; mObervers &#x3D; new ArrayList&lt;&gt;(); &#x2F;&#x2F;注册观察者对象 public void attach(Observer observer) &#123; mObervers.add(observer); &#125; &#x2F;&#x2F;注销观察者对象 public void detach(Observer observer) &#123; mObervers.remove(observer); &#125; &#x2F;&#x2F;通知所有注册的观察者对象 public void notifyEveryOne(int newState) &#123; for (Observer observer : mObervers) &#123; observer.update(newState); &#125; &#125;&#125; 123456789101112131415public class ConcreteSubject extends Subject &#123; private int state; public int getState() &#123; return state; &#125; &#x2F;&#x2F;一旦ConcreteSubject执行change方法，就通知所有观察者 public void change(int newState) &#123; state &#x3D; newState; &#x2F;&#x2F;状态发生改变，通知观察者 notifyEveryOne(newState); &#125;&#125; Observer 类 ​ Observer 抽象类主要做两件事情： 将subject类封装在自己身体里，使具体的观察者初始化时，可以添加到subject的通知集合里 。 提供一个抽象方法，具体的观察者会重写这个方法。如果被观察者改变了，不同的观察者可以做出自己的判断。 1234public abstract class Observer &#123; protected Subject subject; public abstract void update(int i);&#125; ​ 接下来就是创建具体的观察者 。 123456789101112131415public class ObserverOne extends Observer&#123; &#x2F;&#x2F;初始化时将自己添加到subject的通知集合里，自己就会监听所有继承subject的被观察者对象 public ObserverOne(Subject subject)&#123; this.subject &#x3D; subject; this.subject.attach(this); &#125; @Override public void update(int i) &#123; i +&#x3D; 1; System.out.println( &quot;观察者1监听到对象 &quot; +subject.toString() +&quot;,并将state变成：&quot;+ i ); &#125;&#125; 1234567891011121314public class ObserverTwo extends Observer&#123; public ObserverTwo(Subject subject)&#123; this.subject &#x3D; subject; this.subject.attach(this); &#125; @Override public void update(int i) &#123; i +&#x3D; 2; System.out.println( &quot;观察者2监听到对象 &quot; +subject.toString() +&quot;,并将state变成：&quot;+ i ); &#125;&#125; Demo代码123456789101112131415public class ObserverDemo &#123; public static void main(String[] args) &#123; ConcreteSubject concreteSubject &#x3D; new ConcreteSubject(); &#x2F;&#x2F;这里使用多态，表示ObserverOne和ObserverTwo观察concreteSubject对象 new ObserverOne(concreteSubject); new ObserverTwo(concreteSubject); System.out.println(&quot;被观察者ConcreteSubject改变state为5&quot;); concreteSubject.change(5); System.out.println(&quot;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&quot;); System.out.println(&quot;被观察者ConcreteSubject改变state为10&quot;); concreteSubject.change(10); &#125;&#125; ​ 执行代码，输入如下： 1234567被观察者ConcreteSubject改变state为5观察者1监听到对象 ConcreteSubject@4554617c,并将state变成：6观察者2监听到对象 ConcreteSubject@4554617c,并将state变成：7&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;被观察者ConcreteSubject改变state为10观察者1监听到对象 ConcreteSubject@4554617c,并将state变成：11观察者2监听到对象 ConcreteSubject@4554617c,并将state变成：12 自定义监听事件 ​ https://www.jianshu.com/p/897e0a128e54","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://18360732385.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[]},{"title":"工厂模式","slug":"工厂模式","date":"2019-11-01T16:00:00.000Z","updated":"2019-11-01T16:00:00.000Z","comments":true,"path":"2019/11/02/工厂模式/","link":"","permalink":"https://18360732385.github.io/2019/11/02/%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F/","excerpt":"简单的工厂模式","text":"简单的工厂模式 1234567891011121314151617public interface Sender &#123; public void Send(); &#125; public class MailSender implements Sender &#123; @Override public void Send() &#123; System.out.println(&quot;this is mail sender!&quot;); &#125; &#125; public class SmsSender implements Sender &#123; @Override public void Send() &#123; System.out.println(&quot;this is sms sender!&quot;); &#125; &#125; 123456789101112public class SendFactory &#123; public Sender produce(String type) &#123; if (&quot;mail&quot;.equals(type)) &#123; return new MailSender(); &#125; else if (&quot;sms&quot;.equals(type)) &#123; return new SmsSender(); &#125; else &#123; System.out.println(&quot;请输入正确的类型!&quot;); return null; &#125; &#125;&#125; 多个工厂方法模式 ​ 该模式是对普通工厂方法模式的改进，在普通工厂方法模式中，如果传递的字符串出错，则不能正确创建对象，而多个工厂方法模式是提供多个工厂方法，分别创建对象。 123456789 public class SendFactory &#123; public Sender produceMail()&#123; return new MailSender(); &#125; public Sender produceSms()&#123; return new SmsSender(); &#125;&#125; 1234567 public class FactoryTest &#123; public static void main(String[] args) &#123; SendFactory factory &#x3D; new SendFactory(); Sender sender &#x3D; factory.produceMail(); sender.send(); &#125;&#125; 静态工厂模式 ​ 将上面的多个工厂方法模式里的方法置为静态的，不需要创建实例，直接调用即可。 123456789public class SendFactory &#123; public static Sender produceMail()&#123; return new MailSender(); &#125; public static Sender produceSms()&#123; return new SmsSender(); &#125;&#125; 123456 public class FactoryTest &#123; public static void main(String[] args) &#123; Sender sender &#x3D; SendFactory.produceMail(); sender.send(); &#125;&#125; 抽象工厂模式 ​ 工厂方法模式有一个问题就是，类的创建依赖工厂类，也就是说，如果想要拓展程序，必须对工厂类进行修改，这违背了闭包原则，所以，从设计角度考虑，有一定的问题，如何解决？​ 这就用到抽象工厂模式，创建多个工厂类，这样一旦需要增加新的功能，直接增加新的工厂类就可以了，不需要修改之前的代码。 ​ 一个多态和反多态的体现： 12345678910&#x2F;&#x2F;工厂接口public interface Provider &#123; &#x2F;&#x2F;返回值为Sender public Sender produce();&#125;&#x2F;&#x2F;工厂产品接口（发送方式） public interface Sender &#123; public void send();&#125; 12345678910111213public class MailSender implements Sender &#123; @Override public void send() &#123; System.out.println(&quot;this is mail sender!&quot;); &#125;&#125;public class SmsSender implements Sender &#123; @Override public void send() &#123; System.out.println(&quot;this is sms sender!&quot;); &#125; &#125; 12345678910111213public class SendSmsFactory implements Provider &#123; @Override public Sender produce() &#123; return new SmsSender(); &#125; &#125; public class SendMailFactory implements Provider &#123; @Override public Sender produce() &#123; return new MailSender(); &#125;&#125; 12345678public class Test &#123; public static void main(String[] args) &#123; &#x2F;&#x2F;多态 Provider provider &#x3D; new SendMailFactory(); Sender sender &#x3D; provider.produce(); sender.send(); &#125; &#125;","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://18360732385.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[]},{"title":"单例模式","slug":"单例模式","date":"2019-10-31T16:00:00.000Z","updated":"2019-10-31T16:00:00.000Z","comments":true,"path":"2019/11/01/单例模式/","link":"","permalink":"https://18360732385.github.io/2019/11/01/%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/","excerpt":"静态内部类(推荐使用) ​ 这种方式跟饿汉式方式采用的机制类似，但又有不同。两者都是采用了类装载的机制来保证初始化实例时只有一个线程。不同的地方在饿汉式方式是只要Singleton类被装载就会实例化，没有Lazy-Loading的作用，而静态内部类方式在","text":"静态内部类(推荐使用) ​ 这种方式跟饿汉式方式采用的机制类似，但又有不同。两者都是采用了类装载的机制来保证初始化实例时只有一个线程。不同的地方在饿汉式方式是只要Singleton类被装载就会实例化，没有Lazy-Loading的作用，而静态内部类方式在 Singleton类被装载时并不会立即实例化，而是在需要实例化时，调用getInstance方法，才会装载SingletonInstance类，从而完成Singleton的实例化。 ​ 类的静态属性只会在第一次加载类的时候初始化，所以在这里，JVM帮助我们保证了线程的安全性，在类进行初始化时，别的线程是无法进入的。 优点：避免了线程不安全，延迟加载，效率高。 123456789101112131415public class Singleton &#123; &#x2F;&#x2F;构造器私有化 private Singleton() &#123;&#125; &#x2F;&#x2F;静态内部类,实例一个静态常量Singleton private static class SingletonInstance &#123; private static final Singleton INSTANCE &#x3D; new Singleton(); &#125; &#x2F;&#x2F;静态方法,返回SingletonInstance实例出来的Singleton public static Singleton getInstance() &#123; return SingletonInstance.INSTANCE; &#125;&#125; 双重检查 ​ Double-Check概念对于多线程开发者来说不会陌生，如代码中所示，我们进行了两次if (singleton == null)检查，这样就可以保证线程安全了。这样，实例化代码只用执行一次，后面再次访问时，判断if (singleton == null)，直接return实例化对象。 优点：线程安全；延迟加载；效率较高。 123456789101112131415161718192021public class VolatileSingleTon &#123; &#x2F;&#x2F;使用 Volatile 保证了指令重排序在这个对象创建的时候不可用 private volatile static VolatileSingleTon singleTon &#x3D; null; &#x2F;&#x2F;构造器私有化 private VolatileSingleTon() &#123;&#125; &#x2F;&#x2F;双重检查 public static VolatileSingleTon getInstance() &#123; if (singleTon &#x3D;&#x3D; null) &#123; synchronized (VolatileSingleTon.class) &#123; if (singleTon &#x3D;&#x3D; null) &#123; singleTon &#x3D; new VolatileSingleTon(); &#125; &#125; &#125; return singleTon; &#125; &#125; 枚举 ​ 枚举的思想其实是通过共有的静态 final 与为每个枚举常量导出实例的类，由于没有可访问的构造器，所以不能调用枚举常量的构造方法去生成对应的对象，因此在《Effective Java》 中，枚举类型为类型安全的枚举模式，枚举也被称为单例的泛型化。 12345678910public enum Singleton &#123; &#x2F;&#x2F;线程安全的实例 INSTANCE; &#x2F;&#x2F;方法 public void whateverMethod() &#123; &#125;&#125;","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://18360732385.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"单例模式","slug":"单例模式","permalink":"https://18360732385.github.io/tags/%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/"}]},{"title":"加解密流程","slug":"加解密流程","date":"2019-10-13T16:00:00.000Z","updated":"2019-10-13T16:00:00.000Z","comments":true,"path":"2019/10/14/加解密流程/","link":"","permalink":"https://18360732385.github.io/2019/10/14/%E5%8A%A0%E8%A7%A3%E5%AF%86%E6%B5%81%E7%A8%8B/","excerpt":"1.名词解释 key: 对称加密需要的秘钥，用来对明文加密。速度快，适用对报文加密。","text":"1.名词解释 key: 对称加密需要的秘钥，用来对明文加密。速度快，适用对报文加密。 ​ privateKey和publicKey：私钥和公钥，非对称加密需要的一对秘钥，私钥保存一份放在服务端，公钥可以有无数个，分发给客户端。加密速度慢，一般用于对key加密。 salt: 盐值，一串随机数。对明文签名，防止报文被篡改。 sign: 签名，明文+salt后使用散列算法后生成的密文，用于验签确认是数据是完整无篡改的。 2.客户端加密 本地初始化生成一个对称加密的秘钥key，同时随机生成一个salt 将key和salt拼接，使用公钥非对称加密&gt;&gt;&gt;&gt;&gt;&gt;&gt;获得密文KEY 使用key对明文plainText进行对称加密&gt;&gt;&gt;&gt;&gt;&gt;&gt;获得密文content 将密文content和salt进行sha256算法签名&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;获得密文sign 3.服务端解密 使用私钥对密文KEY进行非对称解密&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;获得key和salt 将密文content和salt进行签名，对比sign是否一致：验签&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;确认内容无篡改 使用key对密文content对称解密&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;获得明文plainText 4.CFCA安全控件思路​ App情况下，上述的加解密其实会容易被破解。因为很多黑盒测试可能会破壳或暴力破解app，存在拿到本地秘钥、salt甚至是源码的情况。主流加解密的算法就那么几种，可以试出来。所以如何保证信息安全，其实很大程度上要靠对秘钥的存储方式。 以下是CFCA安全控件的加解密流程： 客户端第一次请求服务端，将必要的自身信息用公钥加密，给服务器 服务端私钥解密后，生成一个随机数Rs，传给客户端，同时储存在Redis中的会话信息里 客户端自己生成一个随机数Ra，计算R=Rs+Ra，将R作为key。 进行正常的客户端加解密 ​ 这种情况中客户端和服务端都参与了对key的生成过程，且Rs和Ra不会在同一个请求中同时出现，加大了破解密码的难度。但使用了Redis中间件，也增加了系统的难度和不稳定性。 ​ 如果不考虑性能，那么甚至使用秘钥集（一次一密），请求过程中全程不传输key。key和salt也可以存储在Redis或内存中。","categories":[],"tags":[{"name":"加解密","slug":"加解密","permalink":"https://18360732385.github.io/tags/%E5%8A%A0%E8%A7%A3%E5%AF%86/"},{"name":"RSA","slug":"RSA","permalink":"https://18360732385.github.io/tags/RSA/"}]},{"title":"微服务的高可用和线程池参数设计","slug":"微服务的高可用和线程池参数设计","date":"2019-10-05T16:00:00.000Z","updated":"2019-10-05T16:00:00.000Z","comments":true,"path":"2019/10/06/微服务的高可用和线程池参数设计/","link":"","permalink":"https://18360732385.github.io/2019/10/06/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E7%9A%84%E9%AB%98%E5%8F%AF%E7%94%A8%E5%92%8C%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%8F%82%E6%95%B0%E8%AE%BE%E8%AE%A1/","excerpt":"微服务架构本身最最核心的保障高可用的措施，就是两点： 一个是基于Hystrix做资源隔离以及熔断；","text":"微服务架构本身最最核心的保障高可用的措施，就是两点： 一个是基于Hystrix做资源隔离以及熔断； 另一个是做备用降级方案。 hystrix资源隔离 ​ 核心服务A调用了核心服务B和C，在核心服务B响应过慢时，会导致核心服务A的某个线程池全部卡死。 ​ 但是此时因为你用了hystrix做了资源隔离，所以核心服务A是可以正常调用服务C的，那么就可以保证用户起码是可以使用APP的部分功能的，只不过跟服务B关联的页面刷不出来，功能无法使用罢了。 优化 要保证一个hystrix线程池可以轻松处理每秒钟的请求 ，一般是sql语句的优化。 同时还有合理的超时时间设置，避免请求太慢卡死线程。 线程池参数的设置。 请求超时后的重试机制。 有重试机制，必须考虑接口的幂等性！！！ ##线程池大小 ​ 在生产环境中，我们到底应该如何设置服务中每个hystrix线程池的大小？ ​ 假设你的服务A，每秒钟会接收30个请求，同时会向服务B发起30个请求，然后每个请求的响应时长经验值大概在200ms，那么你的hystrix线程池需要多少个线程呢？ $$计算公式： 30（每秒请求数量） * 0.2（每个请求的处理秒数） + 4（给点缓冲buffer） = 10（线程数量）。$$ ​ 反推：为什么10个线程可以轻松抗住每秒30个请求？ 请求超时时间 ​ 考虑高并发，线程全部运行的情况，尽量不会导致线程卡死 $$计算公式： 10 (线程数量) / 0.3 (请求超时时间) &gt;= 30 (系统高并发下，每秒请求数量)$$ ​ 考虑极端情况，如果服务B响应变慢，要500毫秒才响应，你一个线程每秒最多只能处理2个请求了，10个线程只能处理20个请求。而每秒是30个请求过来，结局会如何？ ​ 大量的线程会全部卡死，来不及处理那么多请求，最后用户会刷不出来页面。 关于线程池参数设置 https://www.toutiao.com/i6691803530602217988/","categories":[],"tags":[{"name":"微服务","slug":"微服务","permalink":"https://18360732385.github.io/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"线程","slug":"线程","permalink":"https://18360732385.github.io/tags/%E7%BA%BF%E7%A8%8B/"}]},{"title":"加解密算法","slug":"加解密算法","date":"2019-10-04T16:00:00.000Z","updated":"2019-10-04T16:00:00.000Z","comments":true,"path":"2019/10/05/加解密算法/","link":"","permalink":"https://18360732385.github.io/2019/10/05/%E5%8A%A0%E8%A7%A3%E5%AF%86%E7%AE%97%E6%B3%95/","excerpt":"1.散列（哈希） ​ 散列函数，又称散列算法、哈希函数，是一种从任何一种数据中创建小的数字“指纹”的方法。将数据（如一段文字）运算变为另一固定长度值，是散列算法的基础原理，被广泛应用于数字签名。有以下特点：","text":"1.散列（哈希） ​ 散列函数，又称散列算法、哈希函数，是一种从任何一种数据中创建小的数字“指纹”的方法。将数据（如一段文字）运算变为另一固定长度值，是散列算法的基础原理，被广泛应用于数字签名。有以下特点： 对相同数据运算，结果相同 对不同数据运算，结果长度相同 运算单向不可逆 常用算法 : MD5（MD5消息摘要算法） SHA家族（安全散列算法，有sha1、sha256、sha512） 2.对称加密 ​ 这类算法在加密和解密时使用相同的密钥，或是使用两个可以简单地相互推算的密钥。对称加密一般加密后，都使用base64编码。具有以下特点： 计算量小 加密速度快、机密效率高 加密和解密使用相同秘钥，安全隐患大 常用算法: DES（数据加密标准），不安全 3DES（三重数据加密算法） AES（三重数据加密算法） 1密码学中，分组（block）密码的工作模式（mode of operation）允许使用同一个分组密码密钥对多于一块的数据进行加密，并保证其安全性。分组密码自身只能加密长度等于密码分组长度的单块数据，若要加密变长数据，则数据必须先被划分为一些单独的密码块。通常而言，最后一块数据也需要使用合适填充方式将数据扩展到匹配密码块大小的长度。 3.非对称加密 ​ 它需要两个密钥，一个是公开密钥，另一个是私有密钥；一个用作加密的时候，另一个则用作解密。使用其中一个密钥把明文加密后所得的密文，只能用相对应的另一个密钥才能解密得到原本的明文。有以下特点： RSA效率要慢得多，一般加密小量的数据（盐、随机数），所以RSA一般配合对称加密算法一起使用。 对极大整数做因数分解的难度决定了RSA算法的可靠性。安全性可靠。 常用算法： RSA（最广泛的非对称加密算法，麻省理工三人组R、S、A推出） 国密算法 ​ 国密即国家密码局认定的国产密码算法。主要有SM1，SM2，SM3，SM4。密钥长度和分组长度均为128位。其特点分别如下： SM1 为对称加密。其加密强度与AES相当。该算法不公开，调用该算法时，需要通过加密芯片的接口进行调用。 SM2为非对称加密，基于ECC。该算法已公开。由于该算法基于ECC，故其签名速度与秘钥生成速度都快于RSA。ECC 256位（SM2采用的就是ECC 256位的一种）安全强度比RSA 2048位高，但运算速度快于RSA。 SM3 消息摘要。可以用MD5作为对比理解。该算法已公开。校验结果为256位。 SM4 无线局域网标准的分组数据算法。对称加密，密钥长度和分组长度均为128位。","categories":[],"tags":[{"name":"加解密","slug":"加解密","permalink":"https://18360732385.github.io/tags/%E5%8A%A0%E8%A7%A3%E5%AF%86/"},{"name":"国密","slug":"国密","permalink":"https://18360732385.github.io/tags/%E5%9B%BD%E5%AF%86/"}]},{"title":"集合源码解析","slug":"集合源码解析","date":"2019-09-30T16:00:00.000Z","updated":"2019-09-30T16:00:00.000Z","comments":true,"path":"2019/10/01/集合源码解析/","link":"","permalink":"https://18360732385.github.io/2019/10/01/%E9%9B%86%E5%90%88%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/","excerpt":"HashMap ​ HashMap数据结构为数组+链表，jdk8以后，链表数量&gt;8后会转换成红黑树（将链表转换成红黑树前会判断，如果当前数组的长度小于 64，那么会选择先进行数组扩容，而不是转换为红黑树），红黑树节点数量&lt;6退化成链表。","text":"HashMap ​ HashMap数据结构为数组+链表，jdk8以后，链表数量&gt;8后会转换成红黑树（将链表转换成红黑树前会判断，如果当前数组的长度小于 64，那么会选择先进行数组扩容，而不是转换为红黑树），红黑树节点数量&lt;6退化成链表。 插入和扩容原理 插入前判断数组是否为空，为空进行初始化。默认大小16，负载因子0.75; 不为空，计算 k 的 hash 值，通过 (n - 1) &amp; hash计算应当存放在数组中的下标 index ; 查看 table[index] 是否存在数据，没有数据就构造一个Node节点存放在 table[index] 中； 存在数据，说明发生了hash冲突, 继续判断key是否相等，相等，用新的value替换原数据； 如果不相等，判断当前节点类型是不是树型节点，如果是树型节点，创建树型节点插入红黑树中； 如果不是树型节点，创建普通Node加入链表中，然后判断链表长度是否大于 8， 大于的话链表转换为红黑树； 插入完成之后判断当前节点数是否大于阈值（当前大小*0.75），如果大于开始扩容为原数组的二倍。 jdk1.8优化 ​ jdk1.8有以下优化： 数组+链表改成了数组+链表或红黑树； 链表的插入方式从头插法改成了尾插法，简单说就是插入时，如果数组位置上已经有元素，1.7将新元素放到数组中，原始节点作为新节点的后继节点，1.8遍历链表，将元素放置到链表的最后； 扩容的时候，1.7需要对原数组中的元素进行重新hash定位在新数组的位置，1.8采用更简单的判断逻辑，位置不变或索引+旧容量大小； 在插入时，1.7先判断是否需要扩容，再插入，而1.8先进行插入，插入完成再判断是否需要扩容； 这样的好处是： 防止发生hash冲突，链表长度过长，将时间复杂度由O(n)降为O(logn); 因为1.7头插法扩容时，头插法会使链表发生反转，多线程环境下会产生环，死循环； 线程安全 ​ hashMap不是线程安全的。在多线程环境下，1.7 会产生死循环、数据丢失、数据覆盖的问题，1.8 中会有数据覆盖的问题。 多线程下可以使用ConcurrentHashMap ArrayList​ ArrayList 的底层是数组队列，相当于动态数组。与 Java 中的数组相比，它的容量能动态增长。在添加大量元素前，应用程序可以使用ensureCapacity操作来增加 ArrayList 实例的容量。这可以减少递增式再分配的数量。 ​ 创建时不指定长度，则是空数组。当add元素时，会初始化一个默认容量为10的数组，后面如果遇到扩容，会调用grow（）方法，扩容的容量是原先容量的1.5倍。","categories":[],"tags":[{"name":"集合","slug":"集合","permalink":"https://18360732385.github.io/tags/%E9%9B%86%E5%90%88/"},{"name":"HashMap","slug":"HashMap","permalink":"https://18360732385.github.io/tags/HashMap/"}]},{"title":"MySQL乐观锁MVCC机制","slug":"MySQL乐观锁MVCC机制","date":"2019-09-23T16:00:00.000Z","updated":"2019-09-23T16:00:00.000Z","comments":true,"path":"2019/09/24/MySQL乐观锁MVCC机制/","link":"","permalink":"https://18360732385.github.io/2019/09/24/MySQL%E4%B9%90%E8%A7%82%E9%94%81MVCC%E6%9C%BA%E5%88%B6/","excerpt":"1.MVCC机制 ​ MVCC，Multi-Version Concurrency Control，多版本并发控制。MySQL使用的事务存储引擎InnoDB，默认的隔离级别REPETABLR READ采用是乐观锁，而乐观锁的实现采用的就是MVCC。正是因为有了MVCC，才造就了InnoDB强大的事务处理能力。","text":"1.MVCC机制 ​ MVCC，Multi-Version Concurrency Control，多版本并发控制。MySQL使用的事务存储引擎InnoDB，默认的隔离级别REPETABLR READ采用是乐观锁，而乐观锁的实现采用的就是MVCC。正是因为有了MVCC，才造就了InnoDB强大的事务处理能力。 2.乐观锁与悲观锁 ​ 一个读写事务在运行的过程中在访问数据之前先加读/写锁这种实现叫做悲观锁，悲观体现在，先加锁，独占数据，防止别人加锁。 ​ 乐观锁读写事务，在真正的提交之前，不加读/写锁，而是先看一下数据的版本/时间戳，等到真正提交的时候再看一下版本/时间戳，如果两次相同，说明别人期间没有对数据进行过修改，那么就可以放心提交。 ​ 在资源冲突不激烈的场合，用乐观锁性能较好。如果资源冲突严重，乐观锁会急剧消耗性能，不如悲观锁。 ​ 所以通常我们把没有开启MVCC特性的，使用原来的锁机制来保证数据一致性的这种锁叫悲观锁，而对开启MVCC机制的锁，叫做乐观锁。 3.MVCC机制分析 ​ InnoDB的MVCC,是通过在每行记录后面保存两个隐藏的列来实现的,这两个列，分别保存了这个行的创建时间，一个保存的是行的删除时间。这里存储的并不是实际的时间值,而是系统版本号(可以理解为事务的ID)，每开始一个新的事务，系统版本号就会自动递增，事务开始时刻的系统版本号会作为事务的ID。 ​ 我们可以创建表yang，并从下面4种sql语句观察一下MVCC机制。 123create table yang(id int primary key auto_increment,name varchar(20)); INSERT12345start transaction;insert into yang values(NULL,&#39;yang&#39;) ;insert into yang values(NULL,&#39;long&#39;);insert into yang values(NULL,&#39;fei&#39;);commit; 对应在数据中的表如下(后面两列是隐藏列,我们通过查询语句并看不到) id name 创建时间（事务ID） 删除时间（事务ID） 1 yang 1 undefined 2 long 1 undefined 3 fei 1 undefined SELECT ​ InnoDB会根据以下两个条件检查每行记录: InnoDB只会查找版本早于当前事务版本的数据行(也就是,行的系统版本号小于或等于事务的系统版本号)，这样可以确保事务读取的行，要么是在事务开始前已经存在的，要么是事务自身插入或者修改过的。 行的删除版本要么未定义,要么大于当前事务版本号,这可以确保事务读取到的行，在事务开始之前未被删除。 只有a,b同时满足的记录，才能返回作为查询结果。 其他部分查看 https://www.cnblogs.com/moershiwei/p/9766916.html","categories":[],"tags":[{"name":"MySql","slug":"MySql","permalink":"https://18360732385.github.io/tags/MySql/"}]},{"title":"分布式锁和分布式事务概念","slug":"分布式锁和分布式事务概念","date":"2019-09-22T16:00:00.000Z","updated":"2019-09-22T16:00:00.000Z","comments":true,"path":"2019/09/23/分布式锁和分布式事务概念/","link":"","permalink":"https://18360732385.github.io/2019/09/23/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%92%8C%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E6%A6%82%E5%BF%B5/","excerpt":"锁 ​ 单进程的系统中，存在多线程同时操作一个公共变量，此时需要加锁对变量进行同步操作，保证多线程的操作线性执行消除并发修改。解决的是单机中的多线程并发问题。","text":"锁 ​ 单进程的系统中，存在多线程同时操作一个公共变量，此时需要加锁对变量进行同步操作，保证多线程的操作线性执行消除并发修改。解决的是单机中的多线程并发问题。 分布式锁 ​ 只要应用场景是在集群模式的多个相同服务，可能会部署在不同机器上，解决进程间安全问题，防止多进程同时操作一个变量或者数据库数据。解决的是多台服务器中的多线程的并发问题。 ​ 主流的分布式锁有3种，zookeeper、数据库锁（悲观锁和乐观锁）和redis分布式锁。 数据库锁 ​ 悲观锁就是采用行锁，for update ,属于排他锁，其他任何写操作都要等当前操作完成，性能差，并发上来都要等操作完成，需要设置setAutoCommit。​ 乐观锁就是通过添加版本号，在update 时比较版本号方式更新，比如version，不被阻塞。 Redis分布式锁 ​ 最流行的是redis分布式锁。通过高版本redis的原子性命令和Lua脚本命令实现完整的分布式锁，并且可以通过注解来加锁。成熟的解决方案是Redission（redlock被锤了）。 Zookeeper分布式锁 ​ 查看Zookeeper入门认知。 ​ redis和zookeeper分布式锁对比：https://www.jianshu.com/p/f8a8e49362dc 事务 ​ 解决一个会话过程中，上下文的修改对所有数据库表的操作要么全部成功，要不全部失败。所以应用在service层。解决的是一个会话中的操作的数据一致性。 分布式事务 ​ 原则：优先使用本地事务，将操作优先在一个本地事务中完成，无法使用本地事务的采用分布式事务。 ​ 场景：解决一个联动操作，比如一个商品的买卖分为: ​ （1）添加商品到购物车 ​ （2）修改商品库存减1 ​ （3）增加积分 ​ （4）通知物流服务 ​ 此时购物车服务和商品库存服务可能部署在多台台电脑，这时候需要保证对四个服务的操作都全部成功或者全部回退。解决的是组合服务的数据操作的一致性问题。 全局事务 ​ 全局事务（DTP模型），基于2PC、3PC（两段和三段提交）实现，但由于同步阻塞，处理效率低，不适合大型网站分布式场景。​ 原则：ACID，刚性事务。​ 2pc：一阶段预提交，二阶段提交，都成功情况才会成功。 柔性事务 ​ 原则：基本可用，最终一致，即BASE理论。 ​ 关于柔性事务，最主要的有以下三种类型：异步确保型、补偿型、最大努力通知型。 ​ 幂等性：参数列表对比，是否重复请求。 ​ 可补偿：保证原子性。 ##分布式事务解决方案 TCC方案 ​ TCC方案在电商、金融领域落地较多。TCC方案其实是两阶段提交的一种改进。其将整个业务逻辑的每个分支显式的分成了Try、Confirm、Cancel三个操作。Try部分完成业务的准备工作，confirm部分完成业务的提交，cancel部分完成事务的回滚。基本原理如下图所示。 ​ TCC方案让应用自己定义数据库操作的粒度，使得降低锁冲突、提高吞吐量成为可能。但TCC开发成本高的缺点导致它并不适用于微服务，集中表现在以下两个方面： 对应用的侵入性强。业务逻辑的每个分支都需要实现try、confirm、cancel三个操作，应用侵入性较强，改造成本高。 实现难度较大。需要按照网络状态、系统故障等不同的失败原因实现不同的回滚策略。为了满足一致性的要求，confirm和cancel接口必须实现幂等。 基于消息的最终一致性方案 ​ 消息一致性方案是通过消息中间件保证上、下游应用数据操作的一致性。基本思路是将本地操作和发送消息放在一个事务中，保证本地操作和消息发送要么两者都成功或者都失败。下游应用向消息系统订阅该消息，收到消息后执行相应操作。 ​ 这其实是一种避免事务的方案。具体讲解，请看《可靠消息最终一致性方案》。 基于微服务的分布式事务 ​ 阿里的GTS分布式事务解决方案，演化成了开源项目: fescar/seate。 ​ https://seata.io/zh-cn/docs/overview/what-is-seata.html","categories":[{"name":"分布式","slug":"分布式","permalink":"https://18360732385.github.io/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"}],"tags":[{"name":"事务","slug":"事务","permalink":"https://18360732385.github.io/tags/%E4%BA%8B%E5%8A%A1/"}]},{"title":"分布式系ID方案","slug":"分布式系ID方案","date":"2019-09-21T16:00:00.000Z","updated":"2019-09-21T16:00:00.000Z","comments":true,"path":"2019/09/22/分布式系ID方案/","link":"","permalink":"https://18360732385.github.io/2019/09/22/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BBID%E6%96%B9%E6%A1%88/","excerpt":"1—UUID ​ 优点： 代码实现简单","text":"1—UUID ​ 优点： 代码实现简单 本机生成，没有性能问题 因为是全球唯一的ID，所以迁移数据容易 ​缺点： 每次生成的ID是无序的，无法保证趋势递增 UUID的字符串存储，查询效率慢 存储空间大 ID本事无业务含义，不可读 ​ 应用场景： 类似生成token令牌的场景 不适用一些要求有趋势递增的ID场景 ###2—获取系统当前时间 ​ 高并发下，获取系统当前时间可能会存在重复情况，但是我们可以考虑时间跟很多其他的业务字段拼接起来，作为一个全局唯一id 。 ​ 你可以将别的业务字段值跟当前时间拼接起来，组成一个全局唯一的编号，比如说订单编号： ​ 时间戳 + 用户id + 业务含义编码。 ​ 这个方案就是说你每次要获取一个全局唯一id，都往这张表里插一条记录，然后获取一个数据库自增的一个id。然后这个全局唯一id就可以插入订单的分库分表中。 ​ 但这种方案中的拼接字段需要好好思考，并且不适用针对单一对象的高并发情况。例如双十一中某件商品秒杀，一秒内卖出1万件商品（1毫秒就是10件），如果你使用的是 时间戳 + 商品id + 业务编码 ，那么就会存在10个重复的订单id。 3—mysql多实例主键自增优缺点 ​ 这个方案就是解决mysql的单点问题，在auto_increment基本上面，设置step步长 。 ​ 优点：解决了单点问题 ​ 缺点：一旦把步长定好后，就无法扩容；而且单个数据库的压力大，数据库自身性能无法满足高并发 优化思路 ​ 这个方案对数据库的压力很大，是因为每次生成id都会去请求数据库 ，那我们可不可以不用每次都去数据库取 ？ ​ 我们的思路是把每次去数据库取一个id 设计成 每次取一个id区间段 。当然这个方案里有很多需要解决的问题，具体可以看这篇文章。 ​ https://www.toutiao.com/i6682672464708764174/?group_id=6682672464708764174 ###4—Redis生成 优缺点 ​ 利用redis的incr原子性操作自增，一般算法为：年份 + 当天距当年第多少天 + 天数 + 小时 + redis自增 ​ 优点： 有序递增，可读性强 ​ 缺点： 占用带宽，每次要向redis进行请求 对redis依赖过高，万一挂了，id生成服务就挂了，必须做集群 由于redis自增，所以可以猜到下一个id是多少，安全性需要考虑 性能 ​ 需求：同时10万个请求获取ID 并发执行完耗时：9s左右 单任务平均耗时：74ms 单线程最小耗时：不到1ms 单线程最大耗时：4.1s 5—snowflake算法 ​ 雪花算法的核心思想就是：使用一个64 bit的long型的数字作为全局唯一id，这64个bit中，首位的第1个bit是不用的，然后用其中的41 bit作为毫秒数，用10 bit作为工作机器id，12 bit作为序列号。 1位标识符：始终是0，表示正数（1表示负数），无意义。 41位时间戳：41位时间截不是存储当前时间的时间截，而是存储时间截的差值（当前时间截 - 开始时间截 )得到的值，这里的的开始时间截，一般是我们的id生成器开始使用的时间，由我们程序来指定的 10位机器标识码：可以部署在1024（2 ^ 10 - 1）个节点，如果机器分机房（IDC）部署，这10位可以分成两部分 ​ 前5位机房ID + 后5位机器ID 12位序列：毫秒内的计数，就是某个机房某台机器上这一毫秒内同时生成的id的序号。12位的计数顺序号支持每个节点每毫秒(同一机器，同一时间截)产生4096（2 ^ 12 - 1）个ID序号。 优缺点 ​ 优点： 此方案每秒能够产生409.6万个ID，性能快 时间戳在高位，自增序列在低位，整个ID是趋势递增的，按照时间有序递增 灵活度高，可以根据业务需求，调整bit位的划分，满足不同的需求 ​ 缺点： 依赖机器的时钟，如果服务器时钟回拨，会导致重复ID生成 在分布式场景中，服务器时钟回拨会经常遇到，一般存在10ms之间的回拨 。雪花算法就是建立在毫秒级别的生成方案，一旦回拨，就很有可能存在重复ID。 优化思路 ​ 利用zookeeper注册节点，定时比较各个ID生成服务器的时钟，避免服务器时钟回拨问题。 ​ 百度开源的分布式ID生成器UidGenerator，基于雪花算法，同时解决了服务器时钟回拨问题，可以参考一下： ​ https://mp.weixin.qq.com/s/o-bpv_C3t5B_GKcwF_KZ5w","categories":[{"name":"分布式","slug":"分布式","permalink":"https://18360732385.github.io/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"}],"tags":[{"name":"ID生成","slug":"ID生成","permalink":"https://18360732385.github.io/tags/ID%E7%94%9F%E6%88%90/"}]},{"title":"分布式系统开发设计","slug":"分布式系统开发设计","date":"2019-09-21T16:00:00.000Z","updated":"2019-09-21T16:00:00.000Z","comments":true,"path":"2019/09/22/分布式系统开发设计/","link":"","permalink":"https://18360732385.github.io/2019/09/22/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%BC%80%E5%8F%91%E8%AE%BE%E8%AE%A1/","excerpt":"###（1）分布式服务框架 ​ 你如果要让不同的子系统或者服务之间互相通信，首先必须有一套分布式服务框架。也就是各个服务可以互相感知到对方在哪里，可以发送请求过去，可以通过HTTP或者RPC的方式。","text":"###（1）分布式服务框架 ​ 你如果要让不同的子系统或者服务之间互相通信，首先必须有一套分布式服务框架。也就是各个服务可以互相感知到对方在哪里，可以发送请求过去，可以通过HTTP或者RPC的方式。 ​ 在这里，最常见的技术就是dubbo以及spring cloud，当然大厂一般都是自己有服务框架。 ​ 关于微服务的技术点，看这里：https://blog.csdn.net/lissic_blog/article/details/81216508 （2）分布式事务 ​ 一旦你的系统拆分为了多个子系统之后，那么一个贯穿全局的分布式事务应该怎么来实现？这个你需要了解TCC、最终一致性、2PC等分布式事务的实现方案和开源技术。 （3）分布式锁 ​ 不同的系统之间如果需要在全局加锁获取某个资源的锁定，此时应该怎么来做？毕竟大家不是在一个JVM里了，不可能用synchronized来在多个子系统之间实现锁吧，是不是？ （4）分布式缓存 ​ 如果你原来就是个单块系统，那么你其实是可以在单个JVM里进行本地缓存就可以了，比如搞一个HashMap来缓存一些数据。但是现在你有很多个子系统，他们如果要共享一个会话，你应该怎么办？是不是需要引入Redis等缓存系统？ （5）分布式消息系统 ​ 在单块系统内，就一个JVM进程内部，你可以用类似LinkedList之类的数据结构作为一个本地内存里的队列。但是多个子系统之间要进行消息队列的传递呢？那是不是要引入类似RabbitMQ之类的分布式消息中间件？ （6）分布式搜索系统 ​ 如果在单块系统内，你可以比如在本地就基于Lucene来开发一个全文检索模块，但是如果是分布式系统下的很多子系统，你还能直接基于Lucene吗？ ​ 明显不行，你需要在系统里引入一个外部的分布式搜索系统，比如Elasticsearch。 （7）其他很多的技术 ​ 比如说分布式配置中心、分布式日志中心、分布式监控告警中心、分布式会话，等等，都是分布式系统场景下你需要使用和了解的一些技术。","categories":[{"name":"分布式","slug":"分布式","permalink":"https://18360732385.github.io/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"}],"tags":[{"name":"分布式","slug":"分布式","permalink":"https://18360732385.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"}]},{"title":"微服务、分布式和集群的异同点","slug":"微服务、分布式和集群的异同点","date":"2019-09-21T16:00:00.000Z","updated":"2019-09-21T16:00:00.000Z","comments":true,"path":"2019/09/22/微服务、分布式和集群的异同点/","link":"","permalink":"https://18360732385.github.io/2019/09/22/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E3%80%81%E5%88%86%E5%B8%83%E5%BC%8F%E5%92%8C%E9%9B%86%E7%BE%A4%E7%9A%84%E5%BC%82%E5%90%8C%E7%82%B9/","excerpt":"集群 ​ 集群是一种系统部署方式。 ​ 集群是指将多台服务器集中在一起，每台服务器都是一个完整的单体应用 ，实现了相同的业务，做相同的事情。这也保","text":"集群 ​ 集群是一种系统部署方式。 ​ 集群是指将多台服务器集中在一起，每台服务器都是一个完整的单体应用 ，实现了相同的业务，做相同的事情。这也保 证每台服务器并不是缺一不可 ，存在的作用主要是缓解并发压力和单点故障转移问题。 ​ 集群主要具有以下特性：伸缩性、高可用性、负载均衡、高性能 。 分布式 ​ 分布式是一种系统部署方式。 ​ 分布式服务是指将多台服务器集中在一起，服务是分散部署在不同的机器上的。每台服务器都实现总体中的不同业务，做不同的事情。一个服务可能负责几个功能，是一种面向 SOA 的架构。 各分开部署的部分彼此通过各种通讯协议交互信息，并且可能每台服务器都缺一不可 ，如果某台服务器故障，则部分功能缺失，或导致整体无法运行。 微服务 ​ 微服务并不是一种架构，它是一种架构风格。 微服务是将一个大型应用按服务划分，部署在服务器上。微服务与分布式的细微差别是，微服务的应用不一定是分散在多个服务器上，它也可以是同一个服务器。 ​ 微服务相比分布式服务来说，它的粒度更小，服务之间耦合度更低。 由于每个微服务都由独立的小团队负责，因此它敏捷性更高。分布式服务最后都会向微服务架构演化，这是一种趋势。 集群和分布式的差异和共性1.分布式是以缩短单个任务的执行时间来提升效率的，而集群则是通过提高单位时间内执行的任务数来提升效率 ​ 例如：如果一个任务由 10 个子任务组成，每个子任务单独执行需 1 小时，则在一台服务器上执行该任务需 10 小时。 采用分布式方案，提供 10 台服务器，每台服务器只负责处理一个子任务，不考虑子任务间的依赖关系，执行完这个任务只需一个小时。(这种工作模式的一个典型代表就是 Hadoop 的 Map/Reduce 分布式计算模型） 采用集群方案，同样提供 10 台服务器，每台服务器都能独立处理这个任务。假设有 10 个任务同时到达，10 个服务器将同时工作，1 小时后，10 个任务同时完成。这样整体来看，还是 1 小时内完成一个任务。 2.集群模式是不同服务器部署同一套服务对外访问，实现服务的负载均衡 ​ 区别集群的方式是部署多台服务器上的业务，是否相同。分布式中的每一个节点，都可以做集群。而集群并不一定就是分布式的。 3.生产环境中的集群和分布式 ​ 分布式架构的主要功能是用来将我们的系统模块化，将系统进行解耦的。而集群解决的问题是，高并发情况下也能保证服务器如果出现一定数量的宕机后，系统仍然可以正常运转。 ​ 好的设计应该是分布式和集群相结合，先分布式再集群。具体实现就是业务拆分成很多子业务，然后针对每个子业务进行集群部署。 微服务和分布式的异同点1.微服务是架构设计方式，分布式是系统部署方式，两者概念不同。 ​ 从概念理解，分布式服务架构强调的是服务化以及服务的分散化 ，微服务则更强调服务的专业化和精细分工。 ​ 从实践的角度来看，符合微服务的架构通常是分布式服务架构，反之未必，即微服务的架构是分布式架构的子集。 2.微服务的架构是SOA架构的升华 SOA是一种分布式架构，把业务系统分成多个子系统，提供不同的服务，再通过服务组合、编排实现业务流程；通常在SOA架构中，ESB企业服务总线扮演了重要的角色。 微服务是SOA的升华，技术上与分布式的区别：docker技术、去ESB总线、去中心化，部署粒度更细，服务扩展更灵活。","categories":[{"name":"分布式","slug":"分布式","permalink":"https://18360732385.github.io/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"微服务","slug":"微服务","permalink":"https://18360732385.github.io/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"}],"tags":[{"name":"微服务","slug":"微服务","permalink":"https://18360732385.github.io/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"}]},{"title":"分布式事务方案","slug":"分布式事务方案","date":"2019-09-20T16:00:00.000Z","updated":"2019-09-21T16:00:00.000Z","comments":true,"path":"2019/09/21/分布式事务方案/","link":"","permalink":"https://18360732385.github.io/2019/09/21/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E6%96%B9%E6%A1%88/","excerpt":"两阶段提交（2PC） ​ 2PC通过引入协调者（Coordinator）来协调参与者的行为，并最终决定这些参与者是否要真正执行事务。两阶段是准备阶段和提交阶段。","text":"两阶段提交（2PC） ​ 2PC通过引入协调者（Coordinator）来协调参与者的行为，并最终决定这些参与者是否要真正执行事务。两阶段是准备阶段和提交阶段。 1.准备阶段​ 参与者在本地执行自己的事务，协调者询问参与者事务是否执行成功，参与者发回事务执行结果。 2.提交阶段​ 如果事务在每个参与者上都执行成功，事务协调者发送通知让参与者提交事务；否则只要有一个参与者事务是失败的，那么协调者都发送通知让参与者回滚事务。 注意：​ 在准备阶段，参与者执行了事务，但是还未提交。只有在提交阶段接收到协调者发来的通知后，才进行提交或者回滚。 问题 同步阻塞。所有事务，参与者在等待其它参与者响应的时候都处于同步阻塞状态，无法进行其它操作。 单点问题 。协调者在 2PC 中起到非常大的作用，发生故障将会造成很大影响。特别是在提交阶段发生故障，所有参与者会一直等待状态，无法完成其它操作。 数据不一致 。在阶段二，如果协调者只发送了部分 Commit 消息，此时网络发生异常，那么只有部分参与者接收到 Commit 消息，也就是说只有部分参与者提交了事务，使得系统数据不一致。 太过保守。 任意一个节点失败就会导致整个事务失败，没有完善的容错机制。 ​ 三阶段提交协议（3PC） ​ 由于2PC的一系列问题，出现了3PC解决方案。为了解决同步阻塞和单点问题 ，3PC有2个改动点： 引入超时机制。同时在协调者和参与者中都引入超时机制，避免了参与者在长时间无法与协调者节点通讯（协调者挂掉了）的情况下，无法释放资源的问题。这种机制也侧面降低了整个事务的阻塞时间和范围。 3个阶段。在第一阶段和第二阶段中插入一个准备阶段。保证了在最后提交阶段之前各参与节点的状态是一致的。多设置了一个缓冲阶段保证了在最后提交阶段之前各参与节点的状态是一致的。 1.CanCommit阶段​ 之前2PC的一阶段是本地事务执行结束后，最后不Commit，等其它服务都执行结束并返回Yes，由协调者发生commit才真正执行commit。 ​ 而这里的CanCommit指的是 尝试获取数据库锁 如果可以，就返回Yes。 2.PreCommit阶段​ 在阶段一中，如果所有的参与者都返回Yes的话，那么就会进入PreCommit阶段进行事务预提交。 ​ 这里的PreCommit阶段 跟上面的第一阶段是差不多的，只不过这里 协调者和参与者都引入了超时机制 （2PC中只有协调者可以超时，参与者没有超时机制）。 3.DoCommit阶段​ 这里跟2pc的阶段二是差不多的。 问题 3PC依然没有完全解决数据不一致的问题 实现难度大，代码定制要求高 补偿事务（TCC）​ TCC方案的核心思想是：针对每个操作，都要注册一个与其对应的确认和补偿（撤销）操作。 1.try阶段​ 主要是对业务系统做检测及资源预留，比如对需要操作的数据库资源进行加锁或锁定操作。 2.Confirm 阶段​ 主要是对业务系统做确认提交，Try阶段执行成功并开始执行 Confirm阶段时，默认 Confirm阶段是不会出错的。即只要Try成功，Confirm一定成功。 ​ 这个阶段需要在各个服务中引入 TCC 分布式事务框架，目的是感知各个阶段的执行情况以及推进执行下一个阶段。 3.Cancel 阶段：​ 主要是在业务执行错误，需要回滚的状态下执行的业务取消，预留资源释放。 问题 成本太高，需要代码定制。适用于严格一致性要求、执行时间较短的业务，比如处理账户或者收费等等。 需要保留业务活动的日志，以便追溯定位问题。 需要引入TCC 分布式事务框架 链接 ​ https://www.cnblogs.com/jajian/p/10014145.html 最大努力通知​ 利用MQ消息队列，实现最大努力通知 最终一致性方案 ​ 在实际系统的开发过程中，可能服务间的调用是异步的。一个服务发送一个消息给 MQ，然后另外一个服务从 MQ 消费到一条消息后进行处理。这就成了基于 MQ 的异步调用了。 ​ 针对这种基于 MQ 的异步调用，如何保证各个服务间的分布式事务呢？也就是说，我希望的是基于 MQ 实现异步调用的多个服务的业务逻辑，要么一起成功，要么一起失败。 ​ 这个时候，就要用上可靠消息最终一致性方案，来实现分布式事务。 ​ 具体查看《可靠消息最终一致性方案》。","categories":[{"name":"分布式","slug":"分布式","permalink":"https://18360732385.github.io/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"}],"tags":[{"name":"分布式","slug":"分布式","permalink":"https://18360732385.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"事务","slug":"事务","permalink":"https://18360732385.github.io/tags/%E4%BA%8B%E5%8A%A1/"}]},{"title":"秒杀系统","slug":"秒杀系统","date":"2019-08-31T16:00:00.000Z","updated":"2019-08-31T16:00:00.000Z","comments":true,"path":"2019/09/01/秒杀系统/","link":"","permalink":"https://18360732385.github.io/2019/09/01/%E7%A7%92%E6%9D%80%E7%B3%BB%E7%BB%9F/","excerpt":"1.临近开场的时候，查看商品的请求会猛增。怎么让商品详情业务能够hold住这么大量的请求呢？ ​ 静态页面+nginx+CDN","text":"1.临近开场的时候，查看商品的请求会猛增。怎么让商品详情业务能够hold住这么大量的请求呢？ ​ 静态页面+nginx+CDN 2.校验库存 ​ 新建一个秒杀商品，肯定是通过后台管理系统操作的，添加秒杀商品后，应该会有一个类似按钮【上线发布】。这里的代码逻辑应该将库存放在缓存中 3.扣库存 ​ 秒杀系统中的扣库存，会出现超卖现象，这是因为高并发下多线程对库存操作的原因。可以利用锁解决： 分布式锁redis/zookepper redis队列+数据库悲观锁/乐观锁 秒杀系统的库存问题，不建议直接使用数据库锁 4.合适扣库存 ​ 扣库存的时机一般有2种方案： 1、方案一：下单减库存，即当买家下单后，商品库存减去买家购买的数量 2、方案二：付款时库存，即当买家下单后，并不立即减库存，而是等用户付款后才减库存 ​ 生产环境中都会选择方案一。限制支付时间，同时加入定时任务，超出时间设防库存，防止恶意下单不支付。 5.分布式限流 ​ 秒杀过程中有90%的请求都是无效的。100万人参与10万件商品的秒杀，90万的请求都可以在网关或是业务系统前拦截。这里可以使用nginx限流、漏斗法、令牌桶算法等算法限制流量。 ​ 还有一种情况，就是并发量太大，你连接口都进不去，这时候你就需要在网关处把部分请求给刷下来，规则可以自己定，但是不能耍猴哦！ ​ 假设秒杀库存是 200 个，我们可以只放行 200 个请求到后端服务。 ​ 要注意，为了尽量避免库存被机器人和自动脚本抢走，200 个请求不能在秒杀开始瞬间同时放行，可以分段放行，比如秒杀开始后随机选取 100ms 内的 5 个请求放行（这 100ms 内的其他请求直接拒掉，按秒杀失败处理），之后每隔 100ms 放行 5 个请求，4 秒钟可以放行完 200 个请求。分段放行，除了限制了机器人和自动脚本，把请求分散在各个时间段，还进一步缓解了后端服务的压力。 6.异步化 ​ 我们大家想一想，即使我们做了上面的限流机制，卡住了绝大部分请求到业务系统中，但还是有很多请求进入了业务系统。如果我们商品库存数为5000个库存，在大流量的时候，会有很多用户都有资格去下单，那也就是同时会有5000个并发去操作mysql数据库。那数据库也是抗不住的，一般mysql的并发量150左右。 ​ 这个时候我们需要做一些异步处理，让5000个下单请求进入消息队列，让订单消费服务去慢慢处理5000个请求，这样就有效的把并发同步请求，改为了串行异步。 ​ 因为做了串行异步化，用户在下单页面可能有短暂的延时，所以前端页面需要给用户提示【下单中…】的优化页面。 总结​ 其实这种大流量的系统，解决的核心思想就是： 1、尽量把请求拦截在最外层，可以逐级拦截请求：网关层、controller参数校验 2、保护数据库资源，尽量采用缓存：甚至可以将库存放入缓存 3、同步操作异步化 4、尽早失败，保护业务系统 5、分段锁 链接 ​ https://mp.weixin.qq.com/s/Mo_knIRBQQL2s-D2aieZLg ​ https://mp.weixin.qq.com/s/ijw9_TudDTYg7Nyrc2NmMA","categories":[],"tags":[{"name":"并发","slug":"并发","permalink":"https://18360732385.github.io/tags/%E5%B9%B6%E5%8F%91/"},{"name":"秒杀","slug":"秒杀","permalink":"https://18360732385.github.io/tags/%E7%A7%92%E6%9D%80/"}]},{"title":"MQ的高可用的降级方案","slug":"MQ的高可用的降级方案","date":"2019-08-24T16:00:00.000Z","updated":"2019-08-24T16:00:00.000Z","comments":true,"path":"2019/08/25/MQ的高可用的降级方案/","link":"","permalink":"https://18360732385.github.io/2019/08/25/MQ%E7%9A%84%E9%AB%98%E5%8F%AF%E7%94%A8%E7%9A%84%E9%99%8D%E7%BA%A7%E6%96%B9%E6%A1%88/","excerpt":"​ 项目中依靠的MQ一旦完全不可用，就会导致业务系统的各个服务之间无法通过MQ来投递消息，导致业务流程中断。 ​ 对于一些核心业务，我们有必要设计一套针对MQ的高可用的降级方案。","text":"​ 项目中依靠的MQ一旦完全不可用，就会导致业务系统的各个服务之间无法通过MQ来投递消息，导致业务流程中断。 ​ 对于一些核心业务，我们有必要设计一套针对MQ的高可用的降级方案。 基于kv存储中队列的降级方案 ​ 由于redis本身就支持队列的功能，还有类似队列的各种数据结构，所以你可以将消息写入kv存储格式的队列数据结构中去。（这里没有考虑高并发情况） ​ 第一，任何kv存储的集合类数据结构，建议不要往里面写入数据量过大，否则会导致大value的情况发生，引发严重的后果。因此绝不能在redis里搞一个key，就拼命往这个数据结构中一直写入消息，这是肯定不行的。 ​ 第二，绝对不能往少数key对应的数据结构中持续写入数据，那样会导致热key的产生，也就是某几个key特别热。大家要知道，一般kv集群，都是根据key来hash分配到各个机器上的，你要是老写少数几个key，会导致kv集群中的某台机器访问过高，负载过大。 下游服务消费MQ的降级感知 ​ 下游服务消费MQ也是通过自行封装的组件来做的，此时那个组件如果从zk感知到降级开关打开了，首先会判断自己是否还能继续从MQ消费到数据？ ​ 如果不能了，就开启多个线程，并发的从kv存储的各个预设好的上百个队列中不断的获取数据。 ​ 每次获取到一条数据，就交给下游服务的业务逻辑来执行。 ​ 通过这套机制，就实现了MQ故障时候的自动故障感知，以及自动降级。如果系统的负载和并发不是很高的话，用这套方案大致是没没问题的。 故障的自动恢复 ​ 如果降级开关打开之后，自行封装的组件需要开启一个线程，每隔一段时间尝试给MQ投递一个消息看看是否恢复了。 ​ 如果MQ已经恢复可以正常投递消息了，此时就可以通过zk关闭降级开关，然后可靠消息服务继续投递消息到MQ，下游服务在确认kv存储的各个队列中已经没有数据之后，就可以重新切换为从MQ消费消息。","categories":[],"tags":[{"name":"中间件","slug":"中间件","permalink":"https://18360732385.github.io/tags/%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"消息队列","slug":"消息队列","permalink":"https://18360732385.github.io/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"}]},{"title":"链表实现栈","slug":"链表实现栈","date":"2019-08-22T16:00:00.000Z","updated":"2019-08-22T16:00:00.000Z","comments":true,"path":"2019/08/23/链表实现栈/","link":"","permalink":"https://18360732385.github.io/2019/08/23/%E9%93%BE%E8%A1%A8%E5%AE%9E%E7%8E%B0%E6%A0%88/","excerpt":"​ 编程思想：链表栈中包括结点类和栈顶结点，初始化时将栈顶结点指向null 1、添加结点时将新加入的结点指向原本的栈顶结点，将新的栈顶结点指向新插入的节点","text":"​ 编程思想：链表栈中包括结点类和栈顶结点，初始化时将栈顶结点指向null 1、添加结点时将新加入的结点指向原本的栈顶结点，将新的栈顶结点指向新插入的节点 2、出栈时栈顶出栈并将新栈顶结点指向原栈顶结点 1234567891011121314public class ListStack&lt;T&gt; &#123; private class Node&lt;T&gt;&#123; private T data; private Node&lt;T&gt; next; public Node(T t,Node&lt;T&gt; next)&#123; this.data&#x3D;t; this.next&#x3D;next; &#125; &#125; private Node&lt;T&gt; top; public ListStack()&#123; top&#x3D;null; &#125; &#125; 1234567891011121314151617181920212223242526272829303132333435363738 &#x2F;** * 方法描述: 向栈中添加新的结点元素 * 1、new出一个新的节点并它的next指向原本的top结点 * 2、将新的top结点指向新插入的节点 *&#x2F; public void push(T t)&#123; Node&lt;T&gt; node&#x3D;new Node&lt;T&gt;(t,top); top&#x3D;node; &#125; &#x2F;** * 方法描述:弹出栈顶元素 * 1、栈顶出栈并将新栈顶结点指向原栈顶结点 *&#x2F; public T pop() throws Exception&#123; if(top&#x3D;&#x3D;null)&#123; throw new Exception(&quot;栈为空，元素不可出栈&quot;); &#125; Node&lt;T&gt; node&#x3D;top; top&#x3D;top.next; return node.data; &#125; &#x2F;** * * 方法描述:遍历打印栈 *&#x2F; public void printfStack()&#123; Node&lt;T&gt; node&#x3D;top; while(node!&#x3D;null)&#123; System.out.println(node.data); node&#x3D;node.next; &#125; &#125;&#125; 测试1234567891011public static void main(String[] args) throws Exception &#123; ListStack&lt;String&gt; list&#x3D;new ListStack&lt;String&gt;(); list.push(&quot;1&quot;); list.push(&quot;2&quot;); list.push(&quot;3&quot;); System.out.println(list.pop()); System.out.println(list.pop()); System.out.println(list.pop()); System.out.println(list.pop()); &#x2F;&#x2F;list.printfStack();&#125;","categories":[],"tags":[{"name":"算法","slug":"算法","permalink":"https://18360732385.github.io/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"排序算法","slug":"排序算法","date":"2019-08-20T16:00:00.000Z","updated":"2019-08-20T16:00:00.000Z","comments":true,"path":"2019/08/21/排序算法/","link":"","permalink":"https://18360732385.github.io/2019/08/21/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/","excerpt":"本文的基础： https://mp.weixin.qq.com/s/dyRTXwsmUmFcF9PeFR8OrQ 图解十大排序： https://www.toutiao.com/i6626655319999119876/","text":"本文的基础： https://mp.weixin.qq.com/s/dyRTXwsmUmFcF9PeFR8OrQ 图解十大排序： https://www.toutiao.com/i6626655319999119876/ 十大排序动画：https://www.toutiao.com/i6630560211402752516/ 冒泡排序 ​ 相邻的值两两比较大小，满足条件则交换。一个循环下来，每次都能找到一个第 *i *大（小）的值。 ​ 冒泡排序属于交换排序，复杂性：稳定。 ​ 当数据越接近正序时，冒泡排序性能越好。 12345678910111213141516171819public void bubbleSort(int[] list) &#123; int temp &#x3D; 0; &#x2F;&#x2F; 用来交换的临时数 &#x2F;&#x2F; 要遍历的次数 for (int i &#x3D; 0; i &lt; list.length - 1; i++) &#123; &#x2F;&#x2F; 从后向前依次的比较相邻两个数的大小，遍历一次后，把数组中第i小的数放在第i个位置上 for (int j &#x3D; list.length - 1; j &gt; i; j--) &#123; &#x2F;&#x2F; 比较相邻的元素，如果前面的数大于后面的数，则交换 if (list[j - 1] &gt; list[j]) &#123; temp &#x3D; list[j - 1]; list[j - 1] &#x3D; list[j]; list[j] &#x3D; temp; &#125; &#125; System.out.format(&quot;第 %d 趟：\\t&quot;, i); printAll(list); &#125;&#125; 优化 ​ 对冒泡排序常见的改进方法是加入标志性变量 exchange，用于标志某一趟排序过程中是否有数据交换。 ​ 如果进行某一趟排序时并没有进行数据交换，则说明所有数据已经有序，可立即结束排序，避免不必要的比较过程。 123456789101112131415161718192021222324252627&#x2F;&#x2F; 对 bubbleSort 的优化算法public void bubbleSort_2(int[] list) &#123; int temp &#x3D; 0; &#x2F;&#x2F; 用来交换的临时数 boolean bChange &#x3D; false; &#x2F;&#x2F; 交换标志 &#x2F;&#x2F; 要遍历的次数 for (int i &#x3D; 0; i &lt; list.length - 1; i++) &#123; bChange &#x3D; false; &#x2F;&#x2F; 从后向前依次的比较相邻两个数的大小，遍历一次后，把数组中第i小的数放在第i个位置上 for (int j &#x3D; list.length - 1; j &gt; i; j--) &#123; &#x2F;&#x2F; 比较相邻的元素，如果前面的数大于后面的数，则交换 if (list[j - 1] &gt; list[j]) &#123; temp &#x3D; list[j - 1]; list[j - 1] &#x3D; list[j]; list[j] &#x3D; temp; bChange &#x3D; true; &#125; &#125; &#x2F;&#x2F; 如果标志为false，说明本轮遍历没有交换，已经是有序数列，可以结束排序 if (false &#x3D;&#x3D; bChange) break; System.out.format(&quot;第 %d 趟：\\t&quot;, i); printAll(list); &#125;&#125; =================================================================================================== 快速排序 ​ 快速排序是交换排序，不稳定算法。 ​ 数据越随机分布时，快速排序性能越好；数据越接近有序，快速排序性能越差。 ​ 通过一趟排序将要排序的数据分割成独立的两部分：分割点左边都是比它小的数，右边都是比它大的数。然后再按此方法对这两部分数据分别进行快速排序，整个排序过程可以递归进行，以此达到整个数据变成有序序列。 12345678910111213141516171819202122232425262728293031323334353637383940public int division(int[] list, int left, int right) &#123; &#x2F;&#x2F; 以最左边的数(left)为基准 int base &#x3D; list[left]; while (left &lt; right) &#123; &#x2F;&#x2F; 从序列右端开始，向左遍历，直到找到小于base的数 while (left &lt; right &amp;&amp; list[right] &gt;&#x3D; base) right--; &#x2F;&#x2F; 找到了比base小的元素，将这个元素放到最左边的位置 list[left] &#x3D; list[right]; &#x2F;&#x2F; 从序列左端开始，向右遍历，直到找到大于base的数 while (left &lt; right &amp;&amp; list[left] &lt;&#x3D; base) left++; &#x2F;&#x2F; 找到了比base大的元素，将这个元素放到最右边的位置 list[right] &#x3D; list[left]; &#125; &#x2F;&#x2F; 最后将base放到left位置。此时，left位置的左侧数值应该都比left小； &#x2F;&#x2F; 而left位置的右侧数值应该都比left大。 list[left] &#x3D; base; return left;&#125;private void quickSort(int[] list, int left, int right) &#123; &#x2F;&#x2F; 左下标一定小于右下标，否则就越界了 if (left &lt; right) &#123; &#x2F;&#x2F; 对数组进行分割，取出下次分割的基准标号 int base &#x3D; division(list, left, right); System.out.format(&quot;base &#x3D; %d:\\t&quot;, list[base]); printPart(list, left, right); &#x2F;&#x2F; 对“基准标号“左侧的一组数值进行递归的切割，以至于将这些数值完整的排序 quickSort(list, left, base - 1); &#x2F;&#x2F; 对“基准标号“右侧的一组数值进行递归的切割，以至于将这些数值完整的排序 quickSort(list, base + 1, right); &#125;&#125; =================================================================================================== 直接插入排序 ​ 直接插入排序属于插入排序中最简单的排序，稳定，空间复杂性为1。 ​ 数据越接近正序，直接插入排序的算法性能越好。 ​ 每一趟将一个待排序的记录，按照其关键字的大小插入到有序队列的合适位置里，直到全部插入完成。 ​ 请脑补打牌时摸牌阶段的操作。 1234567891011121314151617181920public void insertSort(int[] list) &#123; &#x2F;&#x2F; 打印第一个元素 System.out.format(&quot;i &#x3D; %d:\\t&quot;, 0); printPart(list, 0, 0); &#x2F;&#x2F; 第1个数肯定是有序的，从第2个数开始遍历，依次插入有序序列 for (int i &#x3D; 1; i &lt; list.length; i++) &#123; int j &#x3D; 0; int temp &#x3D; list[i]; &#x2F;&#x2F; 取出第i个数，和前i-1个数比较后，插入合适位置 &#x2F;&#x2F; 因为前i-1个数都是从小到大的有序序列，所以只要当前比较的数(list[j])比temp大，就把这个数后移一位 for (j &#x3D; i - 1; j &gt;&#x3D; 0 &amp;&amp; temp &lt; list[j]; j--) &#123; list[j + 1] &#x3D; list[j]; &#125; list[j + 1] &#x3D; temp; System.out.format(&quot;i &#x3D; %d:\\t&quot;, i); printPart(list, 0, i); &#125;&#125; =================================================================================================== 希尔排序 ​ 希尔排序是一种插入排序。它是直接插入排序算法的一种威力加强版，不稳定。 ​ 不稳定：相同的数值，可能交换位置。 ​ 把记录按步长 gap 分组，对每组记录采用直接插入排序方法进行排序。随着步长逐渐减小，所分成的组包含的记录越来越多，当步长的值减小到 1 时，整个数据合成为一组，构成一组有序记录，则完成排序。 123456789101112131415161718192021public void shellSort(int[] list) &#123; int gap &#x3D; list.length &#x2F; 2; while (1 &lt;&#x3D; gap) &#123; &#x2F;&#x2F; 把距离为 gap 的元素编为一个组，扫描所有组 for (int i &#x3D; gap; i &lt; list.length; i++) &#123; int j &#x3D; 0; int temp &#x3D; list[i]; &#x2F;&#x2F; 对距离为 gap 的元素组进行排序 for (j &#x3D; i - gap; j &gt;&#x3D; 0 &amp;&amp; temp &lt; list[j]; j &#x3D; j - gap) &#123; list[j + gap] &#x3D; list[j]; &#125; list[j + gap] &#x3D; temp; &#125; System.out.format(&quot;gap &#x3D; %d:\\t&quot;, gap); printAll(list); gap &#x3D; gap &#x2F; 2; &#x2F;&#x2F; 减小增量 &#125;&#125; 直接插入排序和希尔排序的比较 直接插入排序是稳定的；而希尔排序是不稳定的。 直接插入排序更适合于原始记录基本有序的集合。 希尔排序的比较次数和移动次数都要比直接插入排序少，当 N 越大时，效果越明显。 在希尔排序中，增量序列 gap 的取法必须满足：最后一个步长必须是 1 。 直接插入排序也适用于链式存储结构；希尔排序不适用于链式结构。 =================================================================================================== 简单选择排序 ​ 简单选择排序是一种选择排序，不稳定，复杂性：简单。 ​ 选择排序：每趟从待排序的记录中选出关键字最小的记录，顺序放在已排序的记录序列末尾，直到全部排序结束为止。 123456789for (int j &#x3D; 0; j &lt; a.length-1; j++) &#123; for (int i &#x3D; j+1; i &lt; a.length; i++) &#123; if(a[i]&lt;a[j])&#123; int temp &#x3D; a[j]; a[j] &#x3D; a[i]; a[i] &#x3D; temp; &#125; &#125; &#125; =================================================================================================== 堆排序 ​ 堆是一棵顺序存储的完全二叉树。 ​ 其中每个结点的关键字都不大于其孩子结点的关键字，这样的堆称为小根堆。其中每个结点的关键字都不小于其孩子结点的关键字，这样的堆称为大根堆。 ​ 堆排序不稳定。当想得到一个序列中第 k 个最小的元素之前的部分排序序列，最好采用堆排序。 1234567891011121314151617181920212223242526272829303132333435363738394041424344public void HeapAdjust(int[] array, int parent, int length) &#123; int temp &#x3D; array[parent]; &#x2F;&#x2F; temp保存当前父节点 int child &#x3D; 2 * parent + 1; &#x2F;&#x2F; 先获得左孩子 while (child &lt; length) &#123; &#x2F;&#x2F; 如果有右孩子结点，并且右孩子结点的值大于左孩子结点，则选取右孩子结点 if (child + 1 &lt; length &amp;&amp; array[child] &lt; array[child + 1]) &#123; child++; &#125; &#x2F;&#x2F; 如果父结点的值已经大于孩子结点的值，则直接结束 if (temp &gt;&#x3D; array[child]) break; &#x2F;&#x2F; 把孩子结点的值赋给父结点 array[parent] &#x3D; array[child]; &#x2F;&#x2F; 选取孩子结点的左孩子结点,继续向下筛选 parent &#x3D; child; child &#x3D; 2 * child + 1; &#125; array[parent] &#x3D; temp;&#125;public void heapSort(int[] list) &#123; &#x2F;&#x2F; 循环建立初始堆 for (int i &#x3D; list.length &#x2F; 2; i &gt;&#x3D; 0; i--) &#123; HeapAdjust(list, i, list.length); &#125; &#x2F;&#x2F; 进行n-1次循环，完成排序 for (int i &#x3D; list.length - 1; i &gt; 0; i--) &#123; &#x2F;&#x2F; 最后一个元素和第一元素进行交换 int temp &#x3D; list[i]; list[i] &#x3D; list[0]; list[0] &#x3D; temp; &#x2F;&#x2F; 筛选 R[0] 结点，得到i-1个结点的堆 HeapAdjust(list, 0, i); System.out.format(&quot;第 %d 趟: \\t&quot;, list.length - i); printPart(list, 0, list.length - 1); &#125;&#125; =================================================================================================== 归并排序 ​ 归并排序是建立在归并操作上的一种有效的排序算法，该算法是采用分治法（Divide and Conquer）的一个非常典型的应用。 ​ 将已有序的子序列合并，得到完全有序的序列；即先使每个子序列有序，再使子序列段间有序。若将两个有序表合并成一个有序表，称为二路归并。 ​ 归并算法是稳定的。 123456789101112131415161718192021222324252627282930313233343536373839public void Merge(int[] array, int low, int mid, int high) &#123; int i &#x3D; low; &#x2F;&#x2F; i是第一段序列的下标 int j &#x3D; mid + 1; &#x2F;&#x2F; j是第二段序列的下标 int k &#x3D; 0; &#x2F;&#x2F; k是临时存放合并序列的下标 int[] array2 &#x3D; new int[high - low + 1]; &#x2F;&#x2F; array2是临时合并序列 &#x2F;&#x2F; 扫描第一段和第二段序列，直到有一个扫描结束 while (i &lt;&#x3D; mid &amp;&amp; j &lt;&#x3D; high) &#123; &#x2F;&#x2F; 判断第一段和第二段取出的数哪个更小，将其存入合并序列，并继续向下扫描 if (array[i] &lt;&#x3D; array[j]) &#123; array2[k] &#x3D; array[i]; i++; k++; &#125; else &#123; array2[k] &#x3D; array[j]; j++; k++; &#125; &#125; &#x2F;&#x2F; 若第一段序列还没扫描完，将其全部复制到合并序列 while (i &lt;&#x3D; mid) &#123; array2[k] &#x3D; array[i]; i++; k++; &#125; &#x2F;&#x2F; 若第二段序列还没扫描完，将其全部复制到合并序列 while (j &lt;&#x3D; high) &#123; array2[k] &#x3D; array[j]; j++; k++; &#125; &#x2F;&#x2F; 将合并序列复制到原始序列中 for (k &#x3D; 0, i &#x3D; low; i &lt;&#x3D; high; i++, k++) &#123; array[i] &#x3D; array2[k]; &#125;&#125; 归并排序和堆排序、快速排序的比较 若从空间复杂度来考虑：首选堆排序，其次是快速排序，最后是归并排序。 若从稳定性来考虑，应选取归并排序，因为堆排序和快速排序都是不稳定的。 若从平均情况下的排序速度考虑，应该选择快速排序。 =================================================================================================== ###基数排序 ​ 基数排序与本系列前面讲解的七种排序方法都不同，它不需要比较关键字的大小。它是根据关键字中各位的值，通过对排序的 N 个元素进行若干趟“分配”与“收集”来实现排序的。 ​ 基数算法是稳定的。","categories":[],"tags":[{"name":"算法","slug":"算法","permalink":"https://18360732385.github.io/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"IO模式","slug":"IO模式","date":"2019-08-19T16:00:00.000Z","updated":"2019-08-19T16:00:00.000Z","comments":true,"path":"2019/08/20/IO模式/","link":"","permalink":"https://18360732385.github.io/2019/08/20/IO%E6%A8%A1%E5%BC%8F/","excerpt":"什么是IO访问​ 对于一次IO访问（以read举例），数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的内存地址空间。所以说，当一个read操作发生时，它会经历两个阶段：","text":"什么是IO访问​ 对于一次IO访问（以read举例），数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的内存地址空间。所以说，当一个read操作发生时，它会经历两个阶段： 等待内核缓冲区中数据准备 (Waiting for the data to be ready) 将数据从内核缓冲区拷贝到进程中 (Copying the data from the kernel to the process) 正式因为这两个阶段，linux系统产生了下面五种IO模式的方案： 阻塞 I/O（blocking IO） 非阻塞 I/O（nonblocking IO） I/O 多路复用（ IO multiplexing） 异步 I/O（asynchronous IO） 信号驱动 I/O（ signal driven IO） 阻塞 I/O（BIO） ​ 当一个进程需要read数据时，它需要先从内核缓存区中读取数据，当在内核中数据还未准备好时，则该进程会被阻塞（block），直到数据准备好为止。 非阻塞 I/O ​ 一个进程需要read数据时，它需要先从内核缓存区中读取数据，当在内核中数据还未准备好时，内核立即返回error，用户进程得知error后，会继续read，直到数据准备好为止。 ​ 使用轮询去获取数据，浪费CPU。 I/O 多路复用 ​ 上述阻塞IO和非阻塞IO，当时一个用户进程对应一个IO操作，IO多路复用则通过一定的机制实现了一个进程可以对应多个IO操作。 ​ IO多路复用主要有select、poll、epoll三种模式，select/poll相差不大，主要是通过轮询来不断的检测是否有描述符已就绪，select默认情况下支持最多监控1024个描述符，poll则没有这个限制（底层通过链表实现，可动态增加）；epoll不是通过轮询，而是通过回调（callback）方式主动通知已有描述符已就绪，相比较select/poll效率有明显提升。 ​ 不管哪种模式的IO多路复用，还都是同步IO，比如当监控的所有的描述符都还没有准备就绪，那么该进程还是阻塞在此，下面将介绍异步IO ​ I/O多路复用的典型是：nignx和redis 异步 I/O（AIO） ​ 相比较同步IO，异步IO就显得更加高大上了，基本原理是用户进程发起read操作之后，立刻就可以开始去做其它的事。而另一方面，从kernel的角度，当它受到一个asynchronous read之后，首先它会立刻返回，所以不会对用户进程产生任何block。然后kernel会等待数据准备完成，然后将数据拷贝到用户内存，当这一切都完成之后，kernel会给用户进程发送一个signal，告诉它read操作完成了，用户进程收到signal后获取已准备好的数据。 信号驱动 I/O ​ 信号驱动IO和异步IO类似，甚至有些人认为信号驱动IO就是异步IO，当kernel准备好数据后，会发送signal给用户进程，告诉进程数据已经准备就绪，这时进程会主动去read数据将数据拷贝到用户内存，这一步是由应用程序自己操作的，而异步IO则是由kernal实现，区别在此。","categories":[],"tags":[{"name":"IO模式","slug":"IO模式","permalink":"https://18360732385.github.io/tags/IO%E6%A8%A1%E5%BC%8F/"}]},{"title":"List去重的几种方法","slug":"List去重的几种方法","date":"2019-08-17T16:00:00.000Z","updated":"2019-08-17T16:00:00.000Z","comments":true,"path":"2019/08/18/List去重的几种方法/","link":"","permalink":"https://18360732385.github.io/2019/08/18/List%E5%8E%BB%E9%87%8D%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E6%B3%95/","excerpt":"使用HashSet/TreetSet","text":"使用HashSet/TreetSet 123456789 public static List removeDuplicationByHashSet(List&lt;Integer&gt; list) &#123; Set set &#x3D; new HashSet(list); &#x2F;&#x2F;无序&#x3D;&#x3D;&#x3D;&#x2F;&#x2F; Set set &#x3D; new TreeSet(list); &#x2F;&#x2F;有序&#x3D;&#x3D;&#x3D; list.clear(); &#x2F;&#x2F;把List集合所有元素清空 list.addAll(set); &#x2F;&#x2F;把HashSet对象添加至List集合 return list; &#125; java8新特性stream实现List去重(有序)public static List removeDuplicationByStream(List&lt;Integer&gt; list) { List newList = list.stream().distinct().collect(Collectors.toList()); return newList; }使用List集合contains方法循环遍历(有序)public static List removeDuplicationByContains(List&lt;Integer&gt; list) { List&lt;Integer&gt; newList =new ArrayList&lt;&gt;(); for (int i=0;i&lt;list.size();i++) { if(!newList.contains(list.get(i))){ newList.add(list.get(i)); } } list.clear(); list.addAll(newList); return list; }","categories":[],"tags":[{"name":"Java基础","slug":"Java基础","permalink":"https://18360732385.github.io/tags/Java%E5%9F%BA%E7%A1%80/"}]},{"title":"手写双向链表","slug":"手写双向链表","date":"2019-08-17T16:00:00.000Z","updated":"2019-08-17T16:00:00.000Z","comments":true,"path":"2019/08/18/手写双向链表/","link":"","permalink":"https://18360732385.github.io/2019/08/18/%E6%89%8B%E5%86%99%E5%8F%8C%E5%90%91%E9%93%BE%E8%A1%A8/","excerpt":"如何手写一个双向链表","text":"如何手写一个双向链表 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167public class LinkedList&lt;E&gt; &#123; private Node&lt;E&gt; first; &#x2F;&#x2F;头节点 尾节点 private Node&lt;E&gt; last; int size; &#x2F;&#x2F;大小 public LinkedList() &#123; &#125; private static class Node&lt;E&gt; &#123; E item; Node&lt;E&gt; prev; Node&lt;E&gt; next; public Node(Node&lt;E&gt; prev, E item, Node&lt;E&gt; next) &#123; this.item &#x3D; item; this.prev &#x3D; prev; this.next &#x3D; next; &#125; &#125; &#x2F;** * 添加数据到最后 *&#x2F; public void add(E e) &#123; linkLast(e); &#125; &#x2F;** * 添加数据在index 位置 *&#x2F; public void add(int index, E e) &#123; if (index &lt; 0 || index &gt; size) &#123; return; &#125; if (index &#x3D;&#x3D; size) &#123;&#x2F;&#x2F;如果添加到最后 linkLast(e); &#125; else &#123; &#x2F;&#x2F;查找当前位置所在的节点 Node&lt;E&gt; target &#x3D; node(index); &#x2F;&#x2F;原节点的上一节点 Node&lt;E&gt; pre &#x3D; target.prev; &#x2F;&#x2F;更新新节点的上一节点 Node&lt;E&gt; newNode &#x3D; new Node&lt;&gt;(pre, e, target); if (pre &#x3D;&#x3D; null) &#123; &#x2F;&#x2F;如何添加的位置 为0 前位置为null first &#x3D; newNode; &#x2F;&#x2F;更新原位置节点的上一个节点 target.prev &#x3D; newNode; &#125; else &#123; &#x2F;&#x2F;更新原前节点的下一个节点 pre.next &#x3D; newNode; &#x2F;&#x2F;更新原位置节点的上一个节点 target.prev &#x3D; newNode; &#125; size++; &#125; &#125; private void linkLast(E e) &#123; Node&lt;E&gt; newNode &#x3D; new Node&lt;E&gt;(last, e, null); Node&lt;E&gt; l &#x3D; last; last &#x3D; newNode; if (l &#x3D;&#x3D; null) &#123; first &#x3D; newNode; &#125; else &#123; l.next &#x3D; newNode; &#125; size++; &#125; &#x2F;** * 查找位置 *&#x2F; public E get(int index) &#123; if (index &lt; 0 || index &gt; size) &#123; return null; &#125; return node(index).item; &#125; &#x2F;** * 获取index位置上的节点 *&#x2F; private Node&lt;E&gt; node(int index) &#123; &#x2F;&#x2F;如果index 在整个链表的前半部分 if (index &lt; (size &#x2F; 2)) &#123; &#x2F;&#x2F;size &gt;&gt;1 Node&lt;E&gt; node &#x3D; first; for (int i &#x3D; 0; i &lt; index; i++) &#123; node &#x3D; node.next; &#125; return node; &#125; else &#123; Node&lt;E&gt; node &#x3D; last; for (int i &#x3D; size - 1; i &gt; index; i--) &#123; node &#x3D; node.prev; &#125; return node; &#125; &#125; &#x2F;** * 获取数据e位置上的节点 *&#x2F; private Node&lt;E&gt; node(E e) &#123; Node&lt;E&gt; node &#x3D; first; boolean isHaveNode &#x3D; false; for (int i &#x3D; 0; i &lt; size; i++) &#123; if (node.item &#x3D;&#x3D; e) &#123; isHaveNode &#x3D; true; break; &#125; node &#x3D; node.next; &#125; if (isHaveNode) &#123; return node; &#125; else &#123; return null; &#125; &#125; &#x2F;** * 删除元素 根据index *&#x2F; public void remove(int index) &#123; Node&lt;E&gt; target &#x3D; node(index); unLinkNode(target); &#125; &#x2F;** * 删除元素 *&#x2F; public void remove(E e) &#123; Node&lt;E&gt; target &#x3D; node(e); unLinkNode(target); &#125; &#x2F;** * 删除节点 * * @param target *&#x2F; private void unLinkNode(Node&lt;E&gt; target) &#123; if (target &#x3D;&#x3D; null) &#123; System.out.println(&quot;没有该数据&quot;); return; &#125; Node&lt;E&gt; pre &#x3D; target.prev; Node&lt;E&gt; next &#x3D; target.next; if (pre !&#x3D; null) &#123; if (next !&#x3D; null) &#123; pre.next &#x3D; next; next.prev &#x3D; pre; &#125; else &#123; &#x2F;&#x2F;下一节点为空 则把要删除的节点的上一节点 设置为结束节点 pre.next &#x3D; null; last &#x3D; pre; &#125; &#125; else &#123; &#x2F;&#x2F;前一节点为空 则把要删除的节点的下一节点 设置为开始节点 if (next !&#x3D; null) &#123; next.prev &#x3D; null; first &#x3D; next; &#125; &#125; size--; &#125;&#125;","categories":[],"tags":[{"name":"算法","slug":"算法","permalink":"https://18360732385.github.io/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"Linux下查看日志命令","slug":"Linux下查看日志命令","date":"2019-08-15T16:00:00.000Z","updated":"2019-08-15T16:00:00.000Z","comments":true,"path":"2019/08/16/Linux下查看日志命令/","link":"","permalink":"https://18360732385.github.io/2019/08/16/Linux%E4%B8%8B%E6%9F%A5%E7%9C%8B%E6%97%A5%E5%BF%97%E5%91%BD%E4%BB%A4/","excerpt":"实时监控日志","text":"实时监控日志 12tail -100f test.log 实时监控100行日志tail -f test.log 实时监控日志 关键字查询日志1cat -n test.log |grep &quot;debug&quot; 查询关键字的日志 ###查询进程 1ps -ef|grep xxx","categories":[],"tags":[{"name":"工具","slug":"工具","permalink":"https://18360732385.github.io/tags/%E5%B7%A5%E5%85%B7/"}]},{"title":"IDEA插件和模板","slug":"IDEA插件和模板","date":"2019-08-14T16:00:00.000Z","updated":"2019-08-14T16:00:00.000Z","comments":true,"path":"2019/08/15/IDEA插件和模板/","link":"","permalink":"https://18360732385.github.io/2019/08/15/IDEA%E6%8F%92%E4%BB%B6%E5%92%8C%E6%A8%A1%E6%9D%BF/","excerpt":"插件Alibaba Java Code Guidelines ​ 阿里巴巴 Java 代码规范，安装后切换中文提示，关闭自动扫描代码","text":"插件Alibaba Java Code Guidelines ​ 阿里巴巴 Java 代码规范，安装后切换中文提示，关闭自动扫描代码 Key Promoter X ​ 快捷键提示，你所操作的步骤都会提示你对应的快捷键，还可以展示你使用的快捷键记录。 Translation ​ 翻译软件，直接在IDEA中翻译单词，无需打开网页。 Codota ​ 代码提示 RoboPOJOGenerator ​ 根据json字符串转化成实例对象。使用时先选择目录，粘贴jsonString，自动生成get、set、toString方法 Lombok ​ 以注解的方式，帮助生成实体类的get、set等方法 VisualVM Launcher ​ GC调优利器 模板 类模板 方法模板 logger模板 ​ https://blog.csdn.net/sdut406/article/details/81750858","categories":[],"tags":[{"name":"Idea","slug":"Idea","permalink":"https://18360732385.github.io/tags/Idea/"},{"name":"工具","slug":"工具","permalink":"https://18360732385.github.io/tags/%E5%B7%A5%E5%85%B7/"}]},{"title":"ZooKeeper入门认知","slug":"ZooKeeper入门认知","date":"2019-08-12T16:00:00.000Z","updated":"2019-08-12T16:00:00.000Z","comments":true,"path":"2019/08/13/ZooKeeper入门认知/","link":"","permalink":"https://18360732385.github.io/2019/08/13/ZooKeeper%E5%85%A5%E9%97%A8%E8%AE%A4%E7%9F%A5/","excerpt":"1.定义 ​ Zookeeper 从设计模式角度来看，是一个基于观察者模式设计的分布式服务管理框架，它负责存储和管理大家都关心的数据，然后接受观察者的注册，一旦这些数据的状态发生变化，Zookeeper 就将负责通知已经在 Zookeeper 上注册的那些","text":"1.定义 ​ Zookeeper 从设计模式角度来看，是一个基于观察者模式设计的分布式服务管理框架，它负责存储和管理大家都关心的数据，然后接受观察者的注册，一旦这些数据的状态发生变化，Zookeeper 就将负责通知已经在 Zookeeper 上注册的那些 观察者做出相应的反应，从而实现集群中类似 Master/Slave 管理模式。 ZooKeeper是一个分布式下数据一致性解决方案，基于CAP理论中的CP（Eurka基于AP，更适用高并发），可以用ZooKeeper来做：统一配置管理、统一命名服务、分布式锁、集群管理。 使用分布式系统就无法避免对节点管理的问题(需要实时感知节点的状态、对节点进行统一管理等等)，而由于这些问题处理起来可能相对麻烦和提高了系统的复杂性，ZooKeeper作为一个能够通用解决这些问题的中间件就应运而生了。 ###2.数据模型 ​ Zookeeper 会维护一个具有层次关系的数据结构，它非常类似于Uinux下标准的文件系统。ZooKeeper的节点我们称之为Znode，Znode分为两种类型： 临时节点(Ephemeral)：当客户端和服务端断开连接后，所创建的Znode(节点)会自动删除 持久节点(Persistent)：当客户端和服务端断开连接后，所创建的Znode(节点)不会删除 ZooKeeper和Redis一样，也是C/S结构(分成客户端和服务端)。 3.节点特征 每个子目录项如 NameService 都被称作为 znode，这个 znode 是被它所在的路径唯一标识，如 Server1 这个 znode 的标识为 /NameService/Server1 znode 可以有子节点目录，并且每个 znode 可以存储数据，*注意临时节点不能有子节点目录（临时节点下不能再存储节点，防止失效后数据丢失） * znode 是有版本的，每个 znode 中存储的数据可以有多个版本，也就是一个访问路径中可以存储多份数据 znode 可以是临时节点，一旦创建这个 znode 的客户端与服务器失去联系，这个 znode 也将自动删除，Zookeeper 的客户端和服务器通信采用长连接方式，每个客户端和服务器通过心跳来保持连接，这个连接状态称为 session，如果 znode 是临时节点，这个 session 失效，znode 也就删除了 znode 的目录名可以自动编号，如 App1 已经存在，再创建的话，将会自动命名为 App2 znode 可以被监控，包括这个目录节点中存储的数据的修改，子节点目录的变化等，一旦变化可以通知设置监控的客户端。 节点监听是 Zookeeper 的核心特性，Zookeeper 的很多功能都是基于这个特性实现的。关于znode节点的监听，可以分为2种： 监听Znode节点的数据变化 监听Znode节点下子节点的增减变化 4.统一配置管理（数据发布/订阅） ​ 配置的管理在分布式应用环境中很常见，例如同一个应用系统需要多台 PC Server 运行，但是它们运行的应用系统的某些配置项是相同的，如果要修改这些相同的配置项，那么就必须同时修改每台运行这个应用系统的 PC Server，这样非常麻烦而且容易出错。 ​ 像这样的配置信息完全可以交给 Zookeeper 来管理，将配置信息保存在 Zookeeper 的某个目录节点中，然后将所有需要修改的应用机器监控配置信息的状态，一旦配置信息发生变化，每台应用机器就会收到 Zookeeper 的通知，然后从 Zookeeper 获取新的配置信息应用到系统中。 ​ https://blog.csdn.net/u011320740/article/details/78742625 5.统一命名服务（负载均衡） ​ 统一命名服务的理解其实跟域名一样，是我们为这某一部分的资源给它取一个名字，别人通过这个名字就可以拿到对应的资源。比如说，现在我有一个域名www.java3y.com，但我这个域名下有多台机器： 192.168.1.1 192.168.1.2 192.168.1.3 192.168.1.4 ​ 别人访问www.java3y.com即可访问到我的机器，而不是通过IP去访问。在此基础上，加上一定的负载均衡算法，则可以 达到动态负载均衡的效果。 6.集群管理和Master选举 ​ 集群管理其实很容易实现，只要在ZooKeeper中创建临时节点即可。只要系统A挂了，那/groupMember/A这个节点就会删除，通过监听groupMember下的子节点，系统B和C就能够感知到系统A已经挂了(新增也是同理)。 ​ 除了能够感知节点的上下线变化，ZooKeeper还可以实现动态选举Master的功能(如果集群是主从架构模式下)。原理也很简单，如果想要实现动态选举Master的功能，Znode节点的类型是带顺序号的临时节点(EPHEMERAL_SEQUENTIAL)就好了。 Zookeeper会每次选举最小编号的作为Master，如果Master挂了，自然对应的Znode节点就会删除。然后让新的最小编号作为Master，这样就可以实现动态选举的功能了。 7.分布式锁排它锁 ​ 通过 ZooKeeper 上的 Znode 可以表示一个锁，/x_lock/lock。 获取锁 所有客户端都会通过调用 create() 接口尝试在 /x_lock 创建临时子节点 /x_lock/lock。最终只有一个客户端创建成功，那么该客户端就获取了锁。同时没有获取到锁的其他客户端，注册一个子节点变更的 Watcher 监听。 释放锁 获取锁的客户端发生宕机或者正常完成业务逻辑后，就会把临时节点删除。临时子节点删除后，其他客户端又开始新的一轮获取锁的过程。 共享锁（读写锁） ​ 如果事务 T1 对数据对象 O1 加上了共享锁，那么当前事务 T1 只能对 O1 进行读取操作，其他事务也只能对这个数据对象加共享锁，直到数据对象上的所有共享锁都被释放。 ​ 通过 ZooKeeper 上的 Znode 表示一个锁，/s_lock/[HOSTNAME]-请求类型-序号。 12345678&#x2F;├── &#x2F;host1-R-000000001├── &#x2F;host2-R-000000002├── &#x2F;host3-W-000000003├── &#x2F;host4-R-000000004├── &#x2F;host5-R-000000005├── &#x2F;host6-R-000000006└── &#x2F;host7-W-000000007 获取锁 需要获得共享锁的客户端都会在 s_lock 这个节点下面创建一个临时顺序节点，如果当前是读请求，就创建类型为 R 的临时节点，如果是写请求，就创建类型为 W 的临时节点。 判断读写顺序 共享锁下不同事务可以同时对同一个数据对象进行读取操作，而更新操作必须在当前没有任何事务进行读写操作的情况下进行。 2.1 创建完节点后，获取 s_lock 的所有子节点，并对该节点注册子节点变更的 Watcher 监听 2.2 然后确定自己的节点序号在所有的子节点中的顺序 2.3 对于读请求，如果没有比自己小的子节点，那么表明自己已经成功获取到了共享锁，同时开始执行读取逻辑，如果有比自己序号小的写请求，那么就需要进行等待 2.4 接收到 Watcher 通知后重复 2.1 释放锁 获取锁的客户端发生宕机或者正常完成业务逻辑后，就会把临时节点删除。临时子节点删除后，其他客户端又开始新的一轮获取锁的过程。 羊群效应 ​ 介绍的共享锁中，在判断读写顺序的时候会出现一个问题，假如 host4 在移除自己的节点的时候，后面 host5-7 都需要接收 Watcher 事件通知，但是实际上，只有 host5 接收到事件就可以了。因此以上的实现方式会产生大量的 Watcher 通知。这样会对 ZooKeeper 服务器造成了巨大的性能影响和网络冲击，这就是羊群效应。 ​ 改进的一步在于，调用 getChildren 接口的时候获取到所有已经创建的子节点列表，但是这个时候不要注册任何的 Watcher。当无法获取共享锁的时候，调用 exist() 来对比自己小的那个节点注册 Wathcer。而对于读写请求，会有不同的定义: 读请求: 在比自己序号小的最后一个写请求节点注册 Watcher。 写请求: 向比自己序号小的最后一个节点注册 Watcher。 8.分布式队列FIFO ​ 使用 ZooKeeper 实现 FIFO 队列，入队操作就是在 queue_fifo 下创建自增序的子节点，并把数据（队列大小）放入节点内。出队操作就是先找到 queue_fifo 下序号最下的那个节点，取出数据，然后删除此节点。 123456&#x2F;queue_fifo|├── &#x2F;host1-000000001├── &#x2F;host2-000000002├── &#x2F;host3-000000003└── &#x2F;host4-000000004 创建完节点后，根据以下步骤确定执行顺序: 通过 get_children() 接口获取 /queue_fifo 节点下所有子节点 通过自己的节点序号在所有子节点中的顺序 如果不是最小的子节点，那么进入等待，同时向比自己序号小的最后一个子节点注册 Watcher 监听 接收到 Watcher 通知后重复 1 同步队列（Barrier） ​ 当有些操作需要并行执行，但后续操作又需要串行执行，此时必须等待所有并行执行的线程全部结束，才开始串行，于是就需要一个屏障，来控制所有线程同时开始，并等待所有线程全部结束。 ​ 利用 ZooKeeper 的实现，开始时 queue_barrier 节点是一个已经存在的默认节点，并且将其节点的数据内容赋值为一个数字 n 来代表 Barrier 值，比如 n=10 代表只有当 /queue_barrier 节点下的子节点个数达到10才会打开 Barrier。之后所有客户端都会在 queue_barrier 节点下创建一个临时节点，如 queue_barrier/host1。 ​ 如何控制所有线程同时开始？ 所有的线程启动时在 ZooKeeper 节点 /queue_barrier 下插入顺序临时节点，然后检查 /queue/barrier 下所有 children 节点的数量是否为所有的线程数，如果不是，则等待，如果是，则开始执行。具体的步骤如下: getData() 获取 /queue_barrier 节点的数据内容 getChildren() 获取 /queue_barrier 节点下的所有子节点，同时注册对子节点列表变更的 Watche 监听。 统计子节点的个数 如果子节点个数不足10，那么进入等待 接收 Watcher 通知后，重复2 ​ 如何等待所有线程结束？ 所有线程在执行完毕后，都检查 /queue/barrier 下所有 children 节点数量是否为0，若不为0，则继续等待。 ​ 用什么类型的节点？ 根节点使用持久节点，子节点使用临时节点，根节点为什么要用持久节点？首先因为临时节点不能有子节点，所以根节点要用持久节点，并且在程序中要判断根节点是否存在。 子节点为什么要用临时节点？临时节点随着连接的断开而消失，在程序中，虽然会删除临时节点，但可能会出现程序在节点被删除之前就 crash了，如果是持久节点，节点不会被删除。 9.ZK集群 ​ 在集群的情况下，zk会选举出一个leader负责写操作，剩下的都可以负载读操作，这样就可以将读写分离 ，保证的单一性，避免数据不一致的情况。ZooKeeper 集群中包含 Leader、Follower 以及 Observer 三个角色： Leader：负责进行投票的发起和决议，更新系统状态，Leader 是由选举产生; Follower： 用于接受客户端请求并向客户端返回结果，在选主过程中参与投票; Observer：可以接受客户端连接，接受读写请求，写请求转发给 Leader，但 Observer 不参加投票过程，只同步 Leader 的状态，Observer 的目的是为了扩展系统，提高读取速度。 10.ZK和Eurka作为高可用注册中心的区别 ​ zk基于CAP理论中的CP，Eurka基于AP，更适用高并发。zookepper集群超过一半的机器宕机就不能对外提供服务，而eurka只要有一台机器就可以对外提供服务 11.链接 ​ 入门认识Zookeeper，看这篇文章，注意它提到的参考资料，也要看 &lt;https://mp.weixin.qq.com/s/lVGbtyDI670jHB5XV9ElAQ ​ Zookeeper的典型使用场景 https://www.ibm.com/developerworks/cn/opensource/os-cn-zookeeper/index.html","categories":[],"tags":[{"name":"中间件","slug":"中间件","permalink":"https://18360732385.github.io/tags/%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"Zookeeper","slug":"Zookeeper","permalink":"https://18360732385.github.io/tags/Zookeeper/"}]},{"title":"MySQL索引","slug":"MySQL索引","date":"2019-08-11T16:00:00.000Z","updated":"2019-08-12T16:00:00.000Z","comments":true,"path":"2019/08/12/MySQL索引/","link":"","permalink":"https://18360732385.github.io/2019/08/12/MySQL%E7%B4%A2%E5%BC%95/","excerpt":"​ 索引就是为了提高数据查询的效率，采用的一种数据结构，以文件形式存储在磁盘上，索引主要有： 哈希索引：不适合做区间查询，适用于只有等值查询的场景 有序数组：修改成本很高，只适用于静态存储引擎，即数据表一旦建立后不再会修改。","text":"​ 索引就是为了提高数据查询的效率，采用的一种数据结构，以文件形式存储在磁盘上，索引主要有： 哈希索引：不适合做区间查询，适用于只有等值查询的场景 有序数组：修改成本很高，只适用于静态存储引擎，即数据表一旦建立后不再会修改。 搜索树：B+树，插入和修改相对稳定。B+树满足了：快速查找值，区间，顺序逆序查找。 1.B+树 ​ B+树是在平衡树的基础上，在最后的叶节点的数据之间，使用双向链表链接。之所以要使用双向链表，是为了满足区间查询和顺序查询 。相比B树有可能退化成线性数据结构，B+树更稳定。 ​ https://mp.weixin.qq.com/s/3RTTRABqCnX2tn7GFgMyOQ 2.索引类型 ​ 根据叶子结点的内容，索引类型分为主键索引和非主键索引 ，在InnoDB引擎中： 主键索引的叶子结点存的是整条记录，主键索引也被称为聚簇索引（clustered index） 非主键索引的叶子结点存的是主键的值，非主键索引也被称为二级索引（secondary index）/普通索引/辅助索引。 非聚簇索引，可能存在回表的情况，即多查询一次数据库 3.联合索引 ​ 联合索引是指对表上的多个列进行索引。涉及到order by排序时，联合索引的优先级可能比主键更高（explain功能）。 联合索引的目的，其实是减少回表情况。 4.最左前缀原则 ​ 针对联合索引（a，b，c），sql语句使用索引的顺序，最好也是a、b、c的顺序（Mysql高版本好像有优化）。同时不能在索引上进行操作。索引abc的位置可以是乱序，但是必须要有索引a。 ​ 不只是索引的全部定义，只要满足最左前缀，就可以利用索引来加速检索。这个最左前缀可以是联合索引的最左 N 个字段，也可以是字符串索引的最左 M 个字符。 ​ 从B+树的结构来看，联合索引（a,b,c）存储在节点上，子节点上是a、b、c列的数据。查询时，mySql其实是先从节点上优先定位索引a，确定到子节点的data域，再去定位其他索引。 5.覆盖索引 ​ 覆盖索引的意义在于减少回表。索引k已经“覆盖了”我们的查询需求，故称为覆盖索引。 ​ 查询count（*）时，某些时候数据库会优化使用覆盖索引（而不是主键索引），因为覆盖索引存储的数据比聚簇索引更少。 6.MySQL优化 数据库表结构优化——冗余字段、类型和长度选择 sql语句优化——查询时具体字段代替*、分页查询用between代替limit、不要对查询字段做计算 合理建立索引 慢查询——explain计划 7.合理建立索引 ​ 首先，索引不能盲目地建 ，因为它是文件，需要维护。频繁更新的字段，不能作为索引。一般一张表不要超过5个。 主键自动建立唯一索引，且最好是自增类型或是整型 频繁作为查询和排序的字段应该创建索引 查询中与其他表关联的字段，外键关系建立索引 单键/组合索引的选择问题 （在高并发下倾向创建组合索引） 查询中排序的字段，排序字段若通过索引去访问将大大提高查询速度 查询中统计或分组字段 8.Explain执行计划 ​ 两个重要的优化指标：type和extra。key、possiable_keys等 type：这是重要的列，显示连接使用了何种类型。是较为重要的一个指标，结果值从好到坏依次是：system &gt; const &gt; eq_ref &gt; ref &gt; fulltext &gt; ref_or_null &gt; index_merge &gt; unique_subquery &gt; index_subquery &gt; range &gt; index &gt; ALL ​ 一般来说，得保证查询至少达到range级别，最好能达到ref。 Extra：关于MYSQL如何解析查询的额外信息。 这里可以看到的坏的例子是Using temporary和Using filesort，意思MYSQL根本不能使用索引，结果是检索会很慢 https://www.cnblogs.com/yycc/p/7338894.html id SELECT识别符。这是SELECT的查询序列号 select_type SELECT类型,可以为以下任何一种:SIMPLE:简单SELECT(不使用UNION或子查询)PRIMARY:最外面的SELECT table 输出的行所引用的表 type 联接类型。下面给出各种联接类型,按照从最佳类型到最坏类型进行排序:system:表仅有一行(=系统表)。这是const联接类型的一个特例。const:表最多有一个匹配行,它将在查询开始时被读取。因为仅有一行,在这行的列值可被优化器剩余部分认为是常数。const表很快,因为它们只读取一次!eq_ref:对于每个来自于前面的表的行组合,从该表中读取一行。这可能是最好的联接类型,除了const类型。ref:对于每个来自于前面的表的行组合,所有有匹配索引值的行将从这张表中读取。ref_or_null:该联接类型如同ref,但是添加了MySQL可以专门搜索包含NULL值的行。index_merge:该联接类型表示使用了索引合并优化方法。unique_subquery:该类型替换了下面形式的IN子查询的ref: value IN (SELECT primary_key FROM single_table WHERE some_expr) unique_subquery是一个索引查找函数,可以完全替换子查询,效率更高。index_subquery:该联接类型类似于unique_subquery。可以替换IN子查询,但只适合下列形式的子查询中的非唯一索引: value IN (SELECT key_column FROM single_table WHERE some_expr)range:只检索给定范围的行,使用一个索引来选择行。index:该联接类型与ALL相同,除了只有索引树被扫描。这通常比ALL快,因为索引文件通常比数据文件小。ALL:对于每个来自于先前的表的行组合,进行完整的表扫描。 possible_keys 指出MySQL能使用哪个索引在该表中找到行 key 显示MySQL实际决定使用的键(索引)。如果没有选择索引,键是NULL。 key_len 显示MySQL决定使用的键长度。如果键是NULL,则长度为NULL。 ref 显示使用哪个列或常数与key一起从表中选择行。 rows 显示MySQL认为它执行查询时必须检查的行数。多行之间的数据相乘可以估算要处理的行数。 filtered 显示了通过条件过滤出的行数的百分比估计值。 Extra 该列包含MySQL解决查询的详细信息Distinct:MySQL发现第1个匹配行后,停止为当前的行组合搜索更多的行。Not exists:MySQL能够对查询进行LEFT JOIN优化,发现1个匹配LEFT JOIN标准的行后,不再为前面的的行组合在该表内检查更多的行。range checked for each record (index map: #):MySQL没有发现好的可以使用的索引,但发现如果来自前面的表的列值已知,可能部分索引可以使用。Using filesort:MySQL需要额外的一次传递,以找出如何按排序顺序检索行。Using index:从只使用索引树中的信息而不需要进一步搜索读取实际的行来检索表中的列信息。Using temporary:为了解决查询,MySQL需要创建一个临时表来容纳结果。Using where:WHERE 子句用于限制哪一个行匹配下一个表或发送到客户。Using sort_union(…), Using union(…), Using intersect(…):这些函数说明如何为index_merge联接类型合并索引扫描。Using index for group-by:类似于访问表的Using index方式,Using index for group-by表示MySQL发现了一个索引,可以用来查 询GROUP BY或DISTINCT查询的所有列,而不要额外搜索硬盘访问实际的表。 9.索引失效情况 最好全值匹配 符合最左前缀原则 不要在索引列上做任何操作 is null判断时，索引会失效 like ‘%abc’这种情况，会导致全表扫描","categories":[],"tags":[{"name":"MySql","slug":"MySql","permalink":"https://18360732385.github.io/tags/MySql/"},{"name":"数据库","slug":"数据库","permalink":"https://18360732385.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"}]},{"title":"fastJson用法","slug":"fastJson用法","date":"2019-08-04T16:00:00.000Z","updated":"2019-08-04T16:00:00.000Z","comments":true,"path":"2019/08/05/fastJson用法/","link":"","permalink":"https://18360732385.github.io/2019/08/05/fastJson%E7%94%A8%E6%B3%95/","excerpt":"测试类Student","text":"测试类Student 1234567891011121314151617public class Student &#123; private String name; private int age; public Student() &#123; &#x2F;&#x2F; TODO Auto-generated constructor stub &#125; public Student(String name,int age)&#123; this.name&#x3D;name; this.age&#x3D;age; &#125; &#x2F;&#x2F;省略get、set、toString方法 ...... &#125; ##XXX 转 json字符串 ​ 转json的字符串都是使用JSON.toJSONString（）方法 JavaBean转json字符串123Student student&#x3D;new Student(&quot;张三&quot;,24);String str1 &#x3D; JSON.toJSONString(student)； 打印str1结果：{“age”:24,”name”:”张三”} List转json字符串1234567List&lt;Student&gt; list&#x3D;new ArrayList&lt;&gt;();Student student&#x3D;new Student(&quot;张三&quot;,24);Student student12&#x3D;new Student(&quot;李四&quot;, 23);list.add(student);list.add(student12);String str2 &#x3D; JSON.toJSONString(list)； 打印str2结果：[{“age”:24,”name”:”张三”},{“age”:23,”name”:”李四”}] jsonObject转json字符串12345Student student&#x3D;new Student(&quot;张三&quot;,24);JSONObject jsonObject1&#x3D;(JSONObject)JSON.toJSON(student);String str4&#x3D;JSON.toJSONString(jsonObject1); 打印str4结果：{“name”:”张三”,”age”:24} ##json字符串 转 XXX json字符串转javaBean方法：JSON.parseObject（）； json字符串转jsonArray方法：JSON.parseArray（）； json字符串转JavaBean1Student stu1 &#x3D; JSON.parseObject(str1,Student.class); 打印stu1结果：student [name=张三, age=24] json字符串转jsonObject12JSONObject jso1&#x3D;JSON.parseObject(str1);String name&#x3D; jso1.getString(&quot;name&quot;); 打印name结果：张三 json字符串转jsonArry12345JSONArray jArray&#x3D;JSON.parseArray(JSON.toJSONString(stulist));for(int i&#x3D;0;i&lt;jArray.size();i++)&#123; System.out.println(jArray.getJSONObject(i));&#125; 循环打印jsonArry结果： {“name”:”student0”,”age”:0}{“name”:”student1”,”age”:1}{“name”:”student2”,”age”:2}{“name”:”student3”,”age”:3}{“name”:”student4”,”age”:4} ##其他转换 JvaBean 转 jsonObject1234Student student&#x3D;new Student(&quot;张三&quot;,24);JSONObject jsonObject1&#x3D;(JSONObject)JSON.toJSON(student);jsonObject1.getString(&quot;name&quot;); 打印字符串结果：张三 jsonObject 转 javaBean1Student student2&#x3D;JSON.toJavaObject(jsonObject1, Student.class); 打印student2结果：student [name=张三, age=24] javaBean to jsonArray12345678910List&lt;Student&gt; stulist&#x3D;new ArrayList&lt;&gt;(); for(int i&#x3D;0;i&lt;5;i++)&#123; stulist.add(new Student(&quot;student&quot;+i, i)); &#125; JSONArray jsonArrays&#x3D;(JSONArray)JSON.toJSON(stulist);for(int i&#x3D;0;i&lt;jsonArrays.size();i++)&#123; System.out.println(jsonArrays.getJSONObject(i));&#125; 循环打印jsonArrays结果： {“name”:”student0”,”age”:0}{“name”:”student1”,”age”:1}{“name”:”student2”,”age”:2}{“name”:”student3”,”age”:3}{“name”:”student4”,”age”:4} jsonArry 转 List1234567891011List&lt;Student&gt; myList&#x3D;new ArrayList&lt;&gt;();for(int i&#x3D;0;i&lt;jsonArrays.size();i++)&#123; Student student3&#x3D;JSON.toJavaObject(jsonArrays.getJSONObject(i), Student.class); myList.add(student3);&#125;for(Student stu:myList)&#123; System.out.println(stu);&#125; 循环打印stu结果： student [name=student0 , age=0]student [name=student1 , age=1]student [name=student2 , age=2]student [name=student3 , age=3]student [name=student4 , age=4]","categories":[],"tags":[{"name":"工具","slug":"工具","permalink":"https://18360732385.github.io/tags/%E5%B7%A5%E5%85%B7/"},{"name":"FastJson","slug":"FastJson","permalink":"https://18360732385.github.io/tags/FastJson/"}]},{"title":"MySql进阶6.索引失效","slug":"MySql进阶6.索引失效","date":"2019-07-19T16:00:00.000Z","updated":"2019-07-19T16:00:00.000Z","comments":true,"path":"2019/07/20/MySql进阶6.索引失效/","link":"","permalink":"https://18360732385.github.io/2019/07/20/MySql%E8%BF%9B%E9%98%B66.%E7%B4%A2%E5%BC%95%E5%A4%B1%E6%95%88/","excerpt":"索引失效1、 最好全值匹配","text":"索引失效1、 最好全值匹配 ——索引怎么建我怎么用 12345678910111213create index 索引名 on 表名(字段)单值索引create index idx_user_name on user(username);复合索引create index idx_user_name_no on user(username,no)应对上述创建的索引，sql语句最好效果：select * from user where username&#x3D;&#39;&#39; and no &#x3D;&#39;&#39;;select * from user where username&#x3D;&#39;&#39;select * from user where no &#x3D; &#39;&#39; 2、 最佳左前缀法则 ——如果索引了多列，要遵守最左前缀法则。指的是查询要从索引的最左前列开始并且不跳过索引中的列。 12345678910111213create index idx_user_name_no on user(username,no,phone)以下sql不同程度上导致了索引失效：select no,phone 失效select no 索引全部失效select phone 索引全部失效select username，phone 其中username 索引生效 phone 失效最好的效果是：select username, no, phone from user 3、不在索引列上做任何操作 ——计算，函数，（自动或者手动）类型装换，会导致索引失效而导致全表扫描。 4、存储引擎不能使用索引中范围条件右边的列。 ——范围之后索引失效。（&lt; ,&gt; between and,） 5、尽量使用覆盖索引 ——只访问索引的查询（索引和查询列一致） 6、在MYSQL使用不等于（&lt;,&gt;,!=）的时候无法使用索引，会导致索引失效。7、is null或者is not null 也会导致无法使用索引。8、like以通配符开头（’%abc…’）也会导致MYSQL全表扫描9、字符串不加单引号索引失效。10、少用or，用它来连接时索引会失效。","categories":[{"name":"MySql进阶","slug":"MySql进阶","permalink":"https://18360732385.github.io/categories/MySql%E8%BF%9B%E9%98%B6/"}],"tags":[]},{"title":"MySql进阶5.视图","slug":"MySql进阶5.视图","date":"2019-07-17T16:00:00.000Z","updated":"2019-07-17T16:00:00.000Z","comments":true,"path":"2019/07/18/MySql进阶5.视图/","link":"","permalink":"https://18360732385.github.io/2019/07/18/MySql%E8%BF%9B%E9%98%B65.%E8%A7%86%E5%9B%BE/","excerpt":"视图","text":"视图 1234567891011121314151617181920相当于虚拟表，仅仅存储的是SQL的逻辑，不存储数据语法：创建视图：CREATE VIEW 视图名 AS 查询语句查询视图：select * from 视图名更新视图：方法一：CREATE OR REPLACE VIEW 视图名 AS 查询语句方法二：ALTER VIEW 视图名 AS 查询语句显示视图结构：SHOW CREATE VIEW 视图名删除视图：DROP VIEW 视图名 123456789101112对比： 存储结构 用法视图 只存储SQL逻辑，并不存储数据 查询数据、（增删改 用的比较少）表 存储的是数据 增删改查新增数据：insert into myview01(username,password,phone,email,created,updated) values(&#39;jack&#39;,&#39;6666&#39;,&#39;1233444&#39;,&quot;12345@qq.com&quot;,now(),now());更新数据：update myview01 set username&#x3D;&quot;tom&quot; where id &#x3D; 6;删除数据：delete from myview01 where id &#x3D; 6; 12345678910111213141516171819202122232425262728注意：如果视图包含下述结构中的任何一种，那么它就是不可更新的：· 聚合函数（SUM(), MIN(), MAX(), COUNT()等）。· DISTINCT · GROUP BY · HAVING · UNION或UNION ALL · 位于选择列表中的子查询· Join · FROM子句中的不可更新视图· WHERE子句中的子查询，引用FROM子句中的表。· 仅引用文字值（在该情况下，没有要更新的基本表）。· ALGORITHM &#x3D; TEMPTABLE（使用临时表总会使视图成为不可更新的）。关于可插入性（可用INSERT语句更新），如果它也满足关于视图列的下述额外要求，可更新的视图也是可插入的：· 不得有重复的视图列名称。· 视图必须包含没有默认值的基表中的所有列。· 视图列必须是简单的列引用而不是导出列。导出列不是简单的列引用，而是从表达式导出的。下面给出了一些导出列示例：· 3.14159· col1 + 3· UPPER(col2)· col3 &#x2F; col4· (subquery)","categories":[{"name":"MySql进阶","slug":"MySql进阶","permalink":"https://18360732385.github.io/categories/MySql%E8%BF%9B%E9%98%B6/"}],"tags":[]},{"title":"MySql进阶4.流程控制（顺序、选择、循环）","slug":"MySql进阶4.流程控制（顺序、选择、循环）","date":"2019-07-11T16:00:00.000Z","updated":"2019-07-11T16:00:00.000Z","comments":true,"path":"2019/07/12/MySql进阶4.流程控制（顺序、选择、循环）/","link":"","permalink":"https://18360732385.github.io/2019/07/12/MySql%E8%BF%9B%E9%98%B64.%E6%B5%81%E7%A8%8B%E6%8E%A7%E5%88%B6%EF%BC%88%E9%A1%BA%E5%BA%8F%E3%80%81%E9%80%89%E6%8B%A9%E3%80%81%E5%BE%AA%E7%8E%AF%EF%BC%89/","excerpt":"流程控制（顺序、选择、循环）选择（IF）","text":"流程控制（顺序、选择、循环）选择（IF） 12345678910111213141516171819202122语法：IF search_condition(条件、表达式) THEN statement_list [ELSEIF search_condition THEN statement_list] ... [ELSE statement_list]END IFCASE语法：CASE case_value WHEN when_value THEN statement_list [WHEN when_value THEN statement_list] ... [ELSE statement_list]END CASEOr: CASE WHEN search_condition THEN statement_list [WHEN search_condition THEN statement_list] ... [ELSE statement_list]END CASE 需求：1.编写函数，接收一个整形分数 如果分数&gt;=90返回优秀 &gt;=80 显示良好 &gt;=60 及格 &lt;60 不及格12345678910111213141516delimiter $$create function myfun4(score int) returns varchar(10)begindeclare result varchar(10);IF score&gt;&#x3D;90 THEN set result&#x3D;&#39;优秀&#39;; ELSEIF score&gt;&#x3D;80 THEN set result&#x3D;&#39;良好&#39;; ELSEIF score&gt;&#x3D;60 THEN set result&#x3D;&#39;及格&#39;; ELSE set result&#x3D;&#39;不及格&#39;;END IF;return result;end $$delimiter ; 12345678910111213141516171819改进：delimiter $$create function myfun6(score int) returns varchar(10)begindeclare result varchar(10);CASE WHEN score&gt;&#x3D;90 THEN set result &#x3D; &#39;A&#39;; WHEN score&gt;&#x3D;80 THEN set result &#x3D; &#39;B&#39;; WHEN score&gt;&#x3D;60 THEN set result &#x3D; &#39;c&#39;; ELSE set result &#x3D; &#39;D&#39;;END CASE;return result;end $$delimiter ; 设置默认值 如果邮箱为NULL 设置默认值”example@example.cn123456789select IF(isnull(email) or email&#x3D;&#39;&#39;,&quot;example@example.cn&quot;,email) as 邮箱from tb_user;# &gt;3000 买不起 &gt;1000 勉强买得起 select id,title,price,case when price&gt;300000 then &#39;买不起&#39;when price &gt;100000 then &#39;勉强吧&#39;else &#39;无压力&#39; end from tb_item; 循环（LOOP REPEAT WHILE）12LEAVE 退出循环，结束循环 类似于breakITERATE 跳出本次循环，继续下次 类似于continue 案例1234567891011121314151617181920212223批量插入100条数据1.编写存储过程 执行delimiter $$create procedure add_batch(IN num int)begindeclare count int default 1;a:loopinsert into user(uno,name) values(count,concat(&quot;name&quot;,count));set count &#x3D; count +1;IF count &gt;&#x3D;NUM+1 THEN LEAVE a;END IF;end loop;end $$delimiter; 在上面的基础上插入偶数数据12345678910111213141516171819delimiter $$create procedure add_batch02(IN num int)begindeclare count int default 1;a:while count&lt;num doset count &#x3D; count+1;if count%2!&#x3D;0 then ITERATE a;end if;insert into user(uno,name) values(count,concat(&quot;new_name&quot;,count));end whileend $$delimiter;","categories":[{"name":"MySql进阶","slug":"MySql进阶","permalink":"https://18360732385.github.io/categories/MySql%E8%BF%9B%E9%98%B6/"}],"tags":[]},{"title":"MySql进阶2.存储过程","slug":"MySql进阶2.存储过程","date":"2019-07-10T16:00:00.000Z","updated":"2019-07-10T16:00:00.000Z","comments":true,"path":"2019/07/11/MySql进阶2.存储过程/","link":"","permalink":"https://18360732385.github.io/2019/07/11/MySql%E8%BF%9B%E9%98%B62.%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B/","excerpt":"存储过程","text":"存储过程 1234567891011存储过程：存储在mysql服务端的一组预编译好的SQL语句（1）提高SQL语句复用性（2）减少SQL语句编译的次数以及客户端与服务端交互的次数面试：解释存储过程、函数区别？相同点：本质上两者类似，都是一组预编译好的SQL语句，提供SQL语句的复用性... 返回值 用法函数 有且只有一个返回值 用于查询某一个结果存储过程 可以有0 1 或多个返回值 可以做一些批量修改 批量更新操作 ###语法 12345678910111213141516存储过程语法：CREATE PROCEDURE 存储过程名 (参数列表) 存储过程体参数格式： OUT param1 INT参数关键字 参数名 参数类型IN（默认） 代表存储过程的入参（代表此处接受一个参数传入存储过程）OUT 出参（将参数传出存储过程）INOUT 既可以当入参 也可以当出参存储过程体（包含一组SQL语句）需要使用BEGIN...AND 限定如果存储过程体只有一行 那么 BEGIN...AND 可以省略存储过程调用call 存储过程名(实参列表) 123456789101112131415# 例1 查询id为1的用户信息修改MySQL默认的SQL语句分割符（;）delimiter $$create procedure my_pro1()beginselect * from tb_user where id &#x3D; 1;end$$delimiter ;call my_pro1(); 存储过程调用 ####1.有参存储过程（IN） 123456789delimiter $$create procedure my_pro3(IN uid int)beginselect * from tb_user where id &#x3D; uid;end$$delimiter ; ####2.有返回值存储过程（OUT） 123456789101112131415161718192021222324252627282930313233查询ID为1的用户的姓名方法1：delimiter $$create procedure my_pro4(OUT name varchar(20))beginselect username into name from tb_user where id &#x3D; 1;end$$delimiter ;# 通过用户变量接受存储过程的返回值set @username&#x3D;&#39;&#39;;call my_pro4(@username);select @username;方法2：delimiter $$create procedure my_pro5()begindeclare name varchar(20) default &#39;&#39;;select username into name from tb_user where id &#x3D; 1;select name;end$$delimiter ;call my_pro5(); ####3.有参数、返回值的存储过程 123456789delimiter $$create procedure my_pro6(IN uid int,OUT name varchar(20))beginselect username into name from tb_user where id &#x3D; uid;end$$delimiter ; ####4.INOUT类型 1234567891011121314151617接收两个整形 返回两个整形的2倍值delimiter $$create procedure my_pro6(INOUT a int,INOUT b int)beginset a &#x3D; a*2;set b &#x3D; b*2;end$$delimiter ;set @a &#x3D; 2;set @b &#x3D; 4;call my_pro7(@a,@b);select @a,@b; ###删除查看 12345删除存储过程drop procedure 存储过程名;查看存储过程SHOW CREATE PROCEDURE 存储过程名","categories":[{"name":"MySql进阶","slug":"MySql进阶","permalink":"https://18360732385.github.io/categories/MySql%E8%BF%9B%E9%98%B6/"}],"tags":[]},{"title":"Bean生命周期","slug":"Bean生命周期","date":"2019-07-09T16:00:00.000Z","updated":"2019-07-09T16:00:00.000Z","comments":true,"path":"2019/07/10/Bean生命周期/","link":"","permalink":"https://18360732385.github.io/2019/07/10/Bean%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F/","excerpt":"1.Bean生命周期阶段及接口 ​ Spring Bean的生命周期有四个阶段：实例化 -&gt; 属性赋值 -&gt; 初始化 -&gt; 销毁 ​ Bean级生命周期接口：(4个)：","text":"1.Bean生命周期阶段及接口 ​ Spring Bean的生命周期有四个阶段：实例化 -&gt; 属性赋值 -&gt; 初始化 -&gt; 销毁 ​ Bean级生命周期接口：(4个)： BeanNameAware BeanFactoryAware InitializingBean DisposableBean 容器级Bean生命周期接口：(下图中星号)： 抽象类：InstantiationAwareBeanPostProcessorAdapter 接口BeanPostProcessor 2.bean生命周期的流程 调用InstantiationAwareBeanPostProcessor（抽象类）的 postProcessBeforeInstantiation（） 方法 实例化 调用InstantiationAwareBeanPostProcessor（抽象类）的 postProcessAfterInstantiation（） 方法 调用InstantiationAwareBeanPostProcessor（抽象类）的 postProcessPropertyValues（） 方法 属性赋值 如果Bean实现了*Aware接口，就调用相应方法，如： 实现了BeanNameAware接口，调用setBeanName()方法； 实现了BeanFactoryAware接口，调用setBeanFactory()方法 执行BeanPostProcessor接口的postProcessBeforeInitialization()方法 初始化。。。 如果Bean实现了InitializingBean接口，执行afterPropertiesSet()方法。 如果Bean在配置文件中的定义包含init-method属性，执行指定的方法。 执行BeanPostProcessor接口的postProcessAfterInitialization()方法。 销毁。。。 如果Bean实现了DisposableBean接口，执行destroy()方法。 如果Bean在配置文件中的定义包含destroy-method属性，执行指定的方法。 3.如何针对指定Bean的初始化设置 ​ 有时我们需要在Bean属性值set好之后（初始化）和Bean销毁之前做一些事情，比如检查Bean中某个属性是否被正常的设置好值了。Spring框架提供了多种方法，让我们可以在Spring Bean的生命周期中执行initialization和pre-destroy方法。 ​ 主要有3种实现方式: 1.实现InitializingBean和DisposableBean接口 ​ 这种方法比较简单，但是不建议使用。因为这样会将Bean的实现和Spring框架耦合在一起。 通过实现InitializingBean接口的afterPropertiesSet()方法，可以在Bean属性值设置好之后做一些操作 实现DisposableBean接口的destroy()方法可以在销毁Bean之前做一些操作。 1234567891011public class GiraffeService implements InitializingBean,DisposableBean &#123; @Override public void afterPropertiesSet() throws Exception &#123; System.out.println(&quot;执行InitializingBean接口的afterPropertiesSet方法&quot;); &#125; @Override public void destroy() throws Exception &#123; System.out.println(&quot;执行DisposableBean接口的destroy方法&quot;); &#125;&#125; 2.在bean的配置文件中指定init-method和destroy-method方法 （推荐） 3.使用@PostConstruct和@PreDestroy注解 ​ 需要配置文件 1&lt;bean class&#x3D;&quot;org.springframework.context.annotation.CommonAnnotationBeanPostProcessor&quot; &#x2F;&gt; 12345678910public class GiraffeService &#123; @PostConstruct public void initPostConstruct()&#123; System.out.println(&quot;执行PostConstruct注解标注的方法&quot;); &#125; @PreDestroy public void preDestroy()&#123; System.out.println(&quot;执行preDestroy注解标注的方法&quot;); &#125;&#125; 4.如何针对所有Bean的初始化设置 ​ Spring可以针对容器中的所有Bean，或者某些Bean定制初始化过程（可以利用BeanName比较判断），只需提供一个实现BeanPostProcessor接口的类即可。 该接口中包含两个方法： postProcessBeforeInitialization方法，会在容器中的Bean初始化之前执行 postProcessAfterInitialization方法，在容器中的Bean初始化之后执行 12345678910111213public class CustomerBeanPostProcessor implements BeanPostProcessor &#123; @Override public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException &#123; System.out.println(&quot;执行BeanPostProcessor的postProcessBeforeInitialization方法,beanName&#x3D;&quot; + beanName); return bean; &#125; @Override public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException &#123; System.out.println(&quot;执行BeanPostProcessor的postProcessAfterInitialization方法,beanName&#x3D;&quot; + beanName); return bean; &#125;&#125; 5.总结 通过实现BeanFactory的Bean生命周期相关接口，虽然让Bean具有了更详细的生命周期阶段，但是也让Spring框架和Bean紧密联系在了一起，同时也增加了代码复杂度。因此，如果用户希望在Bean的生命周期中实现自己的业务，不需要和特定框架关联，可以通过 bean 的 init-method 和 destroy-method 配置方法来进行业务实现。 Spring还拥有一个Bean后置处理器InitDestroyAnnotationBeanPostProcessor，它负责对标注了 @PostConstruct 和 @PreDestroy 的 Bean 进行处理，在 Bean 初始化及销毁前执行相应的业务逻辑。 通常情况下，可以抛弃Bean级的生命周期的4个接口，用更加方便的方法进行代替（如1、2点中的方法）。但是容器级Bean生命周期接口可以合理的使用，处理一些共同的业务。 为什么我们使用ApplicationContext，而不是BeanFactory： ApplicationContext相比BeanFactory来说，实现了更多的接口，功能更丰富和高级。 ApplicationContext会利用Java反射机制自动识别出配置文件中定义的BeanPostProcessor、InstantiationAwareBeanPostProcessor和BeanFactoryPostProcessor，并自动将它们注册到应用上下文中。 而BeanFactory需要在代码中通过手工调用addBeanPostProcessor()方法进行注册。这也是为什么在应用开发时，我们普遍使用ApplicationContext而很少使用BeanFactory的原因之一。 ​","categories":[],"tags":[{"name":"Bean","slug":"Bean","permalink":"https://18360732385.github.io/tags/Bean/"}]},{"title":"MySql进阶1.变量","slug":"MySql进阶1.变量","date":"2019-07-09T16:00:00.000Z","updated":"2019-07-09T16:00:00.000Z","comments":true,"path":"2019/07/10/MySql进阶1.变量/","link":"","permalink":"https://18360732385.github.io/2019/07/10/MySql%E8%BF%9B%E9%98%B61.%E5%8F%98%E9%87%8F/","excerpt":"变量 系统变量：由mysql服务端定义好的（作用于mysql服务端），不需要程序员手动定义","text":"变量 系统变量：由mysql服务端定义好的（作用于mysql服务端），不需要程序员手动定义 ###全局变量 （a）全局变量：作用于所有会话，不能跨重启 12345678910查询所有全局变量SHOW GLOBAL VARIABLES查询指定的全局变量SHOW GLOBAL VARIABLES like &#39;sort_buffer_size&#39;;SELECT @@global.sort_buffer_size;设置全局变量SET GLOBAL sort_buffer_size&#x3D;value;SET @@global.sort_buffer_size&#x3D;value; ###会话变量 （b）会话变量：仅仅作用于当前客户端连接 123456789101112查询所有会话变量SHOW SESSION VARIABLES查询指定的会话变量SHOW SESSION VARIABLES like &#39;sort_buffer_size&#39;;SELECT @@session.sort_buffer_size;SELECT @@sort_buffer_size;设置会话变量SET SESSION sort_buffer_size&#x3D;value;SET @@session.sort_buffer_size&#x3D;value;SET sort_buffer_size&#x3D;value; ###自定义变量 2.2自定义变量：由程序员自定义的变量（1）声明（2）赋值（3）使用 计算、比较等操作 #####用户变量 （a）用户变量（类似于全局变量、作用于整个会话，作用域相当于会话变量） 123456789101112（1）声明 set @变量名&#x3D;&#39;&#39;；（2）赋值（一般声明、赋值一体） set @变量名&#x3D;值 set @变量名:&#x3D;值 select @变量名：&#x3D;值 select 字段名 into @变量名 from 表 （3）查询 select @变量名 1234567891011例子： 计算1+2，如： set @a&#x3D;1; set @b&#x3D;2; set @c&#x3D;@a+@b; select @c; &#x2F;&#x2F;结果为@c&#x3D;3 set @name&#x3D;&#39;&#39;; select username into @name from tb_user where id&#x3D;1; select @name； &#x2F;&#x2F;结果为@name&#x3D;zhangsan 局部变量 （b）局部变量（局部的范围定义在begin和end之间） 123456789101112（1）声明 declare 变量名 数据类型; declare 变量名 数据类型 [default &#39;&#39;];（2）赋值（一般声明、赋值一体） set 变量名&#x3D;值 set 变量名:&#x3D;值 select 变量名：&#x3D;值 select 字段名 into 变量名 from 表（3）查询 select 变量名","categories":[{"name":"MySql进阶","slug":"MySql进阶","permalink":"https://18360732385.github.io/categories/MySql%E8%BF%9B%E9%98%B6/"}],"tags":[]},{"title":"MySql进阶3.函数","slug":"MySql进阶3.函数","date":"2019-07-07T16:00:00.000Z","updated":"2019-07-07T16:00:00.000Z","comments":true,"path":"2019/07/08/MySql进阶3.函数/","link":"","permalink":"https://18360732385.github.io/2019/07/08/MySql%E8%BF%9B%E9%98%B63.%E5%87%BD%E6%95%B0/","excerpt":"函数","text":"函数 1234567创建：CREATE FUNCTION 函数名(形参列表) #形参定义 ：变量名 变量类型 RETURNS 类型 # 声明该函数的返回值类型 函数体 #begin...end 之间调用select 函数名(实参列表); 无参函数1234567891011121314151617181920212223242526返回id为1的用户的姓名局部变量delimiter $$create function myfun1() returns varchar(20)begindeclare name varchar(20);select username into name from tb_user where id &#x3D; 1;return name;end$$delimiter ;用户变量delimiter $$create function myfun2() returns varchar(20)beginset @name &#x3D; &#39;&#39;;select username into @name from tb_user where id &#x3D; 1;return @name;end$$delimiter ; 有参函数123456789101112接收用户名密码 判断用户是否登录成功delimiter $$create function myfun3(username varchar(20),upass varchar(20)) returns varchar(20)begindeclare result int default 0;select count(*) into result from tb_user where username&#x3D;username and password&#x3D;upass;return result;end$$delimiter ;","categories":[{"name":"MySql进阶","slug":"MySql进阶","permalink":"https://18360732385.github.io/categories/MySql%E8%BF%9B%E9%98%B6/"}],"tags":[]},{"title":"Springboot中的过滤器、拦截器、切片","slug":"Springboot中的过滤器、拦截器、切片","date":"2019-07-06T16:00:00.000Z","updated":"2019-07-06T16:00:00.000Z","comments":true,"path":"2019/07/07/Springboot中的过滤器、拦截器、切片/","link":"","permalink":"https://18360732385.github.io/2019/07/07/Springboot%E4%B8%AD%E7%9A%84%E8%BF%87%E6%BB%A4%E5%99%A8%E3%80%81%E6%8B%A6%E6%88%AA%E5%99%A8%E3%80%81%E5%88%87%E7%89%87/","excerpt":"过滤器filter自定义过滤器","text":"过滤器filter自定义过滤器 1234567891011121314151617181920212223&#x2F;&#x2F;直接使用@Compnent注解，表示filter在所有的的url都生效；也可以在配置文件中配置&#x2F;&#x2F;@Compnentpublic class MyFilter implements Filter &#123; @Override public void init(FilterConfig filterConfig) throws ServletException &#123; System.out.println(&quot;my filter init&quot;); &#125; @Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException &#123; System.out.println(&quot;my filter start&quot;); long start &#x3D; new Date().getTime(); filterChain.doFilter(servletRequest, servletResponse); System.out.println(&quot;my filter 耗时:&quot; + (new Date().getTime() - start)); System.out.println(&quot;my filter finish&quot;); &#125; @Override public void destroy() &#123; System.out.println(&quot;my filter destroy&quot;); &#125;&#125; 配置文件WebConfig ​ 使用WebConfig配置过滤器的好处是：统一配置，方便管理。另一种是@WebFilter注解配置，它的好处是简单，一行注解解决问题。 123456789101112131415161718192021@Configurationpublic class WebConfig &#123; @Bean public FilterRegistrationBean timeFilter() &#123; &#x2F;&#x2F;设置过滤器执行顺序，数字越小，执行越早。这里设置Integer.MIN_VALUE，则最先执行 FilterRegistrationBean filterRegistrationBean &#x3D; new FilterRegistrationBean(); filterRegistrationBean.setOrder(Integer.MIN_VALUE); MyFilter myFilter &#x3D; new MyFilter(); filterRegistrationBean.setFilter(myFilter); &#x2F;&#x2F; 设置MyFile在指定url起作用，可以添加多个 List&lt;String&gt; urlList &#x3D; new ArrayList&lt;&gt;(); urlList.add(&quot;&#x2F;*&quot;); filterRegistrationBean.setUrlPatterns(urlList); return filterRegistrationBean; &#125;&#125; 拦截器interceptor自定义拦截器 ​ 实现接口HandlerInterceptor，有3个方法：preHandle、postHandle、afterCompletion 12345678910111213141516171819202122232425262728@Componentpublic class MyInterceptor implements HandlerInterceptor &#123; @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; System.out.println(&quot;preHandle&quot;); request.setAttribute(&quot;startTime&quot;, new Date().getTime()); System.out.println(((HandlerMethod) handler).getBean().getClass().getName()); &#x2F;&#x2F; 获取处理请求的类名称 System.out.println(((HandlerMethod) handler).getMethod().getName()); &#x2F;&#x2F; 获取处理请求的方法名 return true; &#125; @Override public void postHandle(HttpServletRequest request, HttpServletResponse response, Object o, ModelAndView modelAndView) throws Exception &#123; System.out.println(&quot;postHandle&quot;); long startTime &#x3D; (long) request.getAttribute(&quot;startTime&quot;); System.out.println(&quot;my interceptor 耗时：&quot; + (new Date().getTime() - startTime)); &#125; @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object o, Exception e) throws Exception &#123; System.out.println(&quot;afterCompletion&quot;); long startTime &#x3D; (long) request.getAttribute(&quot;startTime&quot;); System.out.println(&quot;my interceptor 耗时：&quot; + (new Date().getTime() - startTime)); System.out.println(&quot;ex is &quot; + e); &#125;&#125; 配置文件 配置文件继承web适配器 123456789101112131415@Configurationpublic class WebConfig implements WebMvcConfigurer &#123; @Autowired private MyInterceptor myInterceptor; &#x2F;&#x2F; 重写WebMvcConfigurerAdapter类的addInterceptors方法，添加自己的拦截器及拦截路径 &#x2F;&#x2F;加入多个拦截器，则加入顺序决定拦截器生效顺序 @Override public void addInterceptors(InterceptorRegistry registry) &#123; registry.addInterceptor(myInterceptor) .addPathPatterns(&quot;&#x2F;**&quot;); &#125;&#125; 切片aspect引入maven依赖1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-starter-aop&lt;&#x2F;artifactId&gt;&lt;&#x2F;dependency&gt; 自定义切面 切面可以作为log日志打印 12345678910111213141516171819@Aspect@Componentpublic class MyAspect &#123; @Around(&quot;execution(* com.sinosoft.controller.UserController.*(..))&quot;) public Object handleControllerMethod(ProceedingJoinPoint pjp) throws Throwable &#123; System.out.println(&quot;my aspect start&quot;); Object[] args &#x3D; pjp.getArgs(); &#x2F;&#x2F; 获取到方法参数的值 for (Object arg : args) &#123; System.out.println(&quot;arg is &quot; + arg); &#125; long startTime &#x3D; new Date().getTime(); Object object &#x3D; pjp.proceed(); System.out.println(&quot;my aspect 耗时：&quot; + (new Date().getTime() - startTime)); System.out.println(&quot;my aspect end&quot;); return object; &#125;&#125; ###顺序 ####结果打印 1234567891011121314151617181920212223my filter start //过滤器 preHandle //拦截器preHandlecom.sinosoft.controller.UserController$$EnhancerBySpringCGLIB$$89062ea2 getInfo //拦截器获取即将访问的controller的路径my aspect start //切片启动arg is “自己输入的参数” //切片可以拿到方法使用的参数值（可能不是请求参数，因为过滤器可能做了加解密）进入controller服务 // controller的业务逻辑my aspect 耗时：2 my aspect end //切片结束postHandle //拦截器postHandlemy interceptor 耗时：41 //拦截器postHandle结束afterCompletion //拦截器结束my interceptor 耗时：41 ex is null //无异常 my filter 耗时:55my filter finish //过滤器结束 ####顺序图片 Filter—&gt;Interceptor—&gt;ControllerAdvice—&gt;Aspect—&gt;Controller ControllerAdvice类一般做controller层的异常处理，Controller层处理完成后，按原路返回： Controller—&gt;Aspect—&gt;ControllerAdvice—&gt;Interceptor—&gt;Filter 总结 1、过滤器能够拿到原始的http请求和响应的信息，但是拿不到真正处理请求的方法的信息； 2、拦截器既能拿到原始的http请求和响应的信息，也能拿到真正处理请求的方法的信息，但是拿不到方法被调用的时候，那个被调用方法的参数的值； 3、切片可以拿到方法被调用的时候，方法参数传进来的值，但是拿不到原始的http请求和响应的对象。","categories":[],"tags":[{"name":"过滤器","slug":"过滤器","permalink":"https://18360732385.github.io/tags/%E8%BF%87%E6%BB%A4%E5%99%A8/"}]},{"title":"Kafka消息队列","slug":"Kafka消息队列","date":"2019-07-04T16:00:00.000Z","updated":"2019-07-24T16:00:00.000Z","comments":true,"path":"2019/07/05/Kafka消息队列/","link":"","permalink":"https://18360732385.github.io/2019/07/05/Kafka%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/","excerpt":"1.Kafka消息模型 ​ 早期的消息队列使用队列（Queue）作为消息载体，Kafka使用的是发布/订阅（Pub/Sub）模型，使用主题（Topic）作为消息的载体。","text":"1.Kafka消息模型 ​ 早期的消息队列使用队列（Queue）作为消息载体，Kafka使用的是发布/订阅（Pub/Sub）模型，使用主题（Topic）作为消息的载体。 2.Broker、Topic、Partition的概念 ​ broker：代理，可以看做一个独立的kafka实例，多个broker组成一个kafka cluster ​ topic：生产者将消息发送给指定的topic，消费者通过订阅该topic来消费消息 ​ partition：分区，属于topic的一部分，消息实际上存储在partition中。一个topic可以有多个partition，并且同一个topic下的partition可以分布在不同的broker上。即一个topic可以存在在多个broker上。 3.消费组 ​ 每个Consumer Group可以包含多个消费实例，即可以启动多个消息队列Kafka版 Consumer，并把参数group.id设置成相同的值。属于同一个Consumer Group的消费实例会负载消费订阅的Topic。 ​ 举例：Consumer Group A订阅了Topic A，并开启三个消费实例C1、C2、C3，则发送到Topic A的每条消息最终只会传给C1、C2、C3的某一个。Kafka默认会均匀地把消息传给各个消息实例，以做到消费负载均衡。 ​ Kafka负载消费的内部原理是，把订阅的Topic的分区，平均分配给各个消费实例。因此，消费实例的个数不要大于分区的数量，否则会有实例分配不到任何分区而处于空跑状态。这个负载均衡发生的时间，除了第一次启动上线之外，后续消费实例发生重启、增加、减少等变更时，都会触发一次负载均衡。 4.分区的多副本机制 ​ Kafka 为分区（Partition）引入了多副本（Replica）机制。分区（Partition）中的多个副本之间会有一个叫做 leader 的家伙，其他副本称为 follower。我们发送的消息会被发送到 leader 副本，然后 follower 副本才能从 leader 副本中拉取消息进行同步。 ​ 生产者和消费者只与 leader 副本交互。你可以理解为其他副本只是 leader 副本的拷贝，它们的存在只是为了保证消息存储的安全性。当 leader 副本发生故障时会从 follower 中选举出一个 leader,但是 follower 中如果有和 leader 同步程度达不到要求的参加不了 leader 的竞选。 5.Kafka如何保证消息的消费顺序 ​ kafka只能保证在同一个分区下的消息的顺序，因为消息加入分区中时都是尾加法，同时分配一个偏移量（offest），offest保证了分区内消息的顺序。 ​ 保证所有分区下的消息都具有顺序有2种方法： 1个topic对应一个partition（浪费了kafka的特性） 发送消息时指定同一个partition 6.Kafka如何保证消息不丢失 生产者消息丢失： ​ 生产者send消息时是异步操作，我们可以把它变成同步操作，更推荐使用回调函数，同时会有重试机制 消费者消息丢失： ​ 消费者拉取partition消息时，kafka利用offest+1确定当前消费的消息位置。当消费者将要消费消息时宕机，消息实际未消费，可offest自动提交。 ​ 解决方法是，关闭自动移交offest，改为手动提交。此时则会出现统一消息多次消费，所以要保证消费的业务幂等性。 Kafka自己丢失了消息: ​ leader宕机时，选举后新的leader缺少了一部分消息。可以设置acks=all 7.关于Kafka实践注意事项 ​ 参数设置和最佳实践： ​ https://help.aliyun.com/document_detail/68166.html?spm=a2c4g.11186623.6.612.2f0064abDtYOPD ​ https://blog.csdn.net/qq_35641192/article/details/80956244","categories":[],"tags":[{"name":"kafka","slug":"kafka","permalink":"https://18360732385.github.io/tags/kafka/"},{"name":"中间件","slug":"中间件","permalink":"https://18360732385.github.io/tags/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}]},{"title":"在千万级的数据库查询中，如何提高效率？","slug":"在千万级的数据库查询中，如何提高效率？","date":"2019-07-04T16:00:00.000Z","updated":"2019-07-04T16:00:00.000Z","comments":true,"path":"2019/07/05/在千万级的数据库查询中，如何提高效率？/","link":"","permalink":"https://18360732385.github.io/2019/07/05/%E5%9C%A8%E5%8D%83%E4%B8%87%E7%BA%A7%E7%9A%84%E6%95%B0%E6%8D%AE%E5%BA%93%E6%9F%A5%E8%AF%A2%E4%B8%AD%EF%BC%8C%E5%A6%82%E4%BD%95%E6%8F%90%E9%AB%98%E6%95%88%E7%8E%87%EF%BC%9F/","excerpt":"在千万级的数据库查询中，如何提高效率？1）数据库设计方面","text":"在千万级的数据库查询中，如何提高效率？1）数据库设计方面 1a. 对查询进行优化，应尽量避免全表扫描，首先应考虑在 where 及 order by 涉及的列上建立索引。 1b. 应尽量避免在 where 子句中对字段进行 null 值判断，否则将导致引擎放弃使用索引而进行全表扫描，如： select id from t where num is null 可以在 num 上设置默认值 0，确保表中 num 列没有 null 值，然后这样查询： select id from t where num&#x3D;0 1c. 并不是所有索引对查询都有效，SQL 是根据表中数据来进行查询优化的，当索引列有大量数据重复时,查询可能不会去利用索引，如一表中有字段 sex，其值male、female 几乎各一半，那么即使在 sex 上建了索引也对查询效率起不了作用。 1d. 索引并不是越多越好，索引固然可以提高相应的 select 的效率，但同时也降低了 insert 及 update 的效率，因为 insert 或 update 时有可能会重建索引，所以怎样建索引需要慎重考虑，视具体情况而定。一个表的索引数最好不要超过 6 个，若太多则应考虑一些不常使用到的列上建的索引是否有必要。 1e. 应尽可能的避免更新索引数据列，因为索引数据列的顺序就是表记录的物理存储顺序，一旦该列值改变将导致整个表记录的顺序的调整，会耗费相当大的资源。若应用系统需要频繁更新索引数据列，那么需要考虑是否应将该索引建为索引。 1f. 尽量使用数字型字段，若只含数值信息的字段尽量不要设计为字符型，这会降低查询和连接的性能，并会增加存储开销。这是因为引擎在处理查询和连接时会逐个比较字符串中每一个字符，而对于数字型而言只需要比较一次就够了。 1g. 尽可能的使用 varchar&#x2F;nvarchar 代替 char&#x2F;nchar ，因为首先变长字段存储空间小，可以节省存储空间，其次对于查询来说，在一个相对较小的字段内搜索效率显然要高些。 1h. 尽量使用表变量来代替临时表。如果表变量包含大量数据，请注意索引非常有限（只有主键索引）。 1i. 避免频繁创建和删除临时表，以减少系统表资源的消耗。 1j. 临时表并不是不可使用，适当地使用它们可以使某些例程更有效，例如，当需要重复引用大型表或常用表中的某个数据集时。但是，对于一次性事件，最好使用导出表。 1k. 在新建临时表时，如果一次性插入数据量很大，那么可以使用 select into 代替 create table，避免造成大量 log ，以提高速度；如果数据量不大，为了缓和系统表的资源，应先 create table，然后 insert。 1l. 如果使用到了临时表，在存储过程的最后务必将所有的临时表显式删除，先 truncate table ，然后 droptable ，这样可以避免系统表的较长时间锁定。 2)SQL 语句方面1a. 应尽量避免在 where 子句中使用!&#x3D;或&lt;&gt;操作符，否则将引擎放弃使用索引而进行全表扫描。 123456b. 应尽量避免在 where 子句中使用 or 来连接条件，否则将导致引擎放弃使用索引而进行全表扫描，如：select id from t where num&#x3D;10 or num&#x3D;20 可以这样查询： select id from t where num&#x3D;10 union all select id from t where num&#x3D;20补充：union和union all的区别UNION 操作符用于合并两个或多个 SELECT 语句的结果集。在多个select语句查询结果中，如果有重复值，union只会显示一个，而union all会全部都显示 12c. in 和 not in 也要慎用，否则会导致全表扫描，如： select id from t where num in(1,2,3) 对于连续的数值，能用 between 就不要用 in 了： select id from t where num between 1 and 3 1d. 下面的查询也将导致全表扫描： select id from t where name like '%abc%' 123e. 如果在 where 子句中使用参数，也会导致全表扫描。因为 SQL 只有在运行时才会解析局部变量，但优化程序不能将访问计划的选择推迟到运行时；它必须在编译时进行选择。然而，如果在编译时建立访问计划，变量的值还是未知的，因而无法作为索引选择的输入项。如下面语句将进行全表扫描：select id from t where num&#x3D;@num 可以改为强制查询使用索引： select id from t with(index(索引名)) where num&#x3D;@num 123f. 应尽量避免在 where 子句中对字段进行表达式操作，这将导致引擎放弃使用索引而进行全表扫描。如：select id from t where num&#x2F;2&#x3D;100 应改为:select id from t where num&#x3D;100*2 123g. 应尽量避免在where子句中对字段进行函数操作，这将导致引擎放弃使用索引而进行全表扫描。如： select id from t where substring(name,1,3)&#x3D; ‘ abc’ – name 以 abc 开头 的 idselect id from t where datediff(day,createdate,’2005-11-30′)&#x3D;0–‘2005-11-30’生成的 id 应改为: select id from t where name like ‘abc%’ select id from t where createdate&gt;&#x3D;’2005-11-30′ and createdate&lt;’2005-12-1′ 1h. 不要在 where 子句中的“&#x3D;”左边进行函数、算术运算或其他表达式运算，否则系统将可能无法正确使用索引。 1i. 不要写一些没有意义的查询，如需要生成一个空表结构： select col1,col2 into #t from t where 1&#x3D;0 这类代码不会返回任何结果集，但是会消耗系统资源的，应改成这样： create table #t(…) 123j. 很多时候用 exists 代替 in 是一个好的选择： select num from a where num in(select num from b) 用下面的语句替换： select num from a where exists(select 1 from b where num&#x3D;a.num) 1k. 任何地方都不要使用 select * from t ，用具体的字段列表代替“*”，不要返回用不到的任何字段。 1l. 尽量避免使用游标，因为游标的效率较差，如果游标操作的数据超过 1 万行，那么就应该考虑改写。 1m. 尽量避免向客户端返回大数据量，若数据量过大，应该考虑相应需求是否合理。 1n. 尽量避免大事务操作，提高系统并发能力。 3)java 方面：123456789a.尽可能的少造对象。b.合理摆正系统设计的位置。大量数据操作，和少量数据操作一定是分开的。大量的数据操作，肯定不是 ORM框架搞定的。c.使用 jDBC 连接数据库操作数据d.控制好内存，让数据流起来，而不是全部读到内存再处理，而是边读取边处理；e.合理利用内存，有的数据要缓存","categories":[],"tags":[{"name":"MySql进阶","slug":"MySql进阶","permalink":"https://18360732385.github.io/tags/MySql%E8%BF%9B%E9%98%B6/"}]},{"title":"SQL Select 语句完整的执行顺序","slug":"SQL Select 语句完整的执行顺序","date":"2019-07-03T16:00:00.000Z","updated":"2019-07-03T16:00:00.000Z","comments":true,"path":"2019/07/04/SQL Select 语句完整的执行顺序/","link":"","permalink":"https://18360732385.github.io/2019/07/04/SQL%20Select%20%E8%AF%AD%E5%8F%A5%E5%AE%8C%E6%95%B4%E7%9A%84%E6%89%A7%E8%A1%8C%E9%A1%BA%E5%BA%8F/","excerpt":"SQL Select 语句完整的执行顺序","text":"SQL Select 语句完整的执行顺序 select * from student where class_id = 1 group dy course having course in （01,02,03） order by score limit 3 查询1班语数外3门科目，分数排名3前三的学生信息 1、from 子句组装来自不同数据源的数据；2、where 子句基于指定的条件对记录行进行筛选；3、group by 子句将数据划分为多个分组；4、使用聚集函数进行计算；5、使用 having 子句筛选分组；6、计算所有的表达式；7、select 的字段；8、使用 order by 对结果集进行排序；9、limit对结果集进行分页取值#####注：having和where本质上都是条件刷选，前者一般和group by结合使用","categories":[],"tags":[{"name":"MySql进阶","slug":"MySql进阶","permalink":"https://18360732385.github.io/tags/MySql%E8%BF%9B%E9%98%B6/"}]},{"title":"MySQL语句优化","slug":"MySQL语句优化","date":"2019-07-01T16:00:00.000Z","updated":"2019-07-01T16:00:00.000Z","comments":true,"path":"2019/07/02/MySQL语句优化/","link":"","permalink":"https://18360732385.github.io/2019/07/02/MySQL%E8%AF%AD%E5%8F%A5%E4%BC%98%E5%8C%96/","excerpt":"1.任何地方都不要使用 select * from t ，用具体的字段列表代替“*”2.where 子句中可以对字段进行 null 值判断吗？","text":"1.任何地方都不要使用 select * from t ，用具体的字段列表代替“*”2.where 子句中可以对字段进行 null 值判断吗？ 123可以，比如 select id from t where num is null 这样的 sql 也是可以的。但是最好不要给数据库留 NULL，尽可能的使用 NOT NULL 填充数据库。不要以为 NULL 不需要空间，比如：char(100) 型，在字段建立时，空间就固定了，不管是否插入值（NULL 也包含在内），都是占用 100 个字符的空间的，如果是 varchar 这样的变长字段null 不占用空间。可以在 num 上设置默认值 0，确保表中 num 列没有 null 值，然后这样查询：select id from t where num&#x3D; 0。 ####3.下面的sql如何优化？ select * from admin left join log on admin.admin_id = log.admin_id where log.admin_id&gt;10 1使用 JOIN 时候，应该用小的结果驱动大的结果（left join 左边表结果尽量小如果有条件应该放到左边先处理，right join 同理反向），同时尽量把牵涉到多表联合的查询拆分多个 query（多个连表查询效率低，容易导致锁表和阻塞）。所以优化为： select * from (select * from admin where admin_id&gt;10) T1 lef join log on T1.admin_id =log.admin_id。 4.分页查询limit 的基数比较大时使用123例如：select * from admin order by admin_id limit 100000,10优化为：select * from admin where admin_id between 100000 and 100010 order by admin_id。当然，仅限于id为int类型的。 5.尽量避免在列上做运算，这样导致索引失效12例如：select * from admin where year(admin_time)&gt;2014优化为： select * from admin where admin_time&gt; &#39;2014-01-01′","categories":[],"tags":[{"name":"MySql进阶","slug":"MySql进阶","permalink":"https://18360732385.github.io/tags/MySql%E8%BF%9B%E9%98%B6/"}]},{"title":"Mysql性能优化","slug":"Mysql性能优化","date":"2019-06-30T16:00:00.000Z","updated":"2019-06-30T16:00:00.000Z","comments":true,"path":"2019/07/01/Mysql性能优化/","link":"","permalink":"https://18360732385.github.io/2019/07/01/Mysql%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/","excerpt":"1、当只要一行数据时使用 limit 1","text":"1、当只要一行数据时使用 limit 1 1查询时如果已知会得到一条数据，这种情况下加上 limit 1 会增加性能。因为 mysql 数据库引擎会在找到一条结果停止搜索，而不是继续查询下一条是否符合标准直到所有记录查询完毕。 2、选择正确的数据库引擎 Mysql 中有两个引擎 MyISAM 和 InnoDB，每个引擎有利有弊。 123MyISAM 适用于一些大量查询的应用，但对于有大量写功能的应用不是很好。甚至你只需要update 一个字段整个表都会被锁起来。而别的进程就算是读操作也不行要等到当前 update 操作完成之后才能继续进行。另外，MyISAM 对于 select count(*)这类操作是超级快的。InnoDB 的趋势会是一个非常复杂的存储引擎，对于一些小的应用会比 MyISAM 还慢，但是支持“行锁”，所以在写操作比较多的时候会比较优秀。并且，它支持很多的高级应用，例如：事物。 3.用 not exists 代替 not in1Not exists 用到了连接能够发挥已经建立好的索引的作用，not in 不能使用索引。Not in 是最慢的方式要同每条记录比较，在数据量比较大的操作中不建议使用这种方式。 4.对操作符的优化，尽量不采用不利于索引的操作符123如：in 、not in、 is null、 is not null、 &lt;、&gt; 等某个字段总要拿来搜索，为其建立索引。Mysql 中可以利用 alter table 语句来为表中的字段添加索引，语法为：alter table 表名 add index (字段名)；","categories":[],"tags":[{"name":"MySql进阶","slug":"MySql进阶","permalink":"https://18360732385.github.io/tags/MySql%E8%BF%9B%E9%98%B6/"}]},{"title":"Mybatis实战笔记16.分页工具PagerHelper","slug":"Mybatis实战笔记16.分页工具PagerHelper","date":"2019-06-23T16:00:00.000Z","updated":"2019-06-23T16:00:00.000Z","comments":true,"path":"2019/06/24/Mybatis实战笔记16.分页工具PagerHelper/","link":"","permalink":"https://18360732385.github.io/2019/06/24/Mybatis%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B016.%E5%88%86%E9%A1%B5%E5%B7%A5%E5%85%B7PagerHelper/","excerpt":"maven依赖","text":"maven依赖 123456&lt;!--MyBatis分页插件--&gt;&lt;dependency&gt; &lt;groupId&gt;com.github.pagehelper&lt;&#x2F;groupId&gt; &lt;artifactId&gt;pagehelper&lt;&#x2F;artifactId&gt; &lt;version&gt;5.1.2&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; 使用规则 ​ 自动的对PageHelper.startPage 方法下的第一个sql 查询进行分页，所以查询语句一定要紧跟着上一句 12345678&#x2F;&#x2F;pageNum:当前页码 pageSize:每页查询条数PageHelper.startPage(pageNum, pageSize);&#x2F;&#x2F;之后进行查询操作将自动进行分页List&lt;PmsBrand&gt; brandList &#x3D; brandMapper.selectByExample(new PmsBrandExample());&#x2F;&#x2F;通过构造PageInfo对象获取分页信息，如当前页码，总页数，总条数PageInfo&lt;PmsBrand&gt; pageInfo &#x3D; new PageInfo&lt;PmsBrand&gt;(brandList); PagerHelper的内部类PageInfo123456789101112131415161718192021222324252627282930313233343536373839404142public class PageInfo&lt;T&gt; implements Serializable &#123; private static final long serialVersionUID &#x3D; 1L; &#x2F;&#x2F;当前页 private int pageNum; &#x2F;&#x2F;每页的数量 private int pageSize; &#x2F;&#x2F;当前页的数量 private int size; &#x2F;&#x2F;由于startRow 和endRow 不常用，这里说个具体的用法 &#x2F;&#x2F;可以在页面中&quot;显示startRow 到endRow 共size 条数据&quot; &#x2F;&#x2F;当前页面第一个元素在数据库中的行号 private int startRow; &#x2F;&#x2F;当前页面最后一个元素在数据库中的行号 private int endRow; &#x2F;&#x2F;总记录数 private long total; &#x2F;&#x2F;总页数 private int pages; &#x2F;&#x2F;结果集 private List&lt;T&gt; list; &#x2F;&#x2F;前一页 private int prePage; &#x2F;&#x2F;下一页 private int nextPage; &#x2F;&#x2F;是否为第一页 private boolean isFirstPage &#x3D; false; &#x2F;&#x2F;是否为最后一页 private boolean isLastPage &#x3D; false; &#x2F;&#x2F;是否有前一页 private boolean hasPreviousPage &#x3D; false; &#x2F;&#x2F;是否有下一页 private boolean hasNextPage &#x3D; false; &#x2F;&#x2F;导航页码数 private int navigatePages; &#x2F;&#x2F;所有导航页号 private int[] navigatepageNums; &#x2F;&#x2F;导航条上的第一页 private int navigateFirstPage; &#x2F;&#x2F;导航条上的最后一页 private int navigateLastPage;&#125;","categories":[{"name":"MyBatis","slug":"MyBatis","permalink":"https://18360732385.github.io/categories/MyBatis/"}],"tags":[]},{"title":"Java8的Stream和Lambda表达式","slug":"Java8的Stream和Lambda表达式","date":"2019-06-21T16:00:00.000Z","updated":"2019-06-21T16:00:00.000Z","comments":true,"path":"2019/06/22/Java8的Stream和Lambda表达式/","link":"","permalink":"https://18360732385.github.io/2019/06/22/Java8%E7%9A%84Stream%E5%92%8CLambda%E8%A1%A8%E8%BE%BE%E5%BC%8F/","excerpt":"集合的两种方式生成流 stream() − 为集合创建串行流 parallelStream() - 为集合创建并行流","text":"集合的两种方式生成流 stream() − 为集合创建串行流 parallelStream() - 为集合创建并行流 ​ 中间操作 主要有以下方法（此类型方法返回的都是Stream）：map (mapToInt, flatMap 等)、 filter、 distinct、 sorted、 peek、 limit、 skip、 parallel、 sequential、 unordered ​ 终止操作 主要有以下方法：forEach、 forEachOrdered、 toArray、 reduce、 collect、 min、 max、 count、 anyMatch、 allMatch、 noneMatch、 findFirst、 findAny、 iterator 创建实体类Student和数据 ​ 实体类Student有参数：id，name，age，address 12345678910Student s1 &#x3D; new Student(1L, &quot;肖战&quot;, 15, &quot;浙江&quot;);Student s2 &#x3D; new Student(2L, &quot;王一博&quot;, 15, &quot;湖北&quot;);Student s3 &#x3D; new Student(3L, &quot;杨紫&quot;, 17, &quot;北京&quot;);Student s4 &#x3D; new Student(4L, &quot;李现&quot;, 17, &quot;浙江&quot;);List&lt;Student&gt; students &#x3D; new ArrayList&lt;&gt;();students.add(s1);students.add(s2);students.add(s3);students.add(s4); filter（筛选） ​ 刷选地址是浙江的学生集合List 12List&lt;String&gt; addresses &#x3D; students.stream().filter(s -&gt; &quot;浙江&quot;.equals(s.getAddress())).collect(Collectors.toList()); map(转换) 结果：地址：浙江 ​ 地址：湖北 地址：北京 12List&lt;String&gt; addresses &#x3D; students.stream().map(s -&gt;&quot;住址:&quot;+s.getAddress()).collect(Collectors.toList());addresses.forEach(a -&gt;System.out.println(a)); distinct(去重) ​ 注意：实体类的去重是无效的，必须要重写实体类的equals和hashCode()方法 123&#x2F;&#x2F;简单字符串的去重List&lt;String&gt; list &#x3D; Arrays.asList(&quot;111&quot;,&quot;222&quot;,&quot;333&quot;,&quot;111&quot;,&quot;222&quot;);list.stream().distinct().forEach(System.out::println); 12&#x2F;&#x2F;实体类去重students.stream().distinct().forEach(System.out::println); 12345678910111213141516&#x2F;&#x2F;实体类去重必须重写equals和hashCode()方法 @Override public boolean equals(Object o) &#123; if (this &#x3D;&#x3D; o) return true; if (o &#x3D;&#x3D; null || getClass() !&#x3D; o.getClass()) return false; Student student &#x3D; (Student) o; return age &#x3D;&#x3D; student.age &amp;&amp; Objects.equals(id, student.id) &amp;&amp; Objects.equals(name, student.name) &amp;&amp; Objects.equals(address, student.address); &#125; @Override public int hashCode() &#123; return Objects.hash(id, name, age, address); &#125; sorted(排序)123&#x2F;&#x2F;简单字符串排序（默认排序）List&lt;String&gt; list &#x3D; Arrays.asList(&quot;333&quot;,&quot;222&quot;,&quot;111&quot;);list.stream().sorted().forEach(System.out::println); 12345&#x2F;&#x2F;按id倒序后，再按age倒序students.stream() .sorted((stu1,stu2) -&gt;Long.compare(stu2.getId(), stu1.getId())) .sorted((stu1,stu2) -&gt; Integer.compare(stu2.getAge(),stu1.getAge())) .forEach(System.out::println); limit（限制返回个数） 集合limit，返回前几个元素 结果：333 ​ 222 12List&lt;String&gt; list &#x3D; Arrays.asList(&quot;333&quot;,&quot;222&quot;,&quot;111&quot;);list.stream().limit(2).forEach(System.out::println); skip(删除元素) 删除前几个元素 结果：111 12List&lt;String&gt; list &#x3D; Arrays.asList(&quot;333&quot;,&quot;222&quot;,&quot;111&quot;);list.stream().skip(2).forEach(System.out::println); reduce(聚合)123List&lt;String&gt; list &#x3D; Arrays.asList(&quot;欢&quot;,&quot;迎&quot;,&quot;你&quot;);String appendStr &#x3D; list.stream().reduce(&quot;北京&quot;,(a,b) -&gt; a+b);System.out.println(appendStr); min和max(求最小/大值) 求集合中的元素最小/大值 12Student minS &#x3D; students.stream().min((stu1,stu2) -&gt;Integer.compare(stu1.getAge(),stu2.getAge())).get();System.out.println(minS.toString()); anyMatch/allMatch/noneMatch（匹配） anyMatch：Stream 中任意一个元素符合传入的 predicate，返回 true allMatch：Stream 中全部元素符合传入的 predicate，返回 true noneMatch：Stream 中没有一个元素符合传入的 predicate，返回 true 结果：有湖北人 ​ 所有学生都满15周岁 ​ 没有叫杨洋的同学 123456789101112Boolean anyMatch &#x3D; students.stream().anyMatch(s -&gt;&quot;湖北&quot;.equals(s.getAddress())); if (anyMatch) &#123; System.out.println(&quot;有湖北人&quot;); &#125; Boolean allMatch &#x3D; students.stream().allMatch(s -&gt; s.getAge()&gt;&#x3D;15); if (allMatch) &#123; System.out.println(&quot;所有学生都满15周岁&quot;); &#125; Boolean noneMatch &#x3D; students.stream().noneMatch(s -&gt; &quot;杨洋&quot;.equals(s.getName())); if (noneMatch) &#123; System.out.println(&quot;没有叫杨洋的同学&quot;); &#125;","categories":[],"tags":[{"name":"Java8新特性","slug":"Java8新特性","permalink":"https://18360732385.github.io/tags/Java8%E6%96%B0%E7%89%B9%E6%80%A7/"}]},{"title":"Mybatis实战笔记15.补充：trim标记","slug":"Mybatis实战笔记15.补充：trim标记","date":"2019-06-20T16:00:00.000Z","updated":"2019-06-20T16:00:00.000Z","comments":true,"path":"2019/06/21/Mybatis实战笔记15.补充：trim标记/","link":"","permalink":"https://18360732385.github.io/2019/06/21/Mybatis%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B015.%E8%A1%A5%E5%85%85%EF%BC%9Atrim%E6%A0%87%E8%AE%B0/","excerpt":"代码演示","text":"代码演示 123456789101112131415161718trim标记是一个格式化的标记，可以完成set或者是where标记的功能，如下代码： select * from user &lt;trim prefix&#x3D;&quot;WHERE&quot; prefixoverride&#x3D;&quot;AND |OR&quot;&gt; &lt;if test&#x3D;&quot;name !&#x3D; null and name.length()&gt;0&quot;&gt; AND name&#x3D;#&#123;name&#125;&lt;&#x2F;if&gt; &lt;if test&#x3D;&quot;gender !&#x3D; null and gender.length()&gt;0&quot;&gt; AND gender&#x3D;#&#123;gender&#125;&lt;&#x2F;if&gt; &lt;&#x2F;trim&gt; 假如说name和gender的值都不为null的话打印的SQL为： select * from user where name &#x3D; &#39;xx&#39; and gender &#x3D; &#39;xx&#39; where name中间是不存在第一个and的，上面两个属性的意思如下： prefix：前缀 (如果name和gender都为空,则前缀where取消) prefixoverride：去掉第一个and或者是or 12345678910111213141516 update user &lt;trim prefix&#x3D;&quot;set&quot; suffixoverride&#x3D;&quot;,&quot; suffix&#x3D;&quot; where id &#x3D; #&#123;id&#125; &quot;&gt; &lt;if test&#x3D;&quot;name !&#x3D; null and name.length()&gt;0&quot;&gt; name&#x3D;#&#123;name&#125; , &lt;&#x2F;if&gt; &lt;if test&#x3D;&quot;gender !&#x3D; null and gender.length()&gt;0&quot;&gt; gender&#x3D;#&#123;gender&#125; , &lt;&#x2F;if&gt; &lt;&#x2F;trim&gt; 假如说name和gender的值都不为null的话打印的SQL为： update user set name&#x3D;&#39;xx&#39; , gender&#x3D;&#39;xx&#39; where id&#x3D;&#39;x&#39; &#39;xx&#39; where间不存在逗号，而且自动加了一个set前缀和where后缀，上面三个属性的意义如下，其中prefix意义如上： suffixoverride：去掉最后一个逗号（也可以是其他的标记，就像是上面前缀中的and一样） suffix：后缀","categories":[{"name":"MyBatis","slug":"MyBatis","permalink":"https://18360732385.github.io/categories/MyBatis/"}],"tags":[]},{"title":"Mybatis实战笔记14.多条数据操作（如checkbox，传参的是集合，可以用foreach遍历）","slug":"Mybatis实战笔记14.多条数据操作（如checkbox，传参的是集合，可以用foreach遍历）","date":"2019-06-18T16:00:00.000Z","updated":"2019-06-18T16:00:00.000Z","comments":true,"path":"2019/06/19/Mybatis实战笔记14.多条数据操作（如checkbox，传参的是集合，可以用foreach遍历）/","link":"","permalink":"https://18360732385.github.io/2019/06/19/Mybatis%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B014.%E5%A4%9A%E6%9D%A1%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C%EF%BC%88%E5%A6%82checkbox%EF%BC%8C%E4%BC%A0%E5%8F%82%E7%9A%84%E6%98%AF%E9%9B%86%E5%90%88%EF%BC%8C%E5%8F%AF%E4%BB%A5%E7%94%A8foreach%E9%81%8D%E5%8E%86%EF%BC%89/","excerpt":"代码演示","text":"代码演示 1234567891011121314151617181920&lt;delete id&#x3D;&quot;deleteByIds&quot; parameterType&#x3D;&quot;CustomerVo&quot;&gt; delete from cst_customer &lt;!-- foreach标签，进行遍历 --&gt; &lt;!-- collection：遍历的集合，这里是QueryVo的ids属性 --&gt; &lt;!-- item：遍历的项目，可以随便写，，但是和后面的#&#123;&#125;里面要一致 --&gt; &lt;!-- open：在前面添加的sql片段 --&gt; &lt;!-- close：在结尾处添加的sql片段 --&gt; &lt;!-- separator：指定遍历的元素之间使用的分隔符 --&gt; &lt;where&gt; &lt;if test&#x3D;&quot;ids !&#x3D;null&quot; &gt; &lt;foreach collection&#x3D;&quot;ids&quot; item&#x3D;&quot;id&quot; open&#x3D;&quot;cust_id in (&quot; close&#x3D;&quot;)&quot; separator&#x3D;&quot;,&quot;&gt; #&#123;id&#125; &lt;&#x2F;foreach&gt; &lt;&#x2F;if&gt; &lt;&#x2F;where&gt; &lt;&#x2F;delete&gt; 1234567891011121314151617&lt;update id&#x3D;&quot;updateUserStatus&quot; parameterType&#x3D;&quot;user&quot;&gt; update tuser &lt;set&gt; &lt;choose&gt; &lt;when test&#x3D;&quot;role.roleId&#x3D;&#x3D;1&quot;&gt; status&#x3D;1 &lt;&#x2F;when&gt; &lt;otherwise&gt; status&#x3D;0 &lt;&#x2F;otherwise&gt; &lt;&#x2F;choose&gt; &lt;&#x2F;set&gt; &lt;where&gt; &lt;!-- 这里用不用where都一样 --&gt; roleId&#x3D;#&#123;role.roleId&#125; &lt;&#x2F;where&gt; &lt;&#x2F;update&gt;","categories":[{"name":"MyBatis","slug":"MyBatis","permalink":"https://18360732385.github.io/categories/MyBatis/"}],"tags":[]},{"title":"Mybatis实战笔记13.多条件检索（动态sql语句）","slug":"Mybatis实战笔记13.多条件检索（动态sql语句）","date":"2019-06-16T16:00:00.000Z","updated":"2019-06-16T16:00:00.000Z","comments":true,"path":"2019/06/17/Mybatis实战笔记13.多条件检索（动态sql语句）/","link":"","permalink":"https://18360732385.github.io/2019/06/17/Mybatis%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B013.%E5%A4%9A%E6%9D%A1%E4%BB%B6%E6%A3%80%E7%B4%A2%EF%BC%88%E5%8A%A8%E6%80%81sql%E8%AF%AD%E5%8F%A5%EF%BC%89/","excerpt":"代码演示","text":"代码演示 12345CustomerVo实体类（检索条件类） private Customer customer; private Date startDate; private Date endDate; private List&lt;Integer&gt; ids; 123456789101112131415161718192021&lt;resultMap type&#x3D;&quot;customer&quot; id&#x3D;&quot;customerDictMap&quot; autoMapping&#x3D;&quot;true&quot;&gt; &lt;association property&#x3D;&quot;custSource&quot; javaType&#x3D;&quot;BaseDict&quot; autoMapping&#x3D;&quot;true&quot;&gt; &lt;!-- column&#x3D;&quot;sid&quot;代表的是数据库表的列(别)名 property&#x3D;&quot;dict_id代表的类的字段名(这里是驼峰设置了) --&gt; &lt;id column&#x3D;&quot;sid&quot; property&#x3D;&quot;dict_id&quot;&#x2F;&gt; &lt;result column&#x3D;&quot;sname&quot; property&#x3D;&quot;dict_item_name&quot;&#x2F;&gt; &lt;&#x2F;association&gt; &lt;association property&#x3D;&quot;custIndustry&quot; javaType&#x3D;&quot;BaseDict&quot; autoMapping&#x3D;&quot;true&quot;&gt; &lt;id column&#x3D;&quot;iid&quot; property&#x3D;&quot;dict_id&quot;&#x2F;&gt; &lt;result column&#x3D;&quot;iname&quot; property&#x3D;&quot;dict_item_name&quot;&#x2F;&gt; &lt;&#x2F;association&gt; &lt;association property&#x3D;&quot;custLevel&quot; javaType&#x3D;&quot;BaseDict&quot; autoMapping&#x3D;&quot;true&quot;&gt; &lt;id column&#x3D;&quot;lid&quot; property&#x3D;&quot;dict_id&quot;&#x2F;&gt; &lt;result column&#x3D;&quot;lname&quot; property&#x3D;&quot;dict_item_name&quot;&#x2F;&gt; &lt;&#x2F;association&gt; &lt;&#x2F;resultMap&gt; 12345678910111213141516171819202122232425262728293031323334&lt;select id&#x3D;&quot;queryCustomers&quot; parameterType&#x3D;&quot;CustomerVo&quot; resultMap&#x3D;&quot;customerDictMap&quot;&gt; select c.*,ds.dict_item_name sname,ds.dict_id sid, di.dict_item_name iname,di.dict_id iid, dl.dict_item_name lname,dl.dict_id lid from cst_customer c,base_dict ds,base_dict di, base_dict dl where c.cust_source&#x3D;ds.dict_id and c.cust_industry&#x3D;di.dict_id and c.cust_level&#x3D;dl.dict_id &lt;if test&#x3D;&quot;customer !&#x3D;null&quot;&gt; &lt;!-- test里面的内容是实体类的字段值及属性 --&gt; &lt;if test&#x3D;&quot;customer.custName !&#x3D;null and customer.custName !&#x3D;&#39;&#39;&quot;&gt; and c.cust_name like &#39;%$&#123;customer.custName&#125;%&#39; &lt;&#x2F;if&gt; &lt;if test&#x3D;&quot;customer.custIndustry !&#x3D; null and customer.custIndustry.dict_id !&#x3D; null and customer.custIndustry.dict_id !&#x3D; &#39;&#39;&quot;&gt; and c.cust_industry &#x3D; #&#123;customer.custIndustry.dict_id&#125; &lt;&#x2F;if&gt; &lt;&#x2F;if&gt; &lt;if test&#x3D;&quot;startDate !&#x3D; null&quot;&gt; and c.cust_createDate &gt;&#x3D; #&#123;startDate&#125; &lt;&#x2F;if&gt; &lt;if test&#x3D;&quot;endDate !&#x3D; null&quot;&gt; &lt;!--注意这里的小于号不是&lt;，而是&lt;代替--&gt; and c.cust_createDate &lt;&#x3D; #&#123;endDate&#125; &lt;&#x2F;if&gt; order by c.cust_id desc &lt;&#x2F;select&gt;","categories":[{"name":"MyBatis","slug":"MyBatis","permalink":"https://18360732385.github.io/categories/MyBatis/"}],"tags":[]},{"title":"Mybatis实战笔记12.sql片段","slug":"Mybatis实战笔记12.sql片段","date":"2019-06-15T16:00:00.000Z","updated":"2019-06-15T16:00:00.000Z","comments":true,"path":"2019/06/16/Mybatis实战笔记12.sql片段/","link":"","permalink":"https://18360732385.github.io/2019/06/16/Mybatis%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B012.sql%E7%89%87%E6%AE%B5/","excerpt":"代码演示","text":"代码演示 123456789101112131415&lt;sql id&#x3D;&quot;userColumn&quot;&gt; u.userId, u.userCode, u.name &lt;&#x2F;sql&gt; &lt;select id&#x3D;&quot;queryUser&quot; parameterType&#x3D;&quot;String&quot; resultType&#x3D;&quot;user&quot;&gt; select &lt;include refid&#x3D;&quot;userColumn&quot; &#x2F;&gt; from tuser u &lt;where&gt; &lt;!-- 判断 - 若为第一个条件 自动去掉and --&gt; &lt;!-- 传参是单个参数，类型为String。这里的value不能写userCode的字段 名,因为不是String类型 --&gt; &lt;if test&#x3D;&quot;value !&#x3D;null and value !&#x3D;&#39;&#39;&quot;&gt; and userCode&#x3D;#&#123;userCode&#125; &lt;&#x2F;if&gt; &lt;&#x2F;where&gt; &lt;&#x2F;select&gt;","categories":[{"name":"MyBatis","slug":"MyBatis","permalink":"https://18360732385.github.io/categories/MyBatis/"}],"tags":[]},{"title":"Mybatis实战笔记11.比较全的多表查询（订单管理员查看[今天]所有订单, 及其详细情况 - 打包快递）","slug":"Mybatis实战笔记11.比较全的多表查询（订单管理员查看[今天]所有订单, 及其详细情况 - 打包快递）","date":"2019-06-14T16:00:00.000Z","updated":"2019-06-14T16:00:00.000Z","comments":true,"path":"2019/06/15/Mybatis实战笔记11.比较全的多表查询（订单管理员查看[今天]所有订单, 及其详细情况 - 打包快递）/","link":"","permalink":"https://18360732385.github.io/2019/06/15/Mybatis%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B011.%E6%AF%94%E8%BE%83%E5%85%A8%E7%9A%84%E5%A4%9A%E8%A1%A8%E6%9F%A5%E8%AF%A2%EF%BC%88%E8%AE%A2%E5%8D%95%E7%AE%A1%E7%90%86%E5%91%98%E6%9F%A5%E7%9C%8B[%E4%BB%8A%E5%A4%A9]%E6%89%80%E6%9C%89%E8%AE%A2%E5%8D%95,%20%E5%8F%8A%E5%85%B6%E8%AF%A6%E7%BB%86%E6%83%85%E5%86%B5%20-%20%E6%89%93%E5%8C%85%E5%BF%AB%E9%80%92%EF%BC%89/","excerpt":"代码演示","text":"代码演示 1234567891011121314order实体类的字段 private int orderId; private String orderNo; private String name; &#x2F;&#x2F; 收货人姓名 private String address; private String mobile; private User user; private List&lt;OrderDetail&gt; details; details实体类字段 private int detailId; private int itemNum; private Order order; private Item item; 12345678910111213141516&lt;!-- m:n 订单管理员查看[今天]所有订单, 及其详细情况 - 打包快递 --&gt; &lt;resultMap type&#x3D;&quot;order&quot; id&#x3D;&quot;todayOrdersDetailsItemsMap&quot; autoMapping&#x3D;&quot;true&quot;&gt; &lt;result column&#x3D;&quot;sname&quot; property&#x3D;&quot;name&quot;&#x2F;&gt; &lt;association property&#x3D;&quot;user&quot; javaType&#x3D;&quot;User&quot; autoMapping&#x3D;&quot;true&quot;&gt; &lt;&#x2F;association&gt; &lt;collection property&#x3D;&quot;details&quot; javaType&#x3D;&quot;list&quot; ofType&#x3D;&quot;OrderDetail&quot; autoMapping&#x3D;&quot;true&quot;&gt; &lt;association property&#x3D;&quot;item&quot; javaType&#x3D;&quot;item&quot; autoMapping&#x3D;&quot;true&quot;&gt; &lt;&#x2F;association&gt; &lt;&#x2F;collection&gt; &lt;&#x2F;resultMap&gt; 12345678910&lt;select id&#x3D;&quot;queryTodayOrdersDetailsItems&quot; parameterType&#x3D;&quot;int&quot; resultMap&#x3D;&quot;todayOrdersDetailsItemsMap&quot;&gt; select o.userId,o.orderId,o.orderNo,o.name as sname,o.address, o.mobile,u.userCode,u.name ,i.itemId, i.itemName,i.price,i.info,d.itemNum from tuser u,torder o,item i,orderDetail d where u.userId&#x3D;o.userId and o.orderId&#x3D;d.orderId and d.itemId&#x3D;i.itemId order by o.orderId &lt;&#x2F;select&gt;","categories":[{"name":"MyBatis","slug":"MyBatis","permalink":"https://18360732385.github.io/categories/MyBatis/"}],"tags":[]},{"title":"Mybatis实战笔记10.懒加载机制（演变：管理员查询用户及其订单）","slug":"Mybatis实战笔记10.懒加载机制（演变：管理员查询用户及其订单）","date":"2019-06-13T16:00:00.000Z","updated":"2019-06-13T16:00:00.000Z","comments":true,"path":"2019/06/14/Mybatis实战笔记10.懒加载机制（演变：管理员查询用户及其订单）/","link":"","permalink":"https://18360732385.github.io/2019/06/14/Mybatis%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B010.%E6%87%92%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6%EF%BC%88%E6%BC%94%E5%8F%98%EF%BC%9A%E7%AE%A1%E7%90%86%E5%91%98%E6%9F%A5%E8%AF%A2%E7%94%A8%E6%88%B7%E5%8F%8A%E5%85%B6%E8%AE%A2%E5%8D%95%EF%BC%89/","excerpt":"代码演示","text":"代码演示 12345678910111213141516171819&lt;!-- 懒加载机制 按需加载查询数据，是优化数据库的方法。比如查询一个结果集，但是只会用到其中一个结果，那么懒加载就只会查询那个使用的结果，其他结果不会查询 1.全局文件中开启lazy机制 2.使用懒加载机制 - 必须使用resultMap 3.每个查询都要分开 --&gt; &lt;resultMap type&#x3D;&quot;user&quot; id&#x3D;&quot;userRoleOrdersLazyMap&quot; autoMapping&#x3D;&quot;true&quot;&gt; &lt;!-- lazy查询 select&#x3D;&quot;selectUserRole&quot; column&#x3D;&quot;roleId&quot; --&gt; &lt;association property&#x3D;&quot;role&quot; javaType&#x3D;&quot;Role&quot; select&#x3D;&quot;selectUserRole&quot; column&#x3D;&quot;roleId&quot;&gt; &lt;&#x2F;association&gt; &lt;collection property&#x3D;&quot;orders&quot; javaType&#x3D;&quot;list&quot; ofType&#x3D;&quot;Order&quot; select&#x3D;&quot;selectUserOrders&quot; column&#x3D;&quot;userId&quot;&gt; &lt;&#x2F;collection&gt; &lt;&#x2F;resultMap&gt; 123456789101112131415161718&lt;!--查询也会分成3部分，每一步前后相接，对应上述的resultMap中的select--&gt;&lt;select id&#x3D;&quot;queryUserRoleOrdersByLazy&quot; parameterType&#x3D;&quot;int&quot; resultMap&#x3D;&quot;userRoleOrdersLazyMap&quot;&gt; select u.userId, u.userCode, u.name, u.roleId from tuser u where u.userId &#x3D; #&#123;value&#125; &lt;&#x2F;select&gt; &lt;select id&#x3D;&quot;selectUserRole&quot; resultType&#x3D;&quot;role&quot;&gt; select roleId,roleName from role where roleId&#x3D;#&#123;roleId&#125; &lt;&#x2F;select&gt; &lt;select id&#x3D;&quot;selectUserOrders&quot; resultType&#x3D;&quot;order&quot;&gt; select * （*应写全字段） from torder where userId&#x3D;#&#123;userId&#125; &lt;&#x2F;select&gt;","categories":[{"name":"MyBatis","slug":"MyBatis","permalink":"https://18360732385.github.io/categories/MyBatis/"}],"tags":[]},{"title":"Mybatis实战笔记9.开启二级缓存：cache","slug":"Mybatis实战笔记9.开启二级缓存：cache","date":"2019-06-12T16:00:00.000Z","updated":"2019-06-12T16:00:00.000Z","comments":true,"path":"2019/06/13/Mybatis实战笔记9.开启二级缓存：cache/","link":"","permalink":"https://18360732385.github.io/2019/06/13/Mybatis%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B09.%E5%BC%80%E5%90%AF%E4%BA%8C%E7%BA%A7%E7%BC%93%E5%AD%98%EF%BC%9Acache/","excerpt":"代码演示","text":"代码演示 12345678910111213&lt;!-- 开启对象二级缓存 eviction是缓存的淘汰算法，可选值有&quot;LRU&quot;、&quot;FIFO&quot;、&quot;SOFT&quot;、&quot;WEAK&quot;，缺省值是LRU LRU最近最少使用，移除最长时间不被使用的对象。默认 FIFO先进先出。 SOFT软引用，基于垃圾回收器状态和软引用规则来移除对象。 WEAK弱引用，基于垃圾回收器状态和弱引用规则 readOnly&#x3D;false （查询时，可以设置只读，即不允许改值） size&#x3D;&quot;1024&quot; 缓存多少个对象，默认值1024 flushInterval缓存过期时间，单位是毫秒（1s&#x3D;1000ms），默认为空，只要容量够则永不过期。 --&gt; &lt;cache&#x2F;&gt; 123456789101112&lt;!-- 二级缓存的刷新机制 设置增删改 - 刷新缓存策略 增删改操作后的缓存过期（数据不一致），需要立即刷新 flushCache&#x3D;true - 变化立即刷新,默认 flushCache&#x3D;false - 不会立即刷新--&gt; &lt;insert id&#x3D;&quot;addUser&quot; parameterType&#x3D;&quot;user&quot; flushCache&#x3D;&quot;true&quot;&gt; insert into tuser() values(null,#&#123;userCode&#125;,#&#123;password&#125;,#&#123;name&#125;,1,#&#123;role.roleId&#125;) &lt;&#x2F;insert&gt;","categories":[{"name":"MyBatis","slug":"MyBatis","permalink":"https://18360732385.github.io/categories/MyBatis/"}],"tags":[]},{"title":"Mybatis实战笔记8.自增长主键返回","slug":"Mybatis实战笔记8.自增长主键返回","date":"2019-06-10T16:00:00.000Z","updated":"2019-06-10T16:00:00.000Z","comments":true,"path":"2019/06/11/Mybatis实战笔记8.自增长主键返回/","link":"","permalink":"https://18360732385.github.io/2019/06/11/Mybatis%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B08.%E8%87%AA%E5%A2%9E%E9%95%BF%E4%B8%BB%E9%94%AE%E8%BF%94%E5%9B%9E/","excerpt":"代码演示","text":"代码演示 1234567891011121314151617&lt;!-- 对象中关联对象 - 关联对象名.属性 --&gt; &lt;insert id&#x3D;&quot;addUser&quot; parameterType&#x3D;&quot;user&quot;&gt; &lt;!-- mysql自增主键返回 --&gt; &lt;!-- selectKey 标签实现主键返回 --&gt; &lt;!-- keyColumn:主键对应的表中的哪一列 --&gt; &lt;!-- keyProperty：主键对应的pojo中的哪一个属性 --&gt; &lt;!-- order：设置在执行insert语句前执行查询id的sql，还是在执行insert语句之后执行查询id的sql AFTER 和 BRFORE--&gt; &lt;!-- resultType：设置返回的id的类型 --&gt; &lt;selectKey keyColumn&#x3D;&quot;userId&quot; keyProperty&#x3D;&quot;userId&quot; order&#x3D;&quot;AFTER&quot; resultType&#x3D;&quot;int&quot;&gt; select LAST_INSERT_ID() &lt;&#x2F;selectKey&gt; insert into tuser values(null, #&#123;userCode&#125;, #&#123;password&#125;, #&#123;name&#125;, 1, #&#123;role.roleId&#125;) &lt;&#x2F;insert&gt;","categories":[{"name":"MyBatis","slug":"MyBatis","permalink":"https://18360732385.github.io/categories/MyBatis/"}],"tags":[]},{"title":"Mybatis实战笔记7.模糊查询","slug":"Mybatis实战笔记7.模糊查询","date":"2019-06-09T16:00:00.000Z","updated":"2019-06-09T16:00:00.000Z","comments":true,"path":"2019/06/10/Mybatis实战笔记7.模糊查询/","link":"","permalink":"https://18360732385.github.io/2019/06/10/Mybatis%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B07.%E6%A8%A1%E7%B3%8A%E6%9F%A5%E8%AF%A2/","excerpt":"代码演示","text":"代码演示 1234567891.使用#符号&lt;!-- 输入客户关键字,模糊查询 使用#&#123;value&#125; - service中处理参数数据 %parameter% &lt;select id&#x3D;&quot;queryCustomersByLikeName&quot; parameterType&#x3D;&quot;String&quot; resultType&#x3D;&quot;customer&quot;&gt; select * from cst_customer where cust_name like #&#123;value&#125; &lt;&#x2F;select&gt; --&gt; 123456789102.使用$符号在sql语句中拼接（不推荐） &lt;!-- 输入客户关键字 - 查询客户 使用$&#123;value&#125; - 可拼接sql字段 &#39;%$&#123;value&#125;%&#39; 若参数为简单类型 - 必须使用$&#123;value&#125; 若参数为对象 - $&#123;属性&#125; &lt;select id&#x3D;&quot;queryCustomersByLikeName&quot; parameterType&#x3D;&quot;String&quot; resultType&#x3D;&quot;customer&quot;&gt; select * from cst_customer where cust_name like &#39;%$&#123;value&#125;%&#39; &lt;&#x2F;select&gt; 12345678面试题 - #&#123;&#125;和$&#123;&#125;使用及区别? #&#123;&#125;表示一个占位符号，通过#&#123;&#125;可以实现preparedStatement向占位符中设置值，自动进行java类型和jdbc类型转换。 #&#123;&#125;可以有效防止sql注入。#&#123;&#125;可以接收简单类型值或pojo属性值。 如果parameterType传输单个简单类型值，#&#123;&#125;括号中可以是value或其它名称。 $&#123;&#125;表示拼接sql串，通过$&#123;&#125;可以将parameterType 传入的内容拼接在sql中且不进行jdbc类型转换， $&#123;&#125;可以接收简单类型值或pojo属性值， 如果parameterType传输单个简单类型值，$&#123;&#125;括号中只能是value --&gt;","categories":[{"name":"MyBatis","slug":"MyBatis","permalink":"https://18360732385.github.io/categories/MyBatis/"}],"tags":[]},{"title":"可靠消息最终一致性方案","slug":"可靠消息最终一致性方案","date":"2019-06-09T16:00:00.000Z","updated":"2019-06-09T16:00:00.000Z","comments":true,"path":"2019/06/10/可靠消息最终一致性方案/","link":"","permalink":"https://18360732385.github.io/2019/06/10/%E5%8F%AF%E9%9D%A0%E6%B6%88%E6%81%AF%E6%9C%80%E7%BB%88%E4%B8%80%E8%87%B4%E6%80%A7%E6%96%B9%E6%A1%88/","excerpt":"​ 针对基于MQ的异步调用，如何保证各个服务间的分布式事务呢？使多个服务的业务逻辑，要么一起成功，要么一起失败。","text":"​ 针对基于MQ的异步调用，如何保证各个服务间的分布式事务呢？使多个服务的业务逻辑，要么一起成功，要么一起失败。 ​ 如果不考虑各种高并发、高可用等技术挑战的话，单从“可靠消息”以及“最终一致性”两个角度来考虑，如下图： ​ 这张图的可靠消息服务应该放在MQ消息队列下方，可靠消息服务的作用是给每一条消息提供第三种状态，目的是为保证上游投递消息100%成功和下游消费消息100%成功！ ​ 在后台的定时任务中，可以设置补偿机制。 （1）上游服务投递消息 首先，上游服务需要发送一条消息给可靠消息服务，即对下游服务一个接口的调用，里面包含了对应的一些请求参数。 这条消息说白了，你可以认为是对下游服务一个接口的调用，里面包含了对应的一些请求参数。 然后，可靠消息服务就得把这条消息存储到自己的数据库里去，状态为“待确认”。 接着，上游服务就可以执行自己本地的数据库操作，根据自己的执行结果，再次调用可靠消息服务的接口。 如果本地数据库操作执行成功了，那么就找可靠消息服务确认那条消息。如果本地数据库操作失败了，那么就找可靠消息服务删除那条消息。 此时如果是确认消息，那么可靠消息服务就把数据库里的消息状态更新为“已发送”，同时将消息发送给MQ。 ​ 这里有一个很关键的点，就是更新数据库里的消息状态和投递消息到MQ。这俩操作，你得放在一个方法里，而且得开启本地事务。 如果数据库里更新消息的状态失败了，那么就抛异常退出了，就别投递到MQ； 如果投递MQ失败报错了，那么就要抛异常让本地数据库事务回滚。 这俩操作必须得一起成功，或者一起失败。 （2）下游服务接收消息 ​ 下游服务就一直等着从MQ消费消息好了，如果消费到了消息，那么就操作自己本地数据库。 ​ 如果操作成功了，就反过来通知可靠消息服务，说自己处理成功了，然后可靠消息服务就会把消息的状态设置为“已完成”。 （3）如何保证上游服务对消息的100%可靠投递？ 如果上游服务给可靠消息服务发送待确认消息的过程出错了，那没关系，上游服务可以感知到调用异常的，就不用执行下面的流程了，这是没问题的。 如果上游服务操作完本地数据库之后，通知可靠消息服务确认消息或者删除消息的时候，出现了问题。比如：没通知成功，或者没执行成功，或者是可靠消息服务没成功的投递消息到MQ。这一系列步骤出了问题怎么办？ 其实也没关系，因为在这些情况下，那条消息在可靠消息服务的数据库里的状态会一直是“待确认”。 补偿机制：​ 我们在可靠消息服务里开发一个后台定时运行的线程，不停的检查各个消息的状态。对于一直是“待确认”的消息，我们会触发机制去询问上游服务。 如果上游服务返回“执行成功”，我们将改消息状态修改为“已完成”；如果上游服务返回“执行失败”，那我们就把这条消息删除。 通过这套机制，就可以保证，可靠消息服务一定会尝试完成消息到MQ的投递。 ​ 关于如何保障生产端100%消息投递成功？ 可以看这篇文章，内容思路大致和这里的一致，但是这篇文章所说的是本地消息服务最终一致性方案，结合下文RocketMQ的相关文章，我们可以在微服务中演化成独立消息服务最终一致性 方案。 ​ https://www.toutiao.com/i6672235084336071179/ （4）如何保证下游服务对消息的100%可靠接收？ ​ 那如果下游服务消费消息出了问题，没消费到？或者是下游服务对消息的处理失败了，怎么办？其实也没关系，在可靠消息服务里开发一个后台线程，不断的检查消息状态。 重试机制：​ 如果消息状态一直是“已发送”，始终没有变成“已消费”，那么就说明下游服务始终没有处理成功。此时可靠消息服务就可以再次尝试重新投递消息到MQ，让下游服务来再次处理。 ​ 此时因为网络不稳定等问题，可能出现下游服务已经消费消息，却没有ack的情况。此时只要下游服务的接口逻辑实现幂等性，保证多次处理一个消息，不会插入重复数据即可。 RocketMQ ​ 市面上的主流MQ有ActiveMQ、RabbitMQ、Kafka、RocketMQ等，但只有阿里的RocketMQ支持事务消息。 ​ 它的解决方案非常的简单，就是其内部实现会有一个定时任务，去轮训状态为待发送的消息，然后给producer发送check请求，而producer必须实现一个check监听器，监听器的内容通常就是去检查与之对应的本地事务是否成功(一般就是查询DB)，如果成功了，则MQ会将消息设置为可发送，否则就删除消息。 ​ 其思想和上面说的一样。 ​ https://www.jianshu.com/p/04bad986a4a2 ​ https://segmentfault.com/a/1190000011479826","categories":[{"name":"分布式","slug":"分布式","permalink":"https://18360732385.github.io/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"}],"tags":[{"name":"事务","slug":"事务","permalink":"https://18360732385.github.io/tags/%E4%BA%8B%E5%8A%A1/"}]},{"title":"Mybatis实战笔记6.resultMap的继承和collection的使用","slug":"Mybatis实战笔记6.resultMap的继承和collection的使用","date":"2019-06-08T16:00:00.000Z","updated":"2019-06-08T16:00:00.000Z","comments":true,"path":"2019/06/09/Mybatis实战笔记6.resultMap的继承和collection的使用/","link":"","permalink":"https://18360732385.github.io/2019/06/09/Mybatis%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B06.resultMap%E7%9A%84%E7%BB%A7%E6%89%BF%E5%92%8Ccollection%E7%9A%84%E4%BD%BF%E7%94%A8/","excerpt":"代码演示","text":"代码演示 12345678User对象字段 private int userId; private String userCode; private String password; private String name; private Role role; &#x2F;&#x2F;角色（pojo实体对象） private List&lt;Order&gt; orders; &#x2F;&#x2F;订单列表 123456789101112&lt;!-- 若resultMap与另一个定义重复 - 直接extends --&gt; &lt;resultMap type&#x3D;&quot;user&quot; id&#x3D;&quot;userOrderMap&quot; autoMapping&#x3D;&quot;true&quot; extends&#x3D;&quot;userRoleMap&quot;&gt; &lt;!-- 关联的对象为List,用collection property&#x3D;&quot;orders&quot; pojo的属性名orders javaType&#x3D;&quot;list&quot; pojo属性的类为list ofType&#x3D;&quot;Order&quot; list里的元素是Order --&gt; &lt;collection property&#x3D;&quot;orders&quot; javaType&#x3D;&quot;list&quot; ofType&#x3D;&quot;Order&quot; autoMapping&#x3D;&quot;true&quot;&gt; &lt;!-- 若多表中查询的列名相同, - 列定义里标题 --&gt; &lt;result column&#x3D;&quot;sname&quot; property&#x3D;&quot;name&quot; &#x2F;&gt; &lt;&#x2F;collection&gt; &lt;&#x2F;resultMap&gt; 123456789&lt;!-- 1:m - 管理员查询用户及其订单 - 用户编号 1 查询我的订单--&gt; &lt;select id&#x3D;&quot;queryUserIdOrders&quot; parameterType&#x3D;&quot;int&quot; resultMap&#x3D;&quot;userOrderMap&quot;&gt; select u.userId, userCode, u.name,r.roleId,r.rolename,o.orderId, o.orderNo,o.name as sname,o.address,o.mobile from tuser u,role r,torder o where u.roleId&#x3D;r.roleId and u.userId&#x3D;o.userId and u.userId&#x3D; #&#123;value&#125; &lt;&#x2F;select&gt;","categories":[{"name":"MyBatis","slug":"MyBatis","permalink":"https://18360732385.github.io/categories/MyBatis/"}],"tags":[]},{"title":"Mybatis实战笔记5.多表查询、关联查询（resultMap）","slug":"Mybatis实战笔记5.多表查询、关联查询（resultMap）","date":"2019-06-06T16:00:00.000Z","updated":"2019-06-06T16:00:00.000Z","comments":true,"path":"2019/06/07/Mybatis实战笔记5.多表查询、关联查询（resultMap）/","link":"","permalink":"https://18360732385.github.io/2019/06/07/Mybatis%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B05.%E5%A4%9A%E8%A1%A8%E6%9F%A5%E8%AF%A2%E3%80%81%E5%85%B3%E8%81%94%E6%9F%A5%E8%AF%A2%EF%BC%88resultMap%EF%BC%89/","excerpt":"代码演示","text":"代码演示 12345678910111213141516171819202122&lt;!-- 为了方便权限管理 - 查询用户及其角色信息 - 关联连接查询 - 只能使用resultMap 1:1 m:1 1:m m:n - 关联对象属性 type&#x3D;“user” 表示对应映射的是user这个pojo对象 id&#x3D;&quot;userRoleMap&quot; id对应给select语句使用的resultMap名称 autoMapping&#x3D;&quot;true&quot; 自动映射相同的表字段和pojo属性 association 对应的pojo对象 property&#x3D;&quot;role&quot; 对应表字段 javaType&#x3D;&quot;Role&quot; 对应pojo对象 --&gt; &lt;resultMap type&#x3D;&quot;user&quot; id&#x3D;&quot;userRoleMap&quot; autoMapping&#x3D;&quot;true&quot;&gt; &lt;id column&#x3D;&quot;userId&quot; property&#x3D;&quot;userId&quot;&#x2F;&gt; （可以不写） &lt;!-- 关联对象 - 一个 1:1 m:1关联 association对象关联数据封装 --&gt; &lt;association property&#x3D;&quot;role&quot; javaType&#x3D;&quot;Role&quot; autoMapping&#x3D;&quot;true&quot;&gt; &lt;&#x2F;association&gt; &lt;&#x2F;resultMap&gt; 1234567&lt;!-- 登录不能使用缓存 ,需要关闭：useCache&#x3D;&quot;false&quot;--&gt; &lt;select id&#x3D;&quot;loginUserRole&quot; parameterType&#x3D;&quot;user&quot; resultMap&#x3D;&quot;userRoleMap&quot; useCache&#x3D;&quot;false&quot;&gt; select u.userId, userCode, name,r.roleId,r.rolename from tuser u,role r where u.roleId&#x3D;r.roleId and userCode &#x3D; #&#123;userCode&#125; and password &#x3D; #&#123;password&#125; &lt;&#x2F;select&gt; 123456&lt;!-- 查询结果多条（即List&lt;User&gt;），用的也是 - resultType - 单个对象 --&gt; &lt;select id&#x3D;&quot;queryUsersRole&quot; resultMap&#x3D;&quot;userRoleMap&quot;&gt; select u.userId, userCode, name,r.roleId,r.rolename from tuser u,role r where u.roleId&#x3D;r.roleId &lt;&#x2F;select&gt;","categories":[{"name":"MyBatis","slug":"MyBatis","permalink":"https://18360732385.github.io/categories/MyBatis/"}],"tags":[]},{"title":"Redis集群模式","slug":"Redis集群模式","date":"2019-06-06T16:00:00.000Z","updated":"2019-06-06T16:00:00.000Z","comments":true,"path":"2019/06/07/Redis集群模式/","link":"","permalink":"https://18360732385.github.io/2019/06/07/Redis%E9%9B%86%E7%BE%A4%E6%A8%A1%E5%BC%8F/","excerpt":"主从复制 ​ 主从复制有两种情况，完全由从节点发起slaveof命令开始： 全量复制：用于初次复制或其他无法进行部分复制的情况，将主节点中的所有数据都发送给从节点，是一个非常重型的操作。","text":"主从复制 ​ 主从复制有两种情况，完全由从节点发起slaveof命令开始： 全量复制：用于初次复制或其他无法进行部分复制的情况，将主节点中的所有数据都发送给从节点，是一个非常重型的操作。 部分复制：用于网络中断等情况后的复制，只将中断期间主节点执行的写命令发送给从节点，与全量复制相比更加高效。需要注意的是，如果网络中断时间过长，导致主节点没有能够完整地保存中断期间执行的写命令，则无法进行部分复制，仍使用全量复制。 1.全量复制 ​ Redis通过psync命令进行全量复制的过程如下： 从节点判断无法进行部分复制，向主节点发送全量复制的请求；或从节点发送部分复制的请求，但主节点判断无法进行全量复制。 主节点收到全量复制的命令后，执行bgsave命令，在后台生成RDB文件，并使用一个缓冲区（称为复制缓冲区）记录从现在开始执行的所有写命令 主节点的bgsave执行完成后，将RDB文件发送给从节点；从节点首先清除自己的旧数据，然后载入接收的RDB文件，将数据库状态更新至主节点执行bgsave时的数据库状态 主节点将前述复制缓冲区中的所有写命令发送给从节点，从节点执行这些写命令，将数据库状态更新至主节点的最新状态 如果从节点开启了AOF，则会触发bgrewriteaof的执行，从而保证AOF文件更新至主节点的最新状态 2.部分复制 ​ 主节点和从节点分别维护一个复制偏移量（offset），代表的是主节点向从节点传递的字节数；主节点每次向从节点传播N个字节数据时，主节点的offset增加N；从节点每次收到主节点传来的N个字节数据时，从节点的offset增加N。 哨兵模式 ​ 在复制的基础上，哨兵实现了自动化的故障恢复。哨兵模式由两部分组成： 哨兵节点： 哨兵系统由一个或多个哨兵节点组成，哨兵节点是特殊的 Redis 节点，不存储数据； 数据节点： 主节点和从节点都是数据节点； 1.哨兵的心跳和选举机制 ​ 故障转移操作的第一步 要做的就是在已下线主服务器属下的所有从服务器中，挑选出一个状态良好、数据完整的从服务器，然后向这个从服务器发送 slaveof no one 命令，将这个从服务器转换为主服务器。 ​ 选择的原则是，首先过滤掉不健康的从节点；然后选择优先级最高的从节点(由slave-priority指定)；如果优先级无法区分，则选择复制偏移量最大的从节点；如果仍无法区分，则选择runid最小的从节点。 集群模式数据分区方案 ​ 三种方案：哈希取余分区、一致性哈希分区、带虚拟节点的一致性哈希分区。Redis集群采用第三种方案，其中的虚拟节点称为 槽（slot）。槽是介于数据和实际节点之间的虚拟概念，每个实际节点包含一定数量的槽，每个槽包含哈希值在一定范围内的数据。 ​ 在使用了槽的一致性哈希分区中，槽是数据管理和迁移的基本单位。槽 解耦 了 数据和实际节点 之间的关系，增加或删除节点对系统的影响很小。 集群伸缩 ​ Redis集群可以在不影响对外服务的情况下实现伸缩。伸缩的核心是槽迁移： ​ 修改槽与节点的对应关系，实现槽(即数据)在节点之间的移动。例如，如果槽均匀分布在集群的3个节点中，此时增加一个节点，则需要从3个节点中分别拿出一部分槽给新节点，从而实现槽在4个节点中的均匀分布。 ​ 集群伸缩的核心是槽迁移。在槽迁移过程中，如果客户端向源节点发送命令，源节点执行流程如下：","categories":[{"name":"Redis","slug":"Redis","permalink":"https://18360732385.github.io/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://18360732385.github.io/tags/Redis/"}]},{"title":"Mybatis实战笔记4.resultMap的简单使用","slug":"Mybatis实战笔记4.resultMap的简单使用","date":"2019-06-05T16:00:00.000Z","updated":"2019-06-05T16:00:00.000Z","comments":true,"path":"2019/06/06/Mybatis实战笔记4.resultMap的简单使用/","link":"","permalink":"https://18360732385.github.io/2019/06/06/Mybatis%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B04.resultMap%E7%9A%84%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8/","excerpt":"代码演示","text":"代码演示 12345678910111213141516 resultType 列名与pojo属性名一致 - 单个对象 resultMap 列名与pojo属性名有不一致 连接关联查询 查询单个对象数据 - 若列名与属性名不一致，不会映射 使用reusltType - 不一致的字段不会自动映射值如何解决：1: 给列名不一致列 - 使用 as 列标题 与属性名保持一致2: resultMap（推荐） 可以通过resultMap将字段名和属性名作一个对应关系，resultMap实质上还需要将查询结果映射到pojo对象中。 resultMap可以实现将查询结果映射为复杂类型的pojo，比如在查询结果映射对象中包括pojo和list实现一对一查询和一对多查询 3: 驼峰设置 - 若列名 aa_bb - 对象自动映射为 aaBb全局中配置 &lt;setting name&#x3D;&quot;mapUnderscoreToCamelCase&quot; value&#x3D;&quot;true&quot;&#x2F;&gt; 1234567891011&lt;resultMap type&#x3D;&quot;customer&quot; id&#x3D;&quot;customerMap&quot;&gt; &lt;id column&#x3D;&quot;cust_id&quot; property&#x3D;&quot;custId&quot;&#x2F;&gt; &lt;result column&#x3D;&quot;cust_name&quot; property&#x3D;&quot;custName&quot;&#x2F;&gt;&lt;&#x2F;resultMap&gt; &#x2F;&#x2F;模糊查询 &lt;select id&#x3D;&quot;queryCustomersResultMap&quot; parameterType&#x3D;&quot;string&quot; resultMap&#x3D;&quot;customerMap&quot;&gt; select * from cst_customer where cust_name like &#39;%$&#123;value&#125;%&#39; &lt;&#x2F;select&gt;","categories":[{"name":"MyBatis","slug":"MyBatis","permalink":"https://18360732385.github.io/categories/MyBatis/"}],"tags":[]},{"title":"Mybatis实战笔记3.当传参传入多个参数时（4种方法）","slug":"Mybatis实战笔记3.当传参传入多个参数时（4种方法）","date":"2019-06-05T16:00:00.000Z","updated":"2019-06-05T16:00:00.000Z","comments":true,"path":"2019/06/06/Mybatis实战笔记3.当传参传入多个参数时（4种方法）/","link":"","permalink":"https://18360732385.github.io/2019/06/06/Mybatis%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B03.%E5%BD%93%E4%BC%A0%E5%8F%82%E4%BC%A0%E5%85%A5%E5%A4%9A%E4%B8%AA%E5%8F%82%E6%95%B0%E6%97%B6%EF%BC%884%E7%A7%8D%E6%96%B9%E6%B3%95%EF%BC%89/","excerpt":"代码演示","text":"代码演示 1234567891011121.在接口中使用@Param注解 public User login(@Param(&quot;userCode&quot;) String userCode, @Param(&quot;password&quot;) String password); 2.同时在sql语句中使用注解内重定义名&lt;!-- mapper动态代理 - 传入多个参数数据 - 不可使用parameterType 集合mybatis参数注解实现--&gt; &lt;select id&#x3D;&quot;login&quot; resultType&#x3D;&quot;user&quot;&gt; select userId,userCode,name from tuser where userCode&#x3D; #&#123;userCode&#125; and password&#x3D; #&#123;password&#125; &lt;&#x2F;select&gt; 1234567891011Java Bean传参法 #&#123;&#125;里面的名称对应的是User类里面的成员属性。&lt;!-- select查询 - 返回值 sql语句 - 注入参数数据 1: #&#123;bean对象属性&#125;（无需写bean对象名） --&gt; &lt;select id&#x3D;&quot;loginUser&quot; parameterType&#x3D;&quot;User&quot; resultType&#x3D;&quot;user&quot;&gt; select userId, userCode, name from tuser where userCode &#x3D; #&#123;userCode&#125; and password &#x3D; #&#123;password&#125; &lt;&#x2F;select&gt; 1234567891011map传参法public User selectUser(Map&lt;String, Object&gt; params);&lt;select id&#x3D;&quot;selectUser&quot; parameterType&#x3D;&quot;java.util.Map&quot; resultMap&#x3D;&quot;UserResultMap&quot;&gt; select * from user where user_name &#x3D; #&#123;userName&#125; and dept_id &#x3D; #&#123;deptId&#125;&lt;&#x2F;select&gt;#&#123;&#125;里面的名称对应的是Map里面的key名称。这种方法适合传递多个参数，且参数易变能灵活传递的情况。 1顺序传参 where user_name &#x3D; #&#123;0&#125; and dept_id &#x3D; #&#123;1&#125; （不推荐）","categories":[{"name":"MyBatis","slug":"MyBatis","permalink":"https://18360732385.github.io/categories/MyBatis/"}],"tags":[]},{"title":"Redis分段锁优化","slug":"Redis分段锁优化","date":"2019-06-05T16:00:00.000Z","updated":"2019-06-05T16:00:00.000Z","comments":true,"path":"2019/06/06/Redis分段锁优化/","link":"","permalink":"https://18360732385.github.io/2019/06/06/Redis%E5%88%86%E6%AE%B5%E9%94%81%E4%BC%98%E5%8C%96/","excerpt":"​ 作为分布式锁，多应用在多服务器针对某一公共资源的竞争问题上，例如电商业务中的库存超卖、秒杀场景中。 ​ 库存超卖问题是有很多种技术解决方案的，比如悲观锁，分布式锁，乐观锁，队列串行化，Redis原子操作等等。","text":"​ 作为分布式锁，多应用在多服务器针对某一公共资源的竞争问题上，例如电商业务中的库存超卖、秒杀场景中。 ​ 库存超卖问题是有很多种技术解决方案的，比如悲观锁，分布式锁，乐观锁，队列串行化，Redis原子操作等等。 ​ 高并发场景下，针对分布式锁的优化，可以考虑分段锁机制 ，类似的如java1.7中的ConcurrentHashMap 和 java8中的LongAdder类 。 优化 ​ 下面是针对库存超卖高并发场景下的分布式锁优化： ​ 这里插一句，针对秒杀超卖场景，这种分布式锁逻辑其实是有点复杂的。可以使用悲观锁，分布式锁，乐观锁，队列串行化，异步队列分散，Redis原子操作等方案。这里推荐redis队列。 链接 ​ https://mp.weixin.qq.com/s?__biz=MzU0OTk3ODQ3Ng==&amp;mid=2247483926&amp;idx=1&amp;sn=2a796ef514dea15790e45d79d233833e&amp;chksm=fba6ea15ccd1630387b8738a00a8c1dc6ae0c535305ec4d6e3c76d64eff48bf1d47ae0eaea07&amp;scene=21#wechat_redirect","categories":[{"name":"Redis","slug":"Redis","permalink":"https://18360732385.github.io/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://18360732385.github.io/tags/Redis/"},{"name":"分布式","slug":"分布式","permalink":"https://18360732385.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"}]},{"title":"Redis缓存设计问题","slug":"Redis缓存设计问题","date":"2019-06-05T16:00:00.000Z","updated":"2019-06-05T16:00:00.000Z","comments":true,"path":"2019/06/06/Redis缓存设计问题/","link":"","permalink":"https://18360732385.github.io/2019/06/06/Redis%E7%BC%93%E5%AD%98%E8%AE%BE%E8%AE%A1%E9%97%AE%E9%A2%98/","excerpt":"​ Redis是单线程（伪，IO多路复用），线程安全。 ​ 设计一个缓存系统，不得不要考虑的问题就是：缓存穿透、缓存击穿与失效时的雪崩效应。","text":"​ Redis是单线程（伪，IO多路复用），线程安全。 ​ 设计一个缓存系统，不得不要考虑的问题就是：缓存穿透、缓存击穿与失效时的雪崩效应。 ##缓存穿透 ​ 缓存穿透是指查询一个一定不存在的数据，由于缓存是不命中时被动写的，并且出于容错考虑，如果从存储层查不到数据则不写入缓存，这将导致这个不存在的数据每次请求都要到存储层去查询，失去了缓存的意义。 ​ 在流量大时，可能DB就挂掉了，要是有人利用不存在的key频繁攻击我们的应用，这就是漏洞。 ####1.布隆过滤器 ​ 有很多种方法可以有效地解决缓存穿透问题，最常见的则是采用布隆过滤器，将所有可能存在的数据哈希到一个足够大的bitmap中，一个一定不存在的数据会被这个bitmap拦截掉，从而避免了对底层存储系统的查询压力。 ​ 布隆过滤器有一个应用场景：优惠券核券，将数百万张已销售未使用的优惠券号放入redis的布隆过滤器。用户到店核券时，先查找 2.设置空缓存​ 另外也有一个更为简单粗暴的方法（我们采用的就是这种），如果一个查询返回的数据为空（不管是数据不存在，还是系统故障），我们仍然把这个空结果进行缓存，但它的过期时间会很短，最长不超过五分钟。 ##缓存雪崩（redis挂了） ​ 缓存雪崩是指在我们设置缓存时采用了相同的过期时间，导致缓存在某一时刻同时失效，请求全部转发到DB，DB瞬时压力过重雪崩。 ####1.防止 ​缓存失效时的雪崩效应对底层系统的冲击非常可怕。大多数系统设计者考虑用加锁或者队列的方式保证缓存的单线程（进程）写，从而避免失效时大量的并发请求落到底层存储系统上。 ​这里分享一个简单方案就是将缓存失效时间分散开，比如我们可以在原有的失效时间基础上增加一个随机值，比如1-5分钟随机，这样每一个缓存的过期时间的重复率就会降低，就很难引发集体失效的事件。 ####2.解决 服务降级（引导，可以直接引导到错误友好界面） 限流（限制流量，天猫双11当天不给退货） ##缓存击穿 ​ 对于一些设置了过期时间的key，如果这些key可能会在某些时间点被超高并发地访问，是一种热点数据，如微博热搜。这个时候，需要考虑一个问题：缓存被“击穿”的问题。 ​ 这个和缓存雪崩的区别在于，击穿针对某一key缓存，而雪崩则是很多key。 ​ 缓存在某个时间点过期的时候，恰好在这个时间点对这个Key有大量的并发请求过来，这些请求发现缓存过期一般都会从后端DB加载数据并回设到缓存，这个时候大并发的请求可能会瞬间把后端DB压垮。 ###使用互斥锁(mutex key) https://www.toutiao.com/i6668092476537963022/?group_id=6668092476537963022 12345业界比较常用的做法，是使用mutex。简单地来说，就是在缓存失效的时候（判断拿出来的值为空），不是立即去load db，而是先使用缓存工具的某些带成功操作返回值的操作（比如Redis的SETNX或者Memcache的ADD）去set一个mutex key，当操作返回成功时，再进行load db的操作并回设缓存；否则，就重试整个get缓存的方法。SETNX，是「SET if Not eXists」的缩写，也就是只有不存在的时候才设置，可以利用它来实现锁的效果。setnx现在已经不建议用了，redis提供的set已经提供了类似setnx的功能。同时set可以设置多个参数，构建同步锁。 ###”提前”使用互斥锁(mutex key) 1在value内部设置1个超时值(timeout1), timeout1比实际的memcache timeout(timeout2)小。当从cache读取到timeout1发现它已经过期时候，马上延长timeout1并重新设置到cache。然后再从数据库加载数据并设置到cache中。 ###永远不过期 1234567这里的“永远不过期”包含两层意思： (1) 从redis上看，确实没有设置过期时间，这就保证了，不会出现热点key过期问题，也就是“物理”不过期。 (2) 从功能上看，如果不过期，那不就成静态的了吗？所以我们把过期时间存在key对应的value里，如果发现要过期了，通过一个后台的异步线程进行缓存的构建，也就是“逻辑”过期。 从实战看，这种方法对于性能非常友好，唯一不足的就是构建缓存时候，其余线程(非构建缓存的线程)可能访问的是老数据，但是对于一般的互联网功能来说这个还是可以忍受。 ###资源保护 1采用netflix的hystrix，可以做资源的隔离保护主线程池，如果把这个应用到缓存的构建也未尝不可。","categories":[{"name":"Redis","slug":"Redis","permalink":"https://18360732385.github.io/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://18360732385.github.io/tags/Redis/"},{"name":"缓存","slug":"缓存","permalink":"https://18360732385.github.io/tags/%E7%BC%93%E5%AD%98/"}]},{"title":"Redis持久化","slug":"Redis持久化","date":"2019-06-05T16:00:00.000Z","updated":"2019-06-05T16:00:00.000Z","comments":true,"path":"2019/06/06/Redis持久化/","link":"","permalink":"https://18360732385.github.io/2019/06/06/Redis%E6%8C%81%E4%B9%85%E5%8C%96/","excerpt":"​ Redis持久化分为RDB持久化和AOF持久化：前者将当前数据快照保存到硬盘，后者则是将每次执行的写命令保存到硬盘（类似于MySQL的binlog）。","text":"​ Redis持久化分为RDB持久化和AOF持久化：前者将当前数据快照保存到硬盘，后者则是将每次执行的写命令保存到硬盘（类似于MySQL的binlog）。 ​ 由于AOF持久化的实时性更好，即当进程意外退出时丢失的数据更少，因此AOF是目前主流的持久化方式，不过RDB持久化仍然有其用武之地。 RDB快照1.触发命令 ​ 当Redis重新启动时，可以读取快照文件恢复数据。RDB持久化可以分为手动触发和自动触发两种。 ​ save命令和bgsave命令都可以生成RDB文件： save命令会阻塞Redis服务器进程，直到RDB文件创建完毕。这段期间，服务器不能处理任何命令请求。 bgsave命令会创建一个子进程，由子进程来负责创建RDB文件，父进程(即Redis主进程)则继续处理请求。bgsave命令执行过程中，只有fork子进程时会阻塞服务器。 ​ 除了配置自动触发save m n 命令以外，还有一些其他情况会触发bgsave： 在主从复制场景下，如果从节点执行全量复制操作，则主节点会执行bgsave命令，并将rdb文件发送给从节点 执行shutdown命令时，自动执行rdb持久化 2.执行流程 ​ 图片中的5个步骤所进行的操作如下： Redis父进程首先判断：当前是否在执行save，或bgsave/bgrewriteaof（AOF重写）的子进程，如果在执行则bgsave命令直接返回。bgsave/bgrewriteaof 的子进程不能同时执行，主要是基于性能方面的考虑：两个并发的子进程同时执行大量的磁盘写操作，可能引起严重的性能问题。 父进程执行fork操作创建子进程，这个过程中父进程是阻塞的，Redis不能执行来自客户端的任何命令 父进程fork后，bgsave命令返回”Background saving started”信息并不再阻塞父进程，并可以响应其他命令 子进程创建RDB文件，根据父进程内存快照生成临时快照文件，完成后对原有文件进行原子替换 子进程发送信号给父进程表示完成，父进程更新统计信息 AOF（Append Only File）1.执行流程 ​ 由于需要记录Redis的每条写命令，因此AOF不需要触发。AOF的执行流程包括： 命令追加(append)：将Redis的写命令追加到缓冲区aof_buf； 文件写入(write)和文件同步(sync)：根据不同的同步策略将aof_buf中的内容同步到硬盘； 文件重写(rewrite)：定期重写AOF文件，达到压缩的目的。 2.文件重写 ​ AOF重写是把Redis进程内的数据转化为写命令，同步到新的AOF文件。不会对旧AOF文件进行任何读取、写入操作! ​ 文件重写的触发，分为手动触发和自动触发： ​ 手动触发时，直接调用bgrewriteaof命令，该命令的执行与bgsave有些类似：都是fork子进程进行具体的工作，且都只有在fork时阻塞。 3.文件重写流程 ​ 文件重写的流程如下： Redis父进程首先判断当前是否存在正在执行 bgsave/bgrewriteaof的子进程，如果存在则bgrewriteaof命令直接返回，如果存在bgsave命令则等bgsave执行完成后再执行。前面曾介绍过，这个主要是基于性能方面的考虑。 父进程执行fork操作创建子进程，这个过程中父进程是阻塞的。 3.1) 父进程fork后，bgrewriteaof命令返回”Background append only file rewrite started”信息并不再阻塞父进程，并可以响应其他命令。Redis的所有写命令依然写入AOF缓冲区，并根据appendfsync策略同步到硬盘，保证原有AOF机制的正确。 3.2) 由于fork操作使用写时复制技术，子进程只能共享fork操作时的内存数据。由于父进程依然在响应命令，因此Redis使用AOF重写缓冲区(图中的aof_rewrite_buf)保存这部分数据，防止新AOF文件生成期间丢失这部分数据。也就是说，bgrewriteaof执行期间，Redis的写命令同时追加到aof_buf和aof_rewirte_buf两个缓冲区。 子进程根据内存快照，按照命令合并规则写入到新的AOF文件。 5.1) 子进程写完新的AOF文件后，向父进程发信号，父进程更新统计信息，具体可以通过info persistence查看。 5.2) 父进程把AOF重写缓冲区的数据写入到新的AOF文件，这样就保证了新AOF文件所保存的数据库状态和服务器当前状态一致。 5.3) 使用新的AOF文件替换老文件，完成AOF重写。 ​ ​ 关于文件重写的流程，有两点需要特别注意： 重写由父进程fork子进程进行； 重写期间Redis执行的写命令，需要追加到新的AOF文件中，为此Redis引入了aof_rewrite_buf缓存。 混合持久化 ​ 重启 Redis 时，我们很少使用 RDB来恢复内存状态，因为会丢失大量数据。我们通常使用 AOF 日志重放，但是重放 AOF 日志性能相对 RDB来说要慢很多，这样在 Redis 实例很大的情况下，启动需要花费很长的时间。 ​ Redis 4.0 为了解决这个问题，带来了一个新的持久化选项——混合持久化。将 RDB文件的内容和增量的 AOF 日志文件存在一起。这里的 AOF 日志不再是全量的日志，而是 自持久化开始到持久化结束 的这段时间发生的增量 AOF 日志，通常这部分 AOF 日志很小。 ​ 于是在 Redis 重启的时候，可以先加载 rdb 的内容，然后再重放增量 AOF 日志就可以完全替代之前的 AOF 全量文件重放，重启效率因此大幅得到提升。","categories":[{"name":"Redis","slug":"Redis","permalink":"https://18360732385.github.io/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://18360732385.github.io/tags/Redis/"}]},{"title":"Mybatis实战笔记2.简单的parameterType和resultType介绍","slug":"Mybatis实战笔记2.简单的parameterType和resultType介绍","date":"2019-06-04T16:00:00.000Z","updated":"2019-06-04T16:00:00.000Z","comments":true,"path":"2019/06/05/Mybatis实战笔记2.简单的parameterType和resultType介绍/","link":"","permalink":"https://18360732385.github.io/2019/06/05/Mybatis%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B02.%E7%AE%80%E5%8D%95%E7%9A%84parameterType%E5%92%8CresultType%E4%BB%8B%E7%BB%8D/","excerpt":"代码演示","text":"代码演示 1234&lt;!-- parameterType如果参数类型为简单类型 #&#123;属性名 | value | 任意字符串&#125; --&gt; &lt;delete id&#x3D;&quot;deleteUser&quot; parameterType&#x3D;&quot;int&quot;&gt; delete from tuser where userId &#x3D; #&#123;value&#125; &lt;&#x2F;delete&gt; 12345&lt;!-- 查询结果多条（如：List&lt;User&gt;） - resultType - 单个对象 --&gt; &lt;select id&#x3D;&quot;queryUsers&quot; resultType&#x3D;&quot;user&quot;&gt; select userId, userCode, name from tuser &lt;&#x2F;select&gt;","categories":[{"name":"MyBatis","slug":"MyBatis","permalink":"https://18360732385.github.io/categories/MyBatis/"}],"tags":[]},{"title":"Mybatis实战笔记1.SqlMapConfig.xml文件配置","slug":"Mybatis实战笔记1.SqlMapConfig.xml文件配置","date":"2019-06-03T16:00:00.000Z","updated":"2019-06-03T16:00:00.000Z","comments":true,"path":"2019/06/04/Mybatis实战笔记1.SqlMapConfig.xml文件配置/","link":"","permalink":"https://18360732385.github.io/2019/06/04/Mybatis%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B01.SqlMapConfig.xml%E6%96%87%E4%BB%B6%E9%85%8D%E7%BD%AE/","excerpt":"代码演示","text":"代码演示 12345&lt;!-- 直接使用包中的对象名直接作为别名 不区分首字母大小写 --&gt; &lt;typeAliases&gt; &lt;package name&#x3D;&quot;com.igeek.mybatis.pojo&quot;&#x2F;&gt; &lt;&#x2F;typeAliases&gt; 123456789101112&lt;!--扫描mapper包--&gt;&lt;mappers&gt; &lt;!-- resource 使用相对于类路径的资源 &lt;mapper resource&#x3D;&quot;com&#x2F;igeek&#x2F;mybatis&#x2F;mapper&#x2F;UserMapper.xml&quot;&#x2F;&gt; --&gt; &lt;!-- class, package 此种方法要求mapper接口名称和mapper映射文件名称相同，且放在同一个目录中 &lt;mapper class&#x3D;&quot;com.igeek.mybatis.mapper.UserMapper&quot;&#x2F;&gt; --&gt; &lt;package name&#x3D;&quot;com.igeek.mybatis.mapper&quot;&#x2F;&gt; &lt;&#x2F;mappers&gt; 12345678&lt;!-- 全局配置 --&gt; &lt;settings&gt; &lt;!-- 开启二级缓存 --&gt; &lt;setting name&#x3D;&quot;cacheEnabled&quot; value&#x3D;&quot;true&quot;&#x2F;&gt; &lt;!-- 懒加载机制 --&gt; &lt;setting name&#x3D;&quot;lazyLoadingEnabled&quot; value&#x3D;&quot;true&quot;&#x2F;&gt; &lt;setting name&#x3D;&quot;lazyLoadTriggerMethods&quot; value&#x3D;&quot;equals,clone,hashCode,toString&quot;&#x2F;&gt; &lt;&#x2F;settings&gt; 12&lt;！--驼峰设置--&gt; &lt;setting name&#x3D;&quot;mapUnderscoreToCamelCase&quot; value&#x3D;&quot;true&quot;&#x2F;&gt; 12345&lt;!-- 引入mybatis - pagehelper插件 （在idea里有变化）--&gt; &lt;plugins&gt; &lt;plugin interceptor&#x3D;&quot;com.github.pagehelper.PageInterceptor&quot;&#x2F;&gt; &lt;&#x2F;plugins&gt;","categories":[{"name":"MyBatis","slug":"MyBatis","permalink":"https://18360732385.github.io/categories/MyBatis/"}],"tags":[]},{"title":"JVM内存区","slug":"JVM内存区","date":"2019-06-01T16:00:00.000Z","updated":"2019-06-01T16:00:00.000Z","comments":true,"path":"2019/06/02/JVM内存区/","link":"","permalink":"https://18360732385.github.io/2019/06/02/JVM%E5%86%85%E5%AD%98%E5%8C%BA/","excerpt":"运行时内存模型​ 内存模型在java1.6和1.8时有一些区别","text":"运行时内存模型​ 内存模型在java1.6和1.8时有一些区别 堆Heap ​ 堆是内存区最大的一块区域，唯一目的就是存放对象实例，几乎所有的对象实例以及数组都在这里分配内存。jdk1.7以前堆内存被分为：新生代、老生代、永生代。jdk1.8将永生代移除，转换成元空间，直接使用直接内存。 ​ 大部分情况，对象都会首先在 Eden 区域分配，在一次新生代垃圾回收后，如果对象还存活，则会进入 s0 或者 s1，并且对象的年龄还会加 1(Eden 区-&gt;Survivor 区后对象的初始年龄变为 1)，当它的年龄增加到一定程度（默认为 15 岁），就会被晋升到老年代中。对象晋升到老年代的年龄阈值，可以通过参数 -XX:MaxTenuringThreshold 来设置。 ​ 现在的虚拟机采用动态计算年龄阈值。 方法区 ​ 用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。 ​ 运行时常量池是方法区的一部分。Class 文件中除了有类的版本、字段、方法、接口等描述信息外，还有常量池信息（用于存放编译期生成的各种字面量和符号引用） 对象的创建和使用 Step1:类加载检查​ 虚拟机遇到一条 new 指令时，首先将去检查这个指令的参数是否能在常量池中定位到这个类的符号引用，并且检查这个符号引用代表的类是否已被加载过、解析和初始化过。如果没有，那必须先执行相应的类加载过程。 Step2:分配内存​ 在类加载检查通过后，接下来虚拟机将为新生对象分配内存。对象所需的内存大小在类加载完成后便可确定，为对象分配空间的任务等同于把一块确定大小的内存从 Java 堆中划分出来。分配方式有 “指针碰撞” 和 “空闲列表” 两种，选择那种分配方式由 Java 堆是否规整决定，而 Java 堆是否规整又由所采用的垃圾收集器是否带有压缩整理功能决定。 Step3:初始化零值​ 内存分配完成后，虚拟机需要将分配到的内存空间都初始化为零值（不包括对象头），这一步操作保证了对象的实例字段在 Java 代码中可以不赋初始值就直接使用，程序能访问到这些字段的数据类型所对应的零值。 Step4:设置对象头​ 初始化零值完成之后，虚拟机要对对象进行必要的设置，例如这个对象是那个类的实例、如何才能找到类的元数据信息、对象的哈希码、GC 分代年龄、锁状态标志等信息。 这些信息存放在对象头中。 另外，根据虚拟机当前运行状态的不同，如是否启用偏向锁等，对象头会有不同的设置方式。 Step5:执行 init 方法​ 在上面工作都完成之后，从虚拟机的视角来看，一个新的对象已经产生了，但从 Java 程序的视角来看，对象创建才刚开始，&lt;init&gt; 方法还没有执行，所有的字段都还为零。所以一般来说，执行 new 指令之后会接着执行 &lt;init&gt; 方法，把对象按照程序员的意愿进行初始化，这样一个真正可用的对象才算完全产生出来。 对象的访问：​ 建立对象就是为了使用对象，我们的 Java 程序通过栈上的 reference 数据来操作堆上的具体对象。对象的访问方式由虚拟机实现而定，目前主流的访问方式有 ①使用句柄 和 ②直接指针 两种： ​ 句柄： 如果使用句柄的话，那么 Java 堆中将会划分出一块内存来作为句柄池，reference 中存储的就是对象的句柄地址，而句柄中包含了对象实例数据与类型数据各自的具体地址信息； ​ 直接指针： 如果使用直接指针访问，那么 Java 堆对象的布局中就必须考虑如何放置访问类型数据的相关信息，而 reference 中存储的直接就是对象的地址。 ​ 这两种对象访问方式各有优势。使用句柄来访问的最大好处是 reference 中存储的是稳定的句柄地址，在对象被移动时只会改变句柄中的实例数据指针，而 reference 本身不需要修改。使用直接指针访问方式最大的好处就是速度快，它节省了一次指针定位的时间开销。","categories":[{"name":"GC调优","slug":"GC调优","permalink":"https://18360732385.github.io/categories/GC%E8%B0%83%E4%BC%98/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"https://18360732385.github.io/tags/JVM/"}]},{"title":"JVM垃圾回收","slug":"JVM垃圾回收","date":"2019-06-01T16:00:00.000Z","updated":"2019-06-01T16:00:00.000Z","comments":true,"path":"2019/06/02/JVM垃圾回收/","link":"","permalink":"https://18360732385.github.io/2019/06/02/JVM%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/","excerpt":"堆的基本结构 ​ 堆分为新生代和老年代，新生代有eden、s0和s1区域，老年代有tentired区域。官方新生代比例内部比例8:1:1，新生代与老年代比例推荐3:5。","text":"堆的基本结构 ​ 堆分为新生代和老年代，新生代有eden、s0和s1区域，老年代有tentired区域。官方新生代比例内部比例8:1:1，新生代与老年代比例推荐3:5。 ​ 大部分情况，对象都会首先在 Eden 区域分配，当 eden 区没有足够空间进行分配时，虚拟机将发起一次 Minor GC。 ​ 在一次新生代垃圾回收后，如果对象还存活，则会进入 s1(“To”)，并且对象的年龄还会加 1(Eden 区-&gt;Survivor 区后对象的初始年龄变为 1)，当它的年龄增加到一定程度（默认为 15 岁），就会被晋升到老年代中。 ​ 对象晋升到老年代的年龄阈值，可以通过参数 -XX:MaxTenuringThreshold 来设置。经过这次GC后，Eden区和”From”区已经被清空。这个时候，”From”和”To”会交换他们的角色，也就是新的”To”就是上次GC前的“From”，新的”From”就是上次GC前的”To”。不管怎样，都会保证名为To的Survivor区域是空的。Minor GC会一直重复这样的过程，直到“To”区被填满，”To”区被填满之后，会将所有对象移动到老年代中。 ​ 注意：大对象（放不下Eden区）会直接进入老年代 判断对象死亡 ​ 引用计数法（放弃）和可达性分析法。两种算法都涉及引用：强引用、软引用、弱引用、虚引用 ​ 我们用的最多的是软引用，因为软引用可以加速 JVM 对垃圾内存的回收速度，可以维护系统的运行安全，防止内存溢出（OutOfMemory）等问题的产生。 GC算法 ​ 垃圾回收算法有以下几种： 标记-清除。存在效率和内存碎片化问题 复制。用空间换时间。 标记-整理。解决了内存碎片化问题 ​ 比如在新生代中，每次收集都会有大量对象死去，所以可以选择复制算法，只需要付出少量对象的复制成本就可以完成每次垃圾收集。而老年代的对象存活几率是比较高的，而且没有额外的空间对它进行分配担保，所以我们必须选择“标记-清除”或“标记-整理”算法进行垃圾收集。","categories":[{"name":"GC调优","slug":"GC调优","permalink":"https://18360732385.github.io/categories/GC%E8%B0%83%E4%BC%98/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"https://18360732385.github.io/tags/JVM/"}]},{"title":"GC调优","slug":"GC调优","date":"2019-05-31T16:00:00.000Z","updated":"2019-05-31T16:00:00.000Z","comments":true,"path":"2019/06/01/GC调优/","link":"","permalink":"https://18360732385.github.io/2019/06/01/GC%E8%B0%83%E4%BC%98/","excerpt":"​ 多数的 Java 应用不需要在服务器上进行 GC 优化。架构调优&gt;代码调优&gt;GC调优。","text":"​ 多数的 Java 应用不需要在服务器上进行 GC 优化。架构调优&gt;代码调优&gt;GC调优。 ​ 在应用上线之前，先考虑将机器的 JVM 参数设置到最优（最适合）， GC 优化是到最后不得已才采用的手段。 在实际使用中，分析 GC 情况优化代码比优化 GC 参数要多得多。 ​ GC调优的目的是： 减少创建对象的数量； 减少使用全局变量和大对象 将转移到老年代的对象数量降低到最小 减少 GC 的执行频率和时间 GC调优策略 ​ 策略 1：将新对象预留在新生代，由于 Full GC 的成本远高于 Minor GC，因此尽可能将对象分配在新生代是明智的做法，实际项目中根据 GC 日志分析新生代空间大小分配是否合理，适当通过“-Xmn”命令调节新生代大小，最大限度降低新对象直接进入老年代的情况。 ​ 策略 2：大对象进入老年代，虽然大部分情况下，将对象分配在新生代是合理的。但是对于大对象这种做法却值得商榷，大对象如果首次在新生代分配可能会出现空间不足导致很多年龄不够的小对象被分配的老年代，破坏新生代的对象结构，可能会出现频繁的 full gc。因此，对于大对象，可以设置直接进入老年代（当然短命的大对象对于垃圾回收来说简直就是噩梦）。-XX:PretenureSizeThreshold 可以设置直接进入老年代的对象大小。 ​ 策略 3：合理设置进入老年代对象的年龄，-XX:MaxTenuringThreshold 设置对象进入老年代的年龄大小，减少老年代的内存占用，降低 full gc 发生的频率。 ​ 策略 4：设置稳定的堆大小，堆大小设置有两个参数：-Xms 初始化堆大小，-Xmx 最大堆大小。一般我们将这2个参数设置成同一个值（1/4的内存），降低堆内存不足引起的扩增操作。 ​ 策略5： 如果满足下面的指标，则一般不需要进行 GC 优化：MinorGC 执行时间不到50ms； Minor GC 执行不频繁，约10秒一次； Full GC 执行时间不到1s； Full GC 执行频率不算频繁，不低于10分钟1次。 GC调优参数 参数名称 含义 默认值 说明 -Xms 初始堆大小 物理内存的1/64(&lt;1GB) 默认(MinHeapFreeRatio参数可以调整)空余堆内存小于40%时，JVM就会增大堆直到-Xmx的最大限制. -Xmx 最大堆大小 物理内存的1/4(&lt;1GB) 默认(MaxHeapFreeRatio参数可以调整)空余堆内存大于70%时，JVM会减少堆直到 -Xms的最小限制 -Xmn 年轻代大小(1.4or lator) 注意：此处的大小是（eden+ 2 survivor space).与jmap -heap中显示的New gen是不同的。整个堆大小=年轻代大小 + 老年代大小 + 持久代（永久代）大小.增大年轻代后,将会减小年老代大小.此值对系统性能影响较大,Sun官方推荐配置为整个堆的3/8 -XX:PermSize 设置持久代(perm gen)初始值 物理内存的1/64 -XX:MaxPermSize 设置持久代最大值 物理内存的1/4 -Xss 每个线程的堆栈大小 JDK5.0以后每个线程堆栈大小为1M,以前每个线程堆栈大小为256K.更具应用的线程所需内存大小进行 调整.在相同物理内存下,减小这个值能生成更多的线程.但是操作系统对一个进程内的线程数还是有限制的,不能无限生成,经验值在3000~5000左右一般小的应用， 如果栈不是很深， 应该是128k够用的 大的应用建议使用256k。这个选项对性能影响比较大，需要严格的测试。（校长）和threadstacksize选项解释很类似,官方文档似乎没有解释,在论坛中有这样一句话:-Xss is translated in a VM flag named ThreadStackSize”一般设置这个值就可以了 -XX:NewRatio 年轻代(包括Eden和两个Survivor区)与年老代的比值(除去持久代) -XX:NewRatio=4表示年轻代与年老代所占比值为1:4,年轻代占整个堆栈的1/5Xms=Xmx并且设置了Xmn的情况下，该参数不需要进行设置。 -XX:SurvivorRatio Eden区与Survivor区的大小比值 设置为8,则两个Survivor区与一个Eden区的比值为2:8,一个Survivor区占整个年轻代的1/10 -XX:+DisableExplicitGC 关闭System.gc() 这个参数需要严格的测试 -XX:PretenureSizeThreshold 对象超过多大是直接在旧生代分配 0 单位字节 新生代采用Parallel ScavengeGC时无效另一种直接在旧生代分配的情况是大的数组对象,且数组中无外部引用对象. -XX:ParallelGCThreads 并行收集器的线程数 此值最好配置与处理器数目相等 同样适用于CMS -XX:MaxGCPauseMillis 每次年轻代垃圾回收的最长时间(最大暂停时间) 如果无法满足此时间,JVM会自动调整年轻代大小,以满足此值. 实际案例 ​ 举例一： ​ 现象：最红星期五活动，堆内存持续飙升导致OOM。 ​ 一般可以设置OOM异常时自动生成堆快照，参数是“-XX:+HeapDumpOnOutOfMemoryError” 。通过堆快照，查看实例数，可以看到HashMap的实例数目和实例大小占比极高，超出大概50%。 ​ 定位代码中，发现有两层for循环，new 一个容量为32位的hashMap发生在第一层循环下，但其实在一系列条件判断后，我们才在第二次循环中使用这个hashMap。 ​ 因为过早地实例出大量无效的实例对象，测试时没有发现。导致线上请求高峰期间，内存飙升发生OOM。 ​ 举例二： ​ 现象： 公众号项目线上运行期间，每隔一段时间（七八天）会固定发生一次OOM，需要重启。 ​ 运维生成2个堆快照（项目重启后、项目OOM时），对比两个时期的堆快照，发现threadLocal对象大小明显提高。 ​ 定位到问题线程后，明显发现线程没有对threadLocal对象进行清除。导致随着时间越久，threadLocal对象越大。 ​ 创建线程时最好设置好线程名称，好定位问题线程。 链接 ​ https://www.ibm.com/developerworks/cn/java/j-lo-visualvm/index.html","categories":[{"name":"GC调优","slug":"GC调优","permalink":"https://18360732385.github.io/categories/GC%E8%B0%83%E4%BC%98/"}],"tags":[{"name":"GC","slug":"GC","permalink":"https://18360732385.github.io/tags/GC/"}]},{"title":"volatile","slug":"volatile","date":"2019-05-12T16:00:00.000Z","updated":"2019-05-12T16:00:00.000Z","comments":true,"path":"2019/05/13/volatile/","link":"","permalink":"https://18360732385.github.io/2019/05/13/volatile/","excerpt":"可见性 ​ 可见性，是指线程之间的可见性，一个线程修改的状态对另一个线程是可见的。比如：用volatile修饰的变量，就会具有可见性。volatile修饰的变量不允许线程内部缓存和重排序，即直接修改内存。所以对其他线程是可见的。","text":"可见性 ​ 可见性，是指线程之间的可见性，一个线程修改的状态对另一个线程是可见的。比如：用volatile修饰的变量，就会具有可见性。volatile修饰的变量不允许线程内部缓存和重排序，即直接修改内存。所以对其他线程是可见的。 ​ 但是这里需要注意一个问题，volatile只能让被他修饰内容具有可见性，但不能保证它具有原子性。比如 volatile int a = 0；之后有一个操作 a++；这个变量a具有可见性，但是a++ 依然是一个非原子操作，也就是这个操作同样存在线程安全问题。 ​ 在 Java 中 volatile、synchronized 和 final 实现可见性。 原子性（无） ​ java的concurrent包下提供了一些原子类：AtomicInteger、AtomicLong、AtomicReference等，注意ABA问题。 ​ 在 Java 中 synchronized 和在 lock、unlock 中操作保证原子性，所以volatile不能保证原子性。 有序性 ​ Java 语言提供了 volatile 和 synchronized 两个关键字来保证线程之间操作的有序性，volatile 是因为其本身包含“禁止指令重排序”的语义，synchronized 是由“一个变量在同一个时刻只允许一条线程对其进行 lock 操作”这条规则获得的，此规则决定了持有同一个对象锁的两个同步块只能串行执行。 重排序 ​ 观察以下代码： ​ NoVisibility可能会持续循环下去，因为读线程可能永远都看不到ready的值。甚至NoVisibility可能会输出0，因为读线程可能看到了写入ready的值，但却没有看到之后写入number的值，这种现象被称为“重排序”。 ​ 在没有同步的情况下，编译器、处理器以及运行时等都可能对操作的执行顺序进行一些意想不到的调整。在缺乏足够同步的多线程程序中，要想对内存操作的执行顺序进行判断，无法得到正确的结论。 12345678910111213141516171819202122public class NoVisibility &#123; private static boolean ready; private static int number; &#x2F;&#x2F;线程:循环打印number private static class ReaderThread extends Thread &#123; @Override public void run() &#123; while(!ready) &#123; Thread.yield(); &#125; System.out.println(number); &#125; &#125; &#x2F;&#x2F;main方法,java编译器会进行重排序 public static void main(String[] args) &#123; new ReaderThread().start(); number &#x3D; 42; ready &#x3D; true; &#125;&#125; volatile的原理 ​ 每一条线程都有自己的独立内存，线程的代码在执行过程中，就可以直接从自己本地缓存里加载变量副本，不需要从主内存(共享内存)加载变量值。这就造成了变量的不可见性！ 变量加了volatile关键字后，线程只要修改data变量的值，就会在修改完自己本地工作内存的data变量值之后，强制将这个data变量最新的值刷回主内存，必须让主内存里的data变量值立马变成最新的值！ 如果此时别的线程的工作内存中有这个data变量的本地缓存，也就是一个变量副本的话，那么会强制让其他线程的工作内存中的data变量缓存直接失效过期，不允许再次读取和使用了！ 如果线程2在代码运行过程中再次需要读取data变量的值，此时尝试从本地工作内存中读取，就会发现这个data = 0已经过期了！此时，他就必须重新从主内存中加载data变量最新的值！那么不就可以读取到data = 1这个最新的值了！ volatile 性能 ​ volatile保证了变量的可见性，有序性（禁止重排序），但是不保证原子性！！！ ​ volatile 的读性能消耗与普通变量几乎相同，但是写操作稍慢，因为它需要在本地代码中插入许多内存屏障指令来保证处理器不发生乱序执行。","categories":[{"name":"Java并发","slug":"Java并发","permalink":"https://18360732385.github.io/categories/Java%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"并发","slug":"并发","permalink":"https://18360732385.github.io/tags/%E5%B9%B6%E5%8F%91/"},{"name":"volatile","slug":"volatile","permalink":"https://18360732385.github.io/tags/volatile/"}]},{"title":"CAS自旋","slug":"CAS自旋","date":"2019-05-11T16:00:00.000Z","updated":"2019-05-11T16:00:00.000Z","comments":true,"path":"2019/05/12/CAS自旋/","link":"","permalink":"https://18360732385.github.io/2019/05/12/CAS%E8%87%AA%E6%97%8B/","excerpt":"###CAS机制及优点 ​ CAS——Compare And Swap，比较并替换。CAS能保证变量操作的原子性。","text":"###CAS机制及优点 ​ CAS——Compare And Swap，比较并替换。CAS能保证变量操作的原子性。 ​ 多线程操作变量时，synchronized加锁，相当于N个线程一个一个的排队在更新那个数值。这相当于悲观锁（互斥锁），认为每次操作都会失败，所以操作前都对变量加锁，让各个线程串行化。 ​ 而CAS是一种无锁机制，认为每次操作都成功，相当于乐观锁（自旋锁）。使用CAS机制的典型是一系列Atomic原子类，与synchronized相比，CAS更加轻量高效。 CAS的缺点 ​ 自旋：高并发情况下，容易造成很多线程会不停的自旋，性能和效率都不是特别好。 ​ 对于资源竞争较少（线程冲突较轻）的情况，使用synchronized同步锁进行线程阻塞和唤醒切换以及用户态内核态间的切换操作额外浪费消耗CPU资源。而CAS基于硬件实现，不需要进入内核，不需要切换线程，自旋的几率较少，因此可以获得更高的性能。 ​ 对于资源竞争严重（线程冲突严重）的情况，CAS自旋的概率会比较大，从而浪费更多的CPU资源，效率低于synchronized。 ​ 不能保证代码块的原子性 ：CAS机制所保证的只是一个变量的原子性操作，而不能保证整个代码块的原子性。比如需要保证3个变量共同进行原子性的更新，就不得不使用Synchronized了。 ​ ABA问题 ：这是CAS机制最大的问题所在，CAS机制中的C是以旧值A和新值B比较，旧值A经过多次线程操作，可能会变成新值A，造成比较值相等的错觉现象，这样会造成银行交易的问题。解决ABA问题的理想途径是比较版本号，如AtomicReference类。 Java8中的LongAdder ​ LongAdder类使用了分段CAS以及自动分段迁移的方式，大幅度提升多线程高并发执行CAS操作的性能！ ​ 类似于Java1.7中ConcurrentHashMap使用的分段锁。java8之后的ConcurrentHashMap又优化成了synchronized加锁。 AtomicStampedReference类和Atomic类的使用demo ​ Atomic类存在ABA问题，AtomicStampedReference类解决了ABA问题。 ​ 下面是AtomicInteger的一个代码实践。 12AtomicInteger count &#x3D; new AtomicInteger(0);count.getAndIncrement(); https://blog.csdn.net/mxj588love/article/details/80136021","categories":[{"name":"Java并发","slug":"Java并发","permalink":"https://18360732385.github.io/categories/Java%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"并发","slug":"并发","permalink":"https://18360732385.github.io/tags/%E5%B9%B6%E5%8F%91/"},{"name":"CAS","slug":"CAS","permalink":"https://18360732385.github.io/tags/CAS/"}]},{"title":"synchronized","slug":"synchronized","date":"2019-05-11T16:00:00.000Z","updated":"2019-05-11T16:00:00.000Z","comments":true,"path":"2019/05/12/synchronized/","link":"","permalink":"https://18360732385.github.io/2019/05/12/synchronized/","excerpt":"使用场景 ​ 在java代码中使用synchronized可是使用在代码块和方法中，根据Synchronized用的位置可以有这些使用场景：","text":"使用场景 ​ 在java代码中使用synchronized可是使用在代码块和方法中，根据Synchronized用的位置可以有这些使用场景： ​ 如图，synchronized可以用在方法上也可以使用在代码块中，其中方法是实例方法和静态方法分别锁的是该类的实例对象和该类的对象。而使用在代码块中也可以分为三种，具体的可以看下面的表格。 ​ 这里的需要注意的是：如果锁的是类对象的话，尽管new多个实例对象，但他们仍然是属于同一个类依然会被锁住，即线程之间保证同步关系。 对象锁（monitor）机制 ​ 写一个demo，编译后查看字节码。代码中有一个同步代码块，锁住的是类对象，并且还有一个同步静态方法，锁住的依然是该类的类对象。 12345678910public class SynchronizedDemo &#123; public static void main(String[] args) &#123; synchronized (SynchronizedDemo.class) &#123; &#125; method(); &#125; private static void method() &#123; &#125;&#125; ​ 如图，上面用黄色高亮的部分就是需要注意的部分了，这也是添Synchronized关键字之后独有的。执行同步代码块后首先要先执行monitorenter指令，退出的时候monitorexit指令。 ​ 通过分析之后可以看出，使用Synchronized进行同步，其关键就是必须要对对象的监视器monitor进行获取，当线程获取monitor后才能继续往下执行，否则就只能等待。而这个获取的过程是互斥的，即同一时刻只有一个线程能够获取到monitor。 ​ 上面的demo中在执行完同步代码块之后紧接着再会去执行一个静态同步方法，而这个方法锁的对象依然就这个类对象，那么这个正在执行的线程还需要获取该锁吗？答案是不必的，从上图中就可以看出来，执行静态同步方法的时候就只有一条monitorexit指令，并没有monitorenter获取锁的指令。这就是锁的重入性，即在同一锁程中，线程不需要再次获取同一把锁。Synchronized先天具有重入性。每个对象拥有一个计数器，当线程获取该对象锁后，计数器就会加一，释放锁后就会将计数器减一。 ​ 结合AQS抽象队列同步器看。 优化 ​ 它最大的特征就是在同一时刻只有一个线程能够获得对象的监视器（monitor），从而进入到同步代码块或者同步方法之中，即表现为互斥性（排它性）。这种方式肯定效率低下，每次只能通过一个线程，既然每次只能通过一个，这种形式不能改变的话，那么我们能不能让每次通过的速度变快一点了。 ​ 在聊到锁的优化也就是锁的几种状态前，有两个知识点需要先关注：（1）CAS操作 （2）Java对象头，这是理解下面知识的前提条件。 ​ CAS知识查看CAS自旋。 java对象头 ​ 在同步的时候是获取对象的monitor,即获取到对象的锁。那么对象的锁怎么理解？无非就是类似对对象的一个标志，那么这个标志就是存放在Java对象的对象头。Java对象头里的Mark Word里默认的存放： ​ 对象的Hashcode、分代年龄和锁标记位等。 ###锁的状态和比较 ​ Java SE 1.6中，锁一共有4种状态，级别从低到高依次是：无锁状态、偏向锁状态、轻量级锁状态和重量级锁状态，这几个状态会随着竞争情况逐渐升级。锁可以升级但不能降级（在jdk8中是可以降级的，但是条件非常苛刻），意味着偏向锁升级成轻量级锁后不能降级成偏向锁。这种锁升级却不能降级的策略，目的是为了提高获得锁和释放锁的效率。对象的MarkWord变化为下图： 锁升级过程 链接 ​ https://www.jianshu.com/p/d53bf830fa09 ​ https://github.com/farmerjohngit/myblog/issues/12","categories":[{"name":"Java并发","slug":"Java并发","permalink":"https://18360732385.github.io/categories/Java%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"并发","slug":"并发","permalink":"https://18360732385.github.io/tags/%E5%B9%B6%E5%8F%91/"},{"name":"synchronized","slug":"synchronized","permalink":"https://18360732385.github.io/tags/synchronized/"}]},{"title":"AQS组件原理及使用","slug":"AQS组件原理及使用","date":"2019-05-10T16:00:00.000Z","updated":"2019-05-10T16:00:00.000Z","comments":true,"path":"2019/05/11/AQS组件原理及使用/","link":"","permalink":"https://18360732385.github.io/2019/05/11/AQS%E7%BB%84%E4%BB%B6%E5%8E%9F%E7%90%86%E5%8F%8A%E4%BD%BF%E7%94%A8/","excerpt":"Semaphore信号量 ​ synchronized 和 ReentrantLock 都是一次只允许一个线程访问某个资源，Semaphore允许多个线程同时访问。获取许可方式是acquire() 方法，释放一个许可则是release()方法。","text":"Semaphore信号量 ​ synchronized 和 ReentrantLock 都是一次只允许一个线程访问某个资源，Semaphore允许多个线程同时访问。获取许可方式是acquire() 方法，释放一个许可则是release()方法。 12345678910111213141516171819202122232425262728293031323334public class SemaphoreExample1 &#123; &#x2F;&#x2F; 请求的数量 private static final int threadCount &#x3D; 550; public static void main(String[] args) throws InterruptedException &#123; &#x2F;&#x2F; 创建一个具有固定线程数量的线程池对象（如果这里线程池的线程数量给太少的话你会发现执行的很慢） ExecutorService threadPool &#x3D; Executors.newFixedThreadPool(300); &#x2F;&#x2F; 一次只能允许执行的线程数量。 final Semaphore semaphore &#x3D; new Semaphore(20); for (int i &#x3D; 0; i &lt; threadCount; i++) &#123; final int threadnum &#x3D; i; threadPool.execute(() -&gt; &#123;&#x2F;&#x2F; Lambda 表达式的运用 try &#123; semaphore.acquire();&#x2F;&#x2F; 获取一个许可，所以可运行线程数量为20&#x2F;1&#x3D;20 test(threadnum); semaphore.release();&#x2F;&#x2F; 释放一个许可 &#125; catch (InterruptedException e) &#123; &#x2F;&#x2F; TODO Auto-generated catch block e.printStackTrace(); &#125; &#125;); &#125; threadPool.shutdown(); System.out.println(&quot;finish&quot;); &#125; public static void test(int threadnum) throws InterruptedException &#123; Thread.sleep(1000);&#x2F;&#x2F; 模拟请求的耗时操作 System.out.println(&quot;threadnum:&quot; + threadnum); Thread.sleep(1000);&#x2F;&#x2F; 模拟请求的耗时操作 &#125;&#125; CountDownLatch倒计时器 ​ CountDownLatch是一个同步工具类，用来协调多个线程之间的同步。这个工具通常用来控制线程等待，它可以让某一个线程等待直到倒计时结束，再开始执行。 ​ CountDownLatch的不足：它是一次性的，计数器的值只能在构造方法中初始化一次，之后没有任何机制再次对其设置值，当CountDownLatch使用完毕后，它不能再次被使用。 ​ 它有以下两种经典用法： 某一线程在开始运行前等待n个线程执行完毕。将 CountDownLatch 的计数器初始化为n ：new CountDownLatch(n)，每当一个任务线程执行完毕，就将计数器减1 countdownlatch.countDown()，当计数器的值变为0时，在CountDownLatch上 await() 的线程就会被唤醒。 ​ 一个典型应用场景就是启动一个服务时，主线程需要等待多个组件加载完毕，之后再继续执行。 123456789101112131415161718192021222324252627282930313233public class CountDownLatchExample1 &#123; &#x2F;&#x2F; 请求的数量 private static final int threadCount &#x3D; 550; public static void main(String[] args) throws InterruptedException &#123; &#x2F;&#x2F; 创建一个具有固定线程数量的线程池对象（如果这里线程池的线程数量给太少的话你会发现执行的很慢） ExecutorService threadPool &#x3D; Executors.newFixedThreadPool(300); final CountDownLatch countDownLatch &#x3D; new CountDownLatch(threadCount); for (int i &#x3D; 0; i &lt; threadCount; i++) &#123; final int threadnum &#x3D; i; threadPool.execute(() -&gt; &#123;&#x2F;&#x2F; Lambda 表达式的运用 try &#123; test(threadnum); &#125; catch (InterruptedException e) &#123; &#x2F;&#x2F; TODO Auto-generated catch block e.printStackTrace(); &#125; finally &#123; countDownLatch.countDown();&#x2F;&#x2F; 表示一个请求已经被完成 &#125; &#125;); &#125; countDownLatch.await(); threadPool.shutdown(); System.out.println(&quot;finish&quot;); &#125; public static void test(int threadnum) throws InterruptedException &#123; Thread.sleep(1000);&#x2F;&#x2F; 模拟请求的耗时操作 System.out.println(&quot;threadnum:&quot; + threadnum); Thread.sleep(1000);&#x2F;&#x2F; 模拟请求的耗时操作 &#125;&#125; ​ 实现多个线程开始执行任务的最大并行性。注意是并行性，不是并发，强调的是多个线程在某一时刻同时开始执行。类似于赛跑，将多个线程放到起点，等待发令枪响，然后同时开跑。可以模拟压测。 ​ 做法是初始化一个共享的 CountDownLatch 对象，将其计数器初始化为 1 ：new CountDownLatch(1)，多个线程在开始执行任务前首先 coundownlatch.await()，当主线程调用 countDown() 时，计数器变为0，多个线程同时被唤醒。 CyclicBarrier循环栅栏 ​ CyclicBarrier 和 CountDownLatch 非常类似，它也可以实现线程间的技术等待，但是它的功能比 CountDownLatch 更加复杂和强大。主要应用场景和 CountDownLatch 类似。 ​ CyclicBarrier 的字面意思是可循环使用（Cyclic）的屏障（Barrier）。它要做的事情是，让一组线程到达一个屏障（也可以叫同步点）时被阻塞，直到最后一个线程到达屏障时，屏障才会开门，所有被屏障拦截的线程才会继续干活。 ​ CyclicBarrier默认的构造方法是 CyclicBarrier(int parties)，其参数表示屏障拦截的线程数量，每个线程调用await方法告诉 CyclicBarrier 我已经到达了屏障，然后当前线程被阻塞。 ​ CyclicBarrier 可以用于多线程计算数据，最后合并计算结果的应用场景。比如我们用一个Excel保存了用户所有银行流水，每个Sheet保存一个帐户近一年的每笔银行流水，现在需要统计用户的日均银行流水，先用多线程处理每个sheet里的银行流水，都执行完之后，得到每个sheet的日均银行流水，最后，再用barrierAction用这些线程的计算结果，计算出整个Excel的日均银行流水。这个其实可以用FutureTask实现。 12345678910111213141516171819202122232425262728293031323334353637public class CyclicBarrierExample2 &#123; &#x2F;&#x2F; 请求的数量 private static final int threadCount &#x3D; 550; &#x2F;&#x2F; 需要同步的线程数量 private static final CyclicBarrier cyclicBarrier &#x3D; new CyclicBarrier(5); public static void main(String[] args) throws InterruptedException &#123; &#x2F;&#x2F; 创建线程池 ExecutorService threadPool &#x3D; Executors.newFixedThreadPool(10); for (int i &#x3D; 0; i &lt; threadCount; i++) &#123; final int threadNum &#x3D; i; Thread.sleep(1000); threadPool.execute(() -&gt; &#123; try &#123; test(threadNum); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (BrokenBarrierException e) &#123; e.printStackTrace(); &#125; &#125;); &#125; threadPool.shutdown(); &#125; public static void test(int threadnum) throws InterruptedException, BrokenBarrierException &#123; System.out.println(&quot;threadnum:&quot; + threadnum + &quot;is ready&quot;); try &#123; cyclicBarrier.await(2000, TimeUnit.MILLISECONDS); &#125; catch (Exception e) &#123; System.out.println(&quot;-----CyclicBarrierException------&quot;); &#125; System.out.println(&quot;threadnum:&quot; + threadnum + &quot;is finish&quot;); &#125;&#125; 12345678910111213141516171819202122运行结果：threadnum:0is readythreadnum:1is readythreadnum:2is readythreadnum:3is readythreadnum:4is readythreadnum:4is finishthreadnum:0is finishthreadnum:1is finishthreadnum:2is finishthreadnum:3is finishthreadnum:5is readythreadnum:6is readythreadnum:7is readythreadnum:8is readythreadnum:9is readythreadnum:9is finishthreadnum:5is finishthreadnum:8is finishthreadnum:7is finishthreadnum:6is finish...... ​ 另外，CyclicBarrier还提供一个更高级的构造函数CyclicBarrier(int parties, Runnable barrierAction)，用于在线程到达屏障时，优先执行barrierAction，方便处理更复杂的业务场景。示例代码如下： 1234567891011121314151617181920212223242526272829303132333435public class CyclicBarrierExample3 &#123; &#x2F;&#x2F; 请求的数量 private static final int threadCount &#x3D; 550; &#x2F;&#x2F; 需要同步的线程数量 private static final CyclicBarrier cyclicBarrier &#x3D; new CyclicBarrier(5, () -&gt; &#123; System.out.println(&quot;------当线程数达到之后，优先执行------&quot;); &#125;); public static void main(String[] args) throws InterruptedException &#123; &#x2F;&#x2F; 创建线程池 ExecutorService threadPool &#x3D; Executors.newFixedThreadPool(10); for (int i &#x3D; 0; i &lt; threadCount; i++) &#123; final int threadNum &#x3D; i; Thread.sleep(1000); threadPool.execute(() -&gt; &#123; try &#123; test(threadNum); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (BrokenBarrierException e) &#123; e.printStackTrace(); &#125; &#125;); &#125; threadPool.shutdown(); &#125; public static void test(int threadnum) throws InterruptedException, BrokenBarrierException &#123; System.out.println(&quot;threadnum:&quot; + threadnum + &quot;is ready&quot;); cyclicBarrier.await(); System.out.println(&quot;threadnum:&quot; + threadnum + &quot;is finish&quot;); &#125;&#125; 123456789101112131415161718192021222324运行结果：threadnum:0is readythreadnum:1is readythreadnum:2is readythreadnum:3is readythreadnum:4is ready------当线程数达到之后，优先执行------threadnum:4is finishthreadnum:0is finishthreadnum:2is finishthreadnum:1is finishthreadnum:3is finishthreadnum:5is readythreadnum:6is readythreadnum:7is readythreadnum:8is readythreadnum:9is ready------当线程数达到之后，优先执行------threadnum:9is finishthreadnum:5is finishthreadnum:6is finishthreadnum:8is finishthreadnum:7is finish...... CyclicBarrier和CountDownLatch的区别 ​ CountDownLatch是计数器，只能使用一次，而CyclicBarrier的计数器提供reset功能，可以多次使用。 ​ 对于CountDownLatch来说，重点是“一个线程（多个线程）等待”，而其他的N个线程在完成“某件事情”之后，可以终止，也可以等待。而对于CyclicBarrier，重点是多个线程，在任意一个线程没有完成，所有的线程都必须等待。 ​ CountDownLatch是计数器，线程完成一个记录一个，只不过计数不是递增而是递减，而CyclicBarrier更像是一个阀门，需要所有线程都到达，阀门才能打开，然后继续执行。 CountDownLatch CyclicBarrier 计数方式 减计数 加计数 线程释放 计算为0时，释放所有等待线程 计数到达指定值时，释放所有等待线程 失效 计数为0后，无法重置，失效 计数达到指定值，重置为0重新开始 方法 调用countDown（），计数减一；调用await（）只进行阻塞，对计数没有影响 调用await（）方法计数加一，若计数加一后的值不为预设值，线程阻塞 可重复性 不可重复利用 可重复利用","categories":[{"name":"Java并发","slug":"Java并发","permalink":"https://18360732385.github.io/categories/Java%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"AQS","slug":"AQS","permalink":"https://18360732385.github.io/tags/AQS/"},{"name":"并发","slug":"并发","permalink":"https://18360732385.github.io/tags/%E5%B9%B6%E5%8F%91/"}]},{"title":"AQS抽象队列同步器","slug":"AQS抽象队列同步器","date":"2019-05-09T16:00:00.000Z","updated":"2019-05-09T16:00:00.000Z","comments":true,"path":"2019/05/10/AQS抽象队列同步器/","link":"","permalink":"https://18360732385.github.io/2019/05/10/AQS%E6%8A%BD%E8%B1%A1%E9%98%9F%E5%88%97%E5%90%8C%E6%AD%A5%E5%99%A8/","excerpt":"​ AbstractQueuedSynchronizer——抽象队列同步器。java并发包下很多API都是基于AQS来实现的加锁和释放锁等功能的，例如：ReentrantLock、ReentrantReadWriteLock等并发类底层都是基于AQS来实现的。","text":"​ AbstractQueuedSynchronizer——抽象队列同步器。java并发包下很多API都是基于AQS来实现的加锁和释放锁等功能的，例如：ReentrantLock、ReentrantReadWriteLock等并发类底层都是基于AQS来实现的。 ​ AQS是一个并发包的基础组件，用来实现各种锁，各种同步组件的。它包含了state变量、加锁线程、等待队列等并发中的核心组件。 ReentrantLock加锁和释放锁的底层原理 ​ 如下图，ReentrantLock类里有一个AQS对象。 ​ 这个AQS对象内部有一个核心的变量叫做state，是int类型的，代表了加锁的状态。初始状态下，这个state的值是0。 ​ 这个AQS内部还有一个关键变量，用来记录当前加锁的是哪个线程，初始化状态下，这个变量是null。 ​ 一个线程尝试用ReentrantLock的lock()方法进行加锁，会发生什么？ ​ 线程1跑过来调用ReentrantLock的lock()方法尝试进行加锁，这个加锁的过程，直接就是用CAS操作将state值从0变为1。如果之前没人加过锁，那么state的值肯定是0，此时线程1就可以加锁成功，设置state=1，同时设置当前加锁线程是自己。 ReentrantLock——可重入锁 ​ 可重入锁的意思，就是你可以对一个ReentrantLock对象多次执行lock()加锁和unlock()释放锁，也就是可以对一个锁加多次，叫做可重入加锁。 ​ 为什么可重入，因为state变量！！！看一下两个线程加锁的过程： ​ 那么线程2会怎么办？答案是等待队列。 ​ 线程2会将自己放入AQS中的一个等待队列，因为自己尝试加锁失败了，此时就要将自己放入队列中来等待，等待线程1释放锁之后，自己就可以重新尝试加锁了。 ​ 所以大家可以看到，AQS是如此的核心！AQS内部还有一个等待队列，专门放那些加锁失败的线程！ ​ 这个等待队列是一个虚拟的双向队列（虚拟的双向队列即不存在队列实例，仅存在结点之间的关联关系）。 ​ 那么如果线程1如何释放AQS锁，释放锁会发生什么？ ​ 线程1在执行完自己的业务逻辑代码之后，释放锁的过程非常的简单，就是将AQS内的state变量的值递减1，如果state值为0，则彻底释放锁，会将“加锁线程”变量也设置为null！ ​ 接下来，会从等待队列的队头唤醒线程2重新尝试加锁 ，这时线程2继续尝试用CAS操作将state从0变为1，如果成功代表加锁完成，就会将state设置为1。此外，还要把“加锁线程”设置为线程2自己，同时线程2自己就从等待队列中出队了。 AQS组件 ​ Semaphore ：计算剩余许可量，允许多个线程访问 ​ CountDownLatch ：发号枪，同步状态保存当前计数值 ​ CyclicBarrier：循环栅栏，是CountDownLatch的进阶版，可以重复使用 ​ FutureTask ：保存任务状态，如正在运行、已完成、已取消 ​ ReentrantReadWriteLock ：读写锁，共享锁和独占锁。AQS同时管理两个锁的计数。AQS同时还维护一个等待线程，当锁可用时，队列头部是写锁，则该写锁获得锁；若头部是读锁，则它后面队列的第一个写锁前的所有读锁都获得锁。 ​ 现实应用中，当多线程中读线程（共享锁）多于写线程时，选择ReentrantReadWriteLock 。 链接 ​ https://mp.weixin.qq.com/s?__biz=Mzg2OTA0Njk0OA==&amp;mid=2247484832&amp;idx=1&amp;sn=f902febd050eac59d67fc0804d7e1ad5&amp;source=41#wechat_redirect","categories":[{"name":"Java并发","slug":"Java并发","permalink":"https://18360732385.github.io/categories/Java%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"AQS","slug":"AQS","permalink":"https://18360732385.github.io/tags/AQS/"},{"name":"并发","slug":"并发","permalink":"https://18360732385.github.io/tags/%E5%B9%B6%E5%8F%91/"}]},{"title":"注册中心—多级缓存设计","slug":"注册中心—多级缓存设计","date":"2019-05-02T16:00:00.000Z","updated":"2019-05-04T16:00:00.000Z","comments":true,"path":"2019/05/03/注册中心—多级缓存设计/","link":"","permalink":"https://18360732385.github.io/2019/05/03/%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83%E2%80%94%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98%E8%AE%BE%E8%AE%A1/","excerpt":"1.读写锁 ​ ReentrantReadWriteLock ——读写锁，其内部封装了。读锁是共享锁，写锁是独占锁。 ​ 如果有线程加了写锁，其他线程就不能加读锁了。如果有一个线程加了读锁，别的线程是可以随意同时加读锁的。","text":"1.读写锁 ​ ReentrantReadWriteLock ——读写锁，其内部封装了。读锁是共享锁，写锁是独占锁。 ​ 如果有线程加了写锁，其他线程就不能加读锁了。如果有一个线程加了读锁，别的线程是可以随意同时加读锁的。 123456789ReentrantReadWriteLock lock &#x3D; new ReentrantReadWriteLock();&#x2F;&#x2F;加写锁lock.writeLock().lock;lock.writeLock().unlock;&#x2F;&#x2F;加读锁lock.readLock().lock();lock.readLock().unlock(); 2.注册中心如何使用读写锁 ​ 一个微服务注册中心，内存中有一个服务注册表的概念。因为不停地有微服务（多线程）来注册和获取服务实例的ip和端口，所以这个内存里的服务注册表数据，天然就是有读写并发问题的。 ​ 这种情况下，我们不应该使用synchronized方法，因为这样会让所有读写线程全部串行化，导致并发性非常的低。我们分析可以知道，注册表这个类肯定是读多写少的，那么可以使用读写锁！ 3.多级缓存机制​ 上图看似挺美好，但是也有问题，那就是如果一堆读线程（发现服务）在读注册表（占用读锁），导致写线程（注册服务）无法操作。能不能尽量在写数据的期间还保证可以继续读数据呢？答案是多级缓存机制 。 在拉取注册表的时候： 首先从ReadOnlyCacheMap里查缓存的注册表。 若没有，就找ReadWriteCacheMap里缓存的注册表。 如果还没有，就从内存中获取实际的注册表数据。 在注册表发生变更的时候： 会在内存中更新变更的注册表数据，同时过期掉ReadWriteCacheMap。 此过程不会影响ReadOnlyCacheMap提供人家查询注册表。 一段时间内（默认30秒），各服务拉取注册表会直接读ReadOnlyCacheMap，这段时间其实各服务拉取的服务信息是旧的、部分服务失效的。 30秒过后，Eureka Server的后台线程发现ReadWriteCacheMap已经清空了，也会清空ReadOnlyCacheMap中的缓存 下次有服务拉取注册表，又会从内存中获取最新的数据了，同时填充各个缓存。 4.总结​ 有一个疑问：ReadWriteCacheMap的作用是什么？ 通过上面的分析可以看到，Eureka通过设置适当的请求频率（拉取注册表30秒间隔，发送心跳30秒间隔），可以保证一个大规模的系统每秒请求Eureka Server的次数在几百次。 同时通过纯内存的注册表，保证了所有的请求都可以在内存处理，确保了极高的性能 另外,多级缓存机制，确保了不会针对内存数据结构发生频繁的读写并发冲突操作，进一步提升性能。 ​ https://mp.weixin.qq.com/s?__biz=MzU0OTk3ODQ3Ng==&amp;mid=2247483759&amp;idx=1&amp;sn=7e6575861a779711a5ef5f27b8e088e4&amp;chksm=fba6e96cccd1607a709c1437eb0b92df703b8d8eac466205798618480abf600b67f032782e88&amp;scene=21#wechat_redirect","categories":[],"tags":[{"name":"并发","slug":"并发","permalink":"https://18360732385.github.io/tags/%E5%B9%B6%E5%8F%91/"},{"name":"Eruka","slug":"Eruka","permalink":"https://18360732385.github.io/tags/Eruka/"}]}],"categories":[{"name":"Redis","slug":"Redis","permalink":"https://18360732385.github.io/categories/Redis/"},{"name":"分布式","slug":"分布式","permalink":"https://18360732385.github.io/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"设计模式","slug":"设计模式","permalink":"https://18360732385.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"微服务","slug":"微服务","permalink":"https://18360732385.github.io/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"MySql进阶","slug":"MySql进阶","permalink":"https://18360732385.github.io/categories/MySql%E8%BF%9B%E9%98%B6/"},{"name":"MyBatis","slug":"MyBatis","permalink":"https://18360732385.github.io/categories/MyBatis/"},{"name":"GC调优","slug":"GC调优","permalink":"https://18360732385.github.io/categories/GC%E8%B0%83%E4%BC%98/"},{"name":"Java并发","slug":"Java并发","permalink":"https://18360732385.github.io/categories/Java%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://18360732385.github.io/tags/Redis/"},{"name":"分布式","slug":"分布式","permalink":"https://18360732385.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"Docker","slug":"Docker","permalink":"https://18360732385.github.io/tags/Docker/"},{"name":"AOP","slug":"AOP","permalink":"https://18360732385.github.io/tags/AOP/"},{"name":"单例模式","slug":"单例模式","permalink":"https://18360732385.github.io/tags/%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/"},{"name":"加解密","slug":"加解密","permalink":"https://18360732385.github.io/tags/%E5%8A%A0%E8%A7%A3%E5%AF%86/"},{"name":"RSA","slug":"RSA","permalink":"https://18360732385.github.io/tags/RSA/"},{"name":"微服务","slug":"微服务","permalink":"https://18360732385.github.io/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"线程","slug":"线程","permalink":"https://18360732385.github.io/tags/%E7%BA%BF%E7%A8%8B/"},{"name":"国密","slug":"国密","permalink":"https://18360732385.github.io/tags/%E5%9B%BD%E5%AF%86/"},{"name":"集合","slug":"集合","permalink":"https://18360732385.github.io/tags/%E9%9B%86%E5%90%88/"},{"name":"HashMap","slug":"HashMap","permalink":"https://18360732385.github.io/tags/HashMap/"},{"name":"MySql","slug":"MySql","permalink":"https://18360732385.github.io/tags/MySql/"},{"name":"事务","slug":"事务","permalink":"https://18360732385.github.io/tags/%E4%BA%8B%E5%8A%A1/"},{"name":"ID生成","slug":"ID生成","permalink":"https://18360732385.github.io/tags/ID%E7%94%9F%E6%88%90/"},{"name":"并发","slug":"并发","permalink":"https://18360732385.github.io/tags/%E5%B9%B6%E5%8F%91/"},{"name":"秒杀","slug":"秒杀","permalink":"https://18360732385.github.io/tags/%E7%A7%92%E6%9D%80/"},{"name":"中间件","slug":"中间件","permalink":"https://18360732385.github.io/tags/%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"消息队列","slug":"消息队列","permalink":"https://18360732385.github.io/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"},{"name":"算法","slug":"算法","permalink":"https://18360732385.github.io/tags/%E7%AE%97%E6%B3%95/"},{"name":"IO模式","slug":"IO模式","permalink":"https://18360732385.github.io/tags/IO%E6%A8%A1%E5%BC%8F/"},{"name":"Java基础","slug":"Java基础","permalink":"https://18360732385.github.io/tags/Java%E5%9F%BA%E7%A1%80/"},{"name":"工具","slug":"工具","permalink":"https://18360732385.github.io/tags/%E5%B7%A5%E5%85%B7/"},{"name":"Idea","slug":"Idea","permalink":"https://18360732385.github.io/tags/Idea/"},{"name":"Zookeeper","slug":"Zookeeper","permalink":"https://18360732385.github.io/tags/Zookeeper/"},{"name":"数据库","slug":"数据库","permalink":"https://18360732385.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"FastJson","slug":"FastJson","permalink":"https://18360732385.github.io/tags/FastJson/"},{"name":"Bean","slug":"Bean","permalink":"https://18360732385.github.io/tags/Bean/"},{"name":"过滤器","slug":"过滤器","permalink":"https://18360732385.github.io/tags/%E8%BF%87%E6%BB%A4%E5%99%A8/"},{"name":"kafka","slug":"kafka","permalink":"https://18360732385.github.io/tags/kafka/"},{"name":"MySql进阶","slug":"MySql进阶","permalink":"https://18360732385.github.io/tags/MySql%E8%BF%9B%E9%98%B6/"},{"name":"Java8新特性","slug":"Java8新特性","permalink":"https://18360732385.github.io/tags/Java8%E6%96%B0%E7%89%B9%E6%80%A7/"},{"name":"缓存","slug":"缓存","permalink":"https://18360732385.github.io/tags/%E7%BC%93%E5%AD%98/"},{"name":"JVM","slug":"JVM","permalink":"https://18360732385.github.io/tags/JVM/"},{"name":"GC","slug":"GC","permalink":"https://18360732385.github.io/tags/GC/"},{"name":"volatile","slug":"volatile","permalink":"https://18360732385.github.io/tags/volatile/"},{"name":"CAS","slug":"CAS","permalink":"https://18360732385.github.io/tags/CAS/"},{"name":"synchronized","slug":"synchronized","permalink":"https://18360732385.github.io/tags/synchronized/"},{"name":"AQS","slug":"AQS","permalink":"https://18360732385.github.io/tags/AQS/"},{"name":"Eruka","slug":"Eruka","permalink":"https://18360732385.github.io/tags/Eruka/"}]}